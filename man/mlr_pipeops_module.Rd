% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/PipeOpModule.R
\name{mlr_pipeops_module}
\alias{mlr_pipeops_module}
\alias{PipeOpModule}
\title{Class for Torch Module Wrappers}
\format{
\code{\link{R6Class}} object inheriting from \link{PipeOp`}.
}
\arguments{
\item{param_vals}{(named \code{list()})\cr
The initial parameters for the object.}

\item{packages}{(\code{character()})\cr
The R packages this object depends on.}

\item{module}{(\code{nn_module})\cr
The \link[torch:nn_module]{module} that is wrapped.}

\item{outname}{(\code{character()})
The names of the output channels.
If this parameter has length 1, the parameter \link[torch:nn_module]{module} must return a \link[torch:tensor]{tensor}.
Otherwise it must return a \code{list()} of tensors of corresponding length.}

\item{(any)\cr}{Additional parameters passed to the printer of the wrapped \code{nn_module}.}
}
\description{
\code{PipeOpModule} wraps an \code{\link{nn_module}} that is being called during the \code{train} phase of this \code{\link{PipeOp}}.
By doing so, this allows to assemble \code{PipeOpModule}s in a computational \link[=`Graph`]{graph} that represents a neural
network architecture. Such a graph can also be used to create a \code{\link{nn_graph}} that can be treated like any other
\code{\link{nn_module}}.

In most cases it is easier to create such a network by creating a isomorphic graph consisting
of nodes of class \code{\link{PipeOpTorchIngress}} and \code{\link{PipeOpTorch}}. This graph will then generate the graph consiting
of \code{PipeOpModule}s during its training phase.

The \verb{$predict} method does currently not serve a meaningful purpose.

Initializes an instance of this \link[R6:R6Class]{R6} class.

Prints the object.
}
\section{Input and Output Channels}{

The number and names of the input and output channels can be set during construction. They input and output
\code{"torch_tensor"} during training, and \code{NULL} during prediction.
}

\section{State}{

The \verb{$state} is an empty \code{list()}.
}

\section{Parameters}{

No parameters.
}

\section{Internals}{

During training, the wrapped \code{\link{nn_module}} is called with the provided inputs in the order in which the channels
are defined. Arguments are \strong{not} matched by name.
}

\examples{
# one input and output channel:#'   of the output channels.
po_module = PipeOpModule$new("linear", torch::nn_linear(10, 20), inname = "input", outname = "output")
x = torch::torch_randn(16, 10)
y = po_module$train(list(input = x))
str(y)

# multiple channels
nn_custom = torch::nn_module("nn_custom",
  initialize = function(in_features, out_features) {
    self$lin1 = torch::nn_linear(in_features, out_features)
    self$lin2 = torch::nn_linear(in_features, out_features)
  },
  forward = function(x, z) {
    list(out1 = self$lin1(x), out2 = torch::nnf_relu(self$lin2(z)))
  }
)

module = nn_custom(3, 2)
po_module = PipeOpModule$new("custom", module, inname = c("x", "z"), outname = c("out1", "out2"))
x = torch::torch_randn(1, 3)
z = torch::torch_randn(1, 3)
out = po_module$train(list(x = x, z = z))
str(out)


# in a nn_graph
graph = pot("ingress_num") \%>>\% pot("linear", out_features = 10L)
result = graph$train(tsk("iris"))
linear_module = result[[1L]]$graph$pipeops$linear
linear_module
formalArgs(linear_module$module)
linear_module$input$name

}
\seealso{
nn_module, mlr_pipeops_torch, nn_graph, model_descriptor_to_module, PipeOp, Graph
}
