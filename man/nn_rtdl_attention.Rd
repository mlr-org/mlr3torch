% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/nn_rtdl_attention.R
\name{nn_rtdl_attention}
\alias{nn_rtdl_attention}
\title{Attention Mechanism from RTDL paper}
\usage{
nn_rtdl_attention(
  d_token,
  n_heads = 1L,
  dropout = 0.5,
  bias = TRUE,
  initialization = "kaiming"
)
}
\arguments{
\item{d_token}{(\code{integer(1)})\cr
The dimension of the tokens.}

\item{n_heads}{(\code{integer(1)})\cr
The number of heads.}

\item{dropout}{(\code{numeric(1)})\cr
The dropout probability.}

\item{bias}{(\code{logical(1)})\cr
Whether to use a bias.}

\item{initialization}{(\code{character(1)})\cr
Which initialization strategy to use.}
}
\description{
This is a reimplementation of the rtdl python library.
}
\references{
Gorishniy Y, Rubachev I, Khrulkov V, Babenko A (2021).
\dQuote{Revisiting Deep Learning Models for Tabular Data.}
\emph{arXiv}, \bold{2106.11959}.

Devlin, Jacob, Chang, Ming-Wei, Lee, Kenton, Toutanova, Kristina (2018).
\dQuote{Bert: Pre-training of deep bidirectional transformers for language understanding.}
\emph{arXiv preprint arXiv:1810.04805}.
}
