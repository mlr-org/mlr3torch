% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/PipeOpTorchOptimizer.R
\name{mlr_pipeops_torch_optimizer}
\alias{mlr_pipeops_torch_optimizer}
\alias{PipeOpTorchOptimizer}
\title{Optimizer Configuration}
\format{
\code{\link{R6Class}} inheriting from \code{\link{PipeOp}}.
}
\description{
Configures the optimizer of a deep learning model.
}
\section{Construction}{
 \code{PipeOpTorchOptimizer$new(optimizer = t_opt("adam"), id = "torch_optimizer", param_vals = list())}
\itemize{
\item \code{optimizer} :: \code{\link{TorchOptimizer}} or \code{character(1)} or \code{torch_optimizer_generator}\cr
The optimizer (or something convertible via \code{\link[=as_torch_optimizer]{as_torch_optimizer()}}).
This object is cloned during construction.
\item \code{id} :: \code{character(1)}\cr The id for the object. The default is "torch_optimizer".
\item \code{param_vals} :: named \code{list()}\cr List of hyperparameter settings to overwrite the initial values. Default is  \code{list()}.
}
}

\section{Input and Output Channels}{

There is one input channel \code{"input"} and one output channel \code{"output"}.
During \emph{training}, the channels are of class \code{\link{ModelDescriptor}}.
During \emph{prediction}, the channels are of class \code{\link{Task}}.
}

\section{State}{

The state is set to an empty \code{list()}.
}

\section{Parameters}{

The parameters are defined dynamically from the optimizer that is set during construction.
}

\section{Fields}{

Only fields inherited from \code{\link{PipeOp}}.
}

\section{Methods}{

Only methods inherited from \code{\link{PipeOp}}.
}

\section{Internals}{

During training, the optimizer is cloned and added to the \code{\link{ModelDescriptor}}.
Note that the parameter set of the stored \code{\link{TorchOptimizer}} is reference-identical to the parameter set of the
pipeop itself.
}

\examples{
po_opt = po("torch_optimizer", "sgd", lr = 0.01)
po_opt$param_set
mdin = po("torch_ingress_num")$train(list(tsk("iris")))
mdin[[1L]]$optimizer
mdout = po_opt$train(mdin)
mdout[[1L]]$optimizer
}
\seealso{
Other model_configuration: 
\code{\link{mlr_pipeops_torch_callbacks}},
\code{\link{mlr_pipeops_torch_loss}}
}
\concept{model_configuration}
