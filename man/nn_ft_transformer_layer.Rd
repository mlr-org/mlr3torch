% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/PipeOpTorchFTTransformerBlock.R,
%   R/PipeOpTorchFTTransformerLayer.R
\name{nn_ft_transformer_layer}
\alias{nn_ft_transformer_layer}
\title{Single Transformer Layer for FT-Transformer}
\usage{
nn_ft_transformer_layer(
  d_token,
  attention_n_heads,
  attention_dropout,
  attention_initialization,
  ffn_d_hidden,
  ffn_dropout,
  ffn_activation,
  residual_dropout,
  prenormalization,
  is_first_layer,
  first_prenormalization,
  attention_normalization,
  ffn_normalization,
  kv_compression_ratio,
  kv_compression_sharing,
  n_tokens = NULL,
  last_layer_query_idx,
  query_idx
)

nn_ft_transformer_layer(
  d_token,
  attention_n_heads,
  attention_dropout,
  attention_initialization,
  ffn_d_hidden,
  ffn_dropout,
  ffn_activation,
  residual_dropout,
  prenormalization,
  is_first_layer,
  first_prenormalization,
  attention_normalization,
  ffn_normalization,
  kv_compression_ratio,
  kv_compression_sharing,
  n_tokens = NULL,
  last_layer_query_idx,
  query_idx
)
}
\arguments{
\item{d_token}{(\code{integer(1)})\cr
The dimension of the embedding.}

\item{attention_n_heads}{(\code{integer(1)})\cr
Number of attention heads.}

\item{attention_dropout}{(\code{numeric(1)})\cr
Dropout probability in the attention mechanism.}

\item{attention_initialization}{(\code{character(1)})\cr
Initialization method for attention weights. Either "kaiming" or "xavier".}

\item{ffn_d_hidden}{(\code{integer(1)})\cr
Hidden dimension of the feed-forward network.}

\item{ffn_dropout}{(\code{numeric(1)})\cr
Dropout probability in the feed-forward network. TODO: explain further}

\item{ffn_activation}{(\code{nn_module})\cr
Instantiated activation function for the feed-forward network. Default value is \code{nn_reglu()}.}

\item{residual_dropout}{(\code{numeric(1)})\cr
Dropout probability for residual connections.}

\item{prenormalization}{(\code{logical(1)})\cr
Whether to apply normalization before attention and FFN (TRUE) or after (FALSE). When this is FALSE, \code{first_prenormalization} must also be FALSE.}

\item{is_first_layer}{(\code{logical(1)})\cr
Whether this is the first layer in the transformer stack. Default value is FALSE.}

\item{first_prenormalization}{(\code{logical(1)})\cr
Whether to apply prenormalization in the first layer. It is recommended to set this to FALSE.}

\item{attention_normalization}{(\code{function})\cr
Normalization function to use for attention. Default value is \code{nn_layer_norm}.}

\item{ffn_normalization}{(\code{function})\cr
Normalization function to use for the feed-forward network. Default value is \code{nn_layer_norm}.}

\item{kv_compression_ratio}{(\code{numeric(1)} or \code{NULL})\cr
Ratio for key-value compression. If NULL, no compression is applied.}

\item{kv_compression_sharing}{(\code{character(1)} or \code{NULL})\cr
How to share compression weights. Options: "headwise", "key_value", or "layerwise".}

\item{n_tokens}{(\code{integer(1)} or \code{NULL})\cr
Number of tokens in the input sequence.}

\item{last_layer_query_idx}{(\code{integer()} or \code{NULL})\cr
Indices to select for the query in the last layer.}

\item{query_idx}{(\code{integer()} or \code{NULL})\cr
Indices to select for the query.}
}
\description{
A transformer layer, consisting of a multi-head self-attention mechanism followed by a feed-forward
network

This is used in the FT-Transformer.

TODO: re-introduce is_first_layer, since there are enough checks based on first_prenormalization that I think this is useful to have, even though it leads to a clunky interface.

A transformer layer, consisting of a multi-head self-attention mechanism followed by a feed-forward
network

This is used in the FT-Transformer.

TODO: re-introduce is_first_layer, since there are enough checks based on first_prenormalization that I think this is useful to have, even though it leads to a clunky interface.
However, this can be factored out once we create the Learner, since we can keep first_prenormalization and prenormalization as parameters for the learner, then
figure out a cleaner interface for the transformer layer based on how they actually get used.
}
\references{
Devlin, Jacob, Chang, Ming-Wei, Lee, Kenton, Toutanova, Kristina (2018).
\dQuote{Bert: Pre-training of deep bidirectional transformers for language understanding.}
\emph{arXiv preprint arXiv:1810.04805}.

Devlin, Jacob, Chang, Ming-Wei, Lee, Kenton, Toutanova, Kristina (2018).
\dQuote{Bert: Pre-training of deep bidirectional transformers for language understanding.}
\emph{arXiv preprint arXiv:1810.04805}.
}
