% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/LearnerClassifAlexNet.R
\name{mlr_learners_classif.alexnet}
\alias{mlr_learners_classif.alexnet}
\alias{LearnerClassifAlexNet}
\title{AlexNet Image Classifier}
\description{
Convolutional network for image classification.
}
\section{Dictionary}{

This \link{Learner} can be instantiated via the \link[mlr3misc:Dictionary]{dictionary} \link{mlr_learners} or with the associated sugar function \code{\link[=lrn]{lrn()}}:

\if{html}{\out{<div class="sourceCode">}}\preformatted{mlr_learners$get("classif.alexnet")
lrn("classif.alexnet")
}\if{html}{\out{</div>}}
}

\section{Meta Information}{

, * Task type: \dQuote{classif}, * Predict Types: \dQuote{response}, * Feature Types: \dQuote{imageuri}, * Required Packages: \CRANpkg{mlr3}, \CRANpkg{mlr3torch}, \CRANpkg{torch}, \CRANpkg{torchvision}
}

\section{Parameters}{

, |Id                   |Type      |Default             |Levels                |Range                                |, |:--------------------|:---------|:-------------------|:---------------------|:------------------------------------|, |augmentation         |untyped   |-                   |                      |-                                    |, |batch_size           |integer   |16                  |                      |\eqn{[1, \infty)}{[1, Inf)}          |, |callbacks            |untyped   |-                   |                      |-                                    |, |device               |character |auto                |auto, cpu, cuda, meta |-                                    |, |drop_last            |logical   |FALSE               |TRUE, FALSE           |-                                    |, |epochs               |integer   |-                   |                      |\eqn{[0, \infty)}{[0, Inf)}          |, |keep_last_prediction |logical   |TRUE                |TRUE, FALSE           |-                                    |, |measures             |untyped   |-                   |                      |-                                    |, |num_threads          |integer   |1                   |                      |\eqn{[1, \infty)}{[1, Inf)}          |, |shuffle              |logical   |TRUE                |TRUE, FALSE           |-                                    |, |valid_split          |numeric   |0.33                |                      |\eqn{[0, 1]}{[0, 1]}                 |, |architecture         |untyped   |-                   |                      |-                                    |, |opt.lr               |numeric   |0.001               |                      |\eqn{[0, \infty)}{[0, Inf)}          |, |opt.betas            |untyped   |c    , 0.9  , 0.999 |                      |-                                    |, |opt.eps              |numeric   |1e-08               |                      |\eqn{[1e-16, 1e-04]}{[1e-16, 1e-04]} |, |opt.weight_decay     |numeric   |0                   |                      |\eqn{[0, 1]}{[0, 1]}                 |, |opt.amsgrad          |logical   |FALSE               |TRUE, FALSE           |-                                    |, |loss.weight          |untyped   |-                   |                      |-                                    |, |loss.ignore_index    |integer   |-                   |                      |\eqn{(-\infty, \infty)}{(-Inf, Inf)} |, |loss.reduction       |character |mean                |mean, sum             |-                                    |, |pretrained           |logical   |TRUE                |TRUE, FALSE           |-                                    |, |freeze               |logical   |TRUE iff pretrained |TRUE, FALSE           |-                                    |
}

\section{Architecture}{

Calls \link[torchvision:model_alexnet]{torchvision::model_alexnet} from package \CRANpkg{torchvision} to load the
network architecture. If the parameter \code{pretrained == TRUE}, the learner is initialized
with pretrained weights and the final layer is replaced with an simple linear
output layer tailored to the task when the method \verb{$train()} is called.
}

\section{Optimizer}{

The following optimizers are supported and can be set during construction using the
\code{.optimizer} argument.
\itemize{
\item adadelta
\item adagrad
\item adam
\item asgd
\item lbfgs
\item rmsprop
\item rprop
\item sgd
}

Depending on \code{.optimizer}, the constructor arguments of the corresponding
\link[torch:optimizer]{optimizer} are dynamically set as parameters of the learner, prefixed by
"opt.".
The optimizer cannot be changed after the learner was constructed.
}

\section{Loss}{

For classificatoin tasks, the following lossses are supported and can be set during construction
using the \code{.loss} argument:
\itemize{
\item mse
}

The default is set to "cross_entropy". Depending on the loss the constructor arguments of the
corresponding loss funcction are dynamically set as parameters of the learner, prefixed by
"loss.", see section \emph{Parameters} for an example using the \code{.loss = "cross_entropy"}.
The loss function cannot be changed after the learner was constructed.
}

\examples{
learner = mlr3::lrn("classif.alexnet")
print(learner)
# available parameters:
learner$param_set$ids()
}
\references{
Krizhevsky, Alex (2014).
\dQuote{One weird trick for parallelizing convolutional neural networks.}
\emph{arXiv preprint arXiv:1404.5997}.
}
\seealso{
\itemize{
\item \link[mlr3misc:Dictionary]{Dictionary} of \link[mlr3:Learner]{Learners}: \link[mlr3:mlr_learners]{mlr3::mlr_learners}.
\item \code{as.data.table(mlr_learners)} for a table of available \link[=Learner]{Learners} in the running session (depending on the loaded packages).
\item Chapter in the \href{https://mlr3book.mlr-org.com/}{mlr3book}: \url{https://mlr3book.mlr-org.com/basics.html#learners}
\item \CRANpkg{mlr3learners} for a selection of recommended learners.
\item \CRANpkg{mlr3cluster} for unsupervised clustering learners.
\item \CRANpkg{mlr3pipelines} to combine learners with pre- and postprocessing steps.
\item \CRANpkg{mlr3tuning} for tuning of hyperparameters, \CRANpkg{mlr3tuningspaces} for established default tuning spaces.
}
}
\section{Super classes}{
\code{\link[mlr3:Learner]{mlr3::Learner}} -> \code{\link[mlr3:LearnerClassif]{mlr3::LearnerClassif}} -> \code{\link[mlr3torch:LearnerClassifTorchAbstract]{mlr3torch::LearnerClassifTorchAbstract}} -> \code{LearnerClassifAlexNet}
}
\section{Methods}{
\subsection{Public methods}{
\itemize{
\item \href{#method-LearnerClassifAlexNet-new}{\code{LearnerClassifAlexNet$new()}}
\item \href{#method-LearnerClassifAlexNet-clone}{\code{LearnerClassifAlexNet$clone()}}
}
}
\if{html}{\out{
<details><summary>Inherited methods</summary>
<ul>
<li><span class="pkg-link" data-pkg="mlr3" data-topic="Learner" data-id="base_learner"><a href='../../mlr3/html/Learner.html#method-Learner-base_learner'><code>mlr3::Learner$base_learner()</code></a></li>
<li><span class="pkg-link" data-pkg="mlr3" data-topic="Learner" data-id="format"><a href='../../mlr3/html/Learner.html#method-Learner-format'><code>mlr3::Learner$format()</code></a></li>
<li><span class="pkg-link" data-pkg="mlr3" data-topic="Learner" data-id="help"><a href='../../mlr3/html/Learner.html#method-Learner-help'><code>mlr3::Learner$help()</code></a></li>
<li><span class="pkg-link" data-pkg="mlr3" data-topic="Learner" data-id="predict"><a href='../../mlr3/html/Learner.html#method-Learner-predict'><code>mlr3::Learner$predict()</code></a></li>
<li><span class="pkg-link" data-pkg="mlr3" data-topic="Learner" data-id="predict_newdata"><a href='../../mlr3/html/Learner.html#method-Learner-predict_newdata'><code>mlr3::Learner$predict_newdata()</code></a></li>
<li><span class="pkg-link" data-pkg="mlr3" data-topic="Learner" data-id="print"><a href='../../mlr3/html/Learner.html#method-Learner-print'><code>mlr3::Learner$print()</code></a></li>
<li><span class="pkg-link" data-pkg="mlr3" data-topic="Learner" data-id="reset"><a href='../../mlr3/html/Learner.html#method-Learner-reset'><code>mlr3::Learner$reset()</code></a></li>
<li><span class="pkg-link" data-pkg="mlr3" data-topic="Learner" data-id="train"><a href='../../mlr3/html/Learner.html#method-Learner-train'><code>mlr3::Learner$train()</code></a></li>
<li><span class="pkg-link" data-pkg="mlr3torch" data-topic="LearnerClassifTorchAbstract" data-id="build"><a href='../../mlr3torch/html/LearnerClassifTorchAbstract.html#method-LearnerClassifTorchAbstract-build'><code>mlr3torch::LearnerClassifTorchAbstract$build()</code></a></li>
</ul>
</details>
}}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-LearnerClassifAlexNet-new"></a>}}
\if{latex}{\out{\hypertarget{method-LearnerClassifAlexNet-new}{}}}
\subsection{Method \code{new()}}{
Creates a new instance of this \link[R6:R6Class]{R6} class.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{LearnerClassifAlexNet$new(.optimizer = "adam", .loss = "cross_entropy")}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{.optimizer}}{(\code{character()})\cr
A character string containing the name of the optimizer.
Possible values are \code{torch_reflections$optimizer}.}

\item{\code{.loss}}{(\code{character()})\cr
A character string containing the name of the loss function
For possible values see \code{torch_reflections$loss}.}
}
\if{html}{\out{</div>}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-LearnerClassifAlexNet-clone"></a>}}
\if{latex}{\out{\hypertarget{method-LearnerClassifAlexNet-clone}{}}}
\subsection{Method \code{clone()}}{
The objects of this class are cloneable with this method.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{LearnerClassifAlexNet$clone(deep = FALSE)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{deep}}{Whether to make a deep clone.}
}
\if{html}{\out{</div>}}
}
}
}
