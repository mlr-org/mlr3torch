% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/LearnerTorch.R
\name{mlr_learners_regr.torch}
\alias{mlr_learners_regr.torch}
\alias{LearnerRegrTorch}
\title{Abstract Base Class for a Torch Learner}
\format{
\code{\link{R6Class}} inheriting from \code{\link{LearnerRegr}}, \code{\link{Learner}}.
}
\description{
This base class provides the basic functionality for training and prediction of a neural network.
All torch regression learners should inherit from the respective subclass.
}
\section{Methods}{
 Methods inherited from \code{\link{LearnerRegr}}, \code{\link{Learner}}.
}

\section{Construction}{

Classification:

\code{LearnerClassifTorch$new(id, optimizer, loss, param_set, properties = c("twoclass", "multiclass"), packages = character(0), predict_types = c("response", "prob"), feature_types, man, label, callbacks = list())}

Regression:

\code{LearnerRegrTorch$new(id, optimizer, loss, param_set, properties = character(0), packages = character(0), predict_types = "response", feature_types, man, label, callbacks = list())}
\itemize{
\item \code{id} :: \code{character(1)}\cr The id for the object.
\item \code{optimizer} :: \code{\link{TorchOptimizer}}\cr
The optimizer for the model.
\item \code{loss} :: \code{\link{TorchLoss}}\cr
The loss for the model.
\item \code{callbacks}:: \code{list()} of \code{\link{TorchCallback}} objects\cr
The callbacks used for training. Must have unique IDs.
\item \code{param_set} :: \code{link{ParamSet}{paradox}}\cr The parameter set.
\item \code{properties} :: \code{character()}\cr
The properties for the learner, see \code{mlr_reflections$learner_properties}.
\item \code{packages} :: \code{character()}\cr
The additional packages on which the learner depends. The packages \code{"torch"} and \code{"mlr3torch"} are automatically
added so do not have to be passed explicitly.
\item \code{predict_types} :: \code{character()}\cr
The learner's predict types, see \code{mlr_reflections$learner_predict_types}.
The default is \code{"response"} and \code{"prob"}.
\item \code{feature_types} :: \code{character()}\cr
The feature types the learner can deal with, see \code{mlr_reflections$task_feature_types}.
\item \code{man} :: \code{character(1)}\cr
String in the format \verb{[pkg]::[topic]} pointing to a manual page for this object.
The referenced help package can be opened via method \verb{$help()}.
\item \code{label} :: \code{character(1)}\cr
The label for the learner.
}
}

\section{State}{

The state is a list with elements \code{network}, \code{optimizer}, \code{loss_fn} and \code{callbacks}.
}

\section{Parameters}{

\itemize{
\item \code{batch_size} :: (\code{integer(1)})\cr
The batch size.
\item \code{epochs} :: \code{integer(1)}\cr
The number of epochs.
\item \code{device} :: \code{character(1)}\cr
The device. One of \code{"auto"}, \code{"cpu"}, or \code{"cuda"}.
\item \code{measures_train} :: \code{\link{Measure}} or \code{list()} of \code{\link{Measure}}s.
Measures to be evaluated during training.
\item \code{measures_valid} :: \code{\link{Measure}} or \code{list()} of \code{\link{Measure}}s.
Measures to be evaluated during validation.
\item \code{drop_last} :: \code{logical(1)}\cr
Whether to drop the last batch in each epoch during training. Default is \code{FALSE}.
\item \code{num_threads} :: \code{integer(1)}\cr
The number of threads (if \code{device} is \code{"cpu"}). Default is 1.
\item \code{shuffle} :: \code{logical(1)}\cr
Whether to shuffle the instances in the dataset. Default is \code{TRUE}.
\item \code{early_stopping_rounds} :: \code{integer(1)}\cr
How many rounds to wait for early stopping. The default is 0.
\item \code{seed} :: \code{integer(1)}\cr
The seed that is used during training. The value \code{seed + 1} is used during prediction.
If this is missing (default), a random seed is generated.
}

Additionally there are the parameters for the optimizer, the loss function and the callbacks.
\code{"opt."}, \code{"loss."} and \code{"cb.<callback id>"} respectively.
}

\section{Fields}{

Fields inherited from \code{\link{LearnerClassif}} or \code{\link{LearnerRegr}} and
\itemize{
\item \code{network} :: (\code{\link[torch:nn_module]{nn_module()}})\cr
The network (only available after training).
\item \code{history} :: \code{\link{CallbackTorchHistory}}\cr
The training history.
}
}

\section{Internals}{

A \code{\link{ParamSetCollection}} is created that combines the \code{param_set} from the construction with the
default torch parameters, as well as the loss, optimizer and callback parameters
(prefixed with \code{"loss."}, \code{"opt."}, and \code{"cb."} respectively.
}

\section{Inheriting}{

When inheriting from this class, one should overload two, methods, namely the
\code{private$.network(task, param_vals)} and the \code{private$.dataset(task, param_vals)}.
The former should construct \code{\link[torch:nn_module]{torch::nn_module}} object for the given task and parameter values, while the latter
is responsible for creating a \code{\link[torch:dataset]{torch::dataset}}.
Note that the output of this network are expected to be the scores before the application of the final softmax
layer.

It is also possible to overwrite the private \verb{$.dataloader()} method, which otherwise calls \verb{$.dataset()} and
creates a dataloader from that dataset. When doing so, it is important to respect the parameter \code{shuffle}, because
this method is used to ceate the dataloader for prediction as well.

While it is possible to add parameters by specifying the \code{param_set} construction argument, it is currently
not possible to change these parameters.
Note that none of the parameters provided in \code{param_set} can have an id that starts with \code{"loss."}, \verb{"opt.", or }"cb."`, as these are preserved for the dynamically constructed parameters of the optimizer and the loss
function.
}

\seealso{
Other Learners: 
\code{\link{mlr_learners_classif.torch_featureless}},
\code{\link{mlr_learners_classif.torch}}
}
\concept{Learners}
