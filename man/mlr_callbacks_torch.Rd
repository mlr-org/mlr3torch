% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/CallbackTorch.R
\name{mlr_callbacks_torch}
\alias{mlr_callbacks_torch}
\alias{CallbackTorch}
\title{Base Class for Torch Callbacks}
\description{
Base class from which Callbacks should inherit.
They can be used to gain more control over the training process of a neural network without
having to write everything from scratch.

For each available stage (see section \emph{Stages}) a public method \verb{$on_<stage>()} can be defined.
The evaluation context (a \code{\link{ContextTorch}}) can be accessed via \code{self$ctx}, which contains
the current state of the training loop.
This context is assigned at the beginning of the training loop and removed afterwards.
Different stages of a callback can communicate with each other by assigning values to \verb{$self}.

When used a in torch learner, the \code{CallbackTorch} is wrapped in a \code{\link{DescriptorTorchCallback}}.
The latters parameter set represents the arguments of the \code{\link{CallbackTorch}}'s \verb{$initialize()} method and can
be specified in the learner. The callback is then initialized at the beginning of the training loop.

For creating custom callbacks, the function \code{\link[=callback_descriptor]{callback_descriptor()}} is recommended, which creates a
\code{\link{CallbackTorch}} and then wraps it in a \code{\link{DescriptorTorchCallback}}.
}
\section{Inheriting}{

When one inherits from this class and overwrites the private \verb{$deep_clone()} method, it is
important to ensure that a deep clone can only be created if the \code{ctx} field is \code{NULL},
as this should never be cloned.
Furthermore, it is convenient to set \code{lock_objects} to \code{FALSE} to be able to freely assign variables
to \code{self}.
}

\section{Stages}{

\itemize{
\item \code{begin} :: Run before the training loop begins.
\item \code{epoch_begin} :: Run he beginning of each epoch.
\item \code{before_validation} :: Run before each validation loop.
\item \code{batch_begin} :: Run before the forward call.
\item \code{after_backward} :: Run after the backward call.
\item \code{batch_end} :: Run after the optimizer step.
\item \code{batch_valid_begin} :: Run before the forward call in the validation loop.
\item \code{batch_valid_end} :: Run after the forward call in the validation loop.
\item \code{epoch_end} :: Run at the end of each epoch.
\item \code{end} :: Run at last, using \code{on.exit()}.
}
}

\seealso{
Other Callback: 
\code{\link{DescriptorTorchCallback}},
\code{\link{as_descriptor_torch_callbacks}()},
\code{\link{as_descriptor_torch_callback}()},
\code{\link{callback_descriptor}()},
\code{\link{callback_torch}()},
\code{\link{mlr3torch_callbacks}},
\code{\link{mlr_callbacks_torch.checkpoint}},
\code{\link{mlr_callbacks_torch.progress}},
\code{\link{mlr_context_torch}},
\code{\link{t_clbk}()}
}
\concept{Callback}
\section{Public fields}{
\if{html}{\out{<div class="r6-fields">}}
\describe{
\item{\code{ctx}}{(\code{\link{TorchContext}} or \code{NULL})\cr
The evaluation context for the callback.
This field should always be \code{NULL} except during the \verb{$train()} call of the torch learner.}
}
\if{html}{\out{</div>}}
}
\section{Methods}{
\subsection{Public methods}{
\itemize{
\item \href{#method-CallbackTorch-clone}{\code{CallbackTorch$clone()}}
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-CallbackTorch-clone"></a>}}
\if{latex}{\out{\hypertarget{method-CallbackTorch-clone}{}}}
\subsection{Method \code{clone()}}{
The objects of this class are cloneable with this method.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{CallbackTorch$clone(deep = FALSE)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{deep}}{Whether to make a deep clone.}
}
\if{html}{\out{</div>}}
}
}
}
