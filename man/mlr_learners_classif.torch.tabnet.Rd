% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/classif.torch.tabnet.R
\name{mlr_learners_classif.torch.tabnet}
\alias{mlr_learners_classif.torch.tabnet}
\alias{LearnerClassifTorchTabnet}
\title{Classification TabNet Learner}
\description{
Calls \link[tabnet:tabnet]{tabnet::tabnet} from package \CRANpkg{tabnet}.
}
\section{Dictionary}{
 This \link{Learner} can be instantiated via the
\link[mlr3misc:Dictionary]{dictionary} \link{mlr_learners} or with the associated
sugar function \code{\link[=lrn]{lrn()}}:\preformatted{mlr_learners$get("classif.torch.tabnet")
lrn("classif.torch.tabnet")
}
}

\section{Meta Information}{

\itemize{
\item Task type: \dQuote{classif}
\item Predict Types: \dQuote{response}, \dQuote{prob}
\item Feature Types: \dQuote{logical}, \dQuote{integer}, \dQuote{numeric}, \dQuote{factor}, \dQuote{ordered}
\item Required Packages: \CRANpkg{mlr3}, \CRANpkg{tabnet}
}
}

\section{Parameters}{
\tabular{lllll}{
   Id \tab Type \tab Default \tab Levels \tab Range \cr
   num_threads \tab integer \tab 1 \tab  \tab \eqn{[1, \infty)}{[1, Inf)} \cr
   batch_size \tab integer \tab 256 \tab  \tab \eqn{[1, \infty)}{[1, Inf)} \cr
   penalty \tab numeric \tab 0.001 \tab  \tab \eqn{(-\infty, \infty)}{(-Inf, Inf)} \cr
   clip_value \tab untyped \tab  \tab  \tab - \cr
   loss \tab character \tab auto \tab auto, mse, cross_entropy \tab - \cr
   epochs \tab integer \tab 5 \tab  \tab \eqn{[1, \infty)}{[1, Inf)} \cr
   drop_last \tab logical \tab FALSE \tab TRUE, FALSE \tab - \cr
   decision_width \tab integer \tab 8 \tab  \tab \eqn{[1, \infty)}{[1, Inf)} \cr
   attention_width \tab integer \tab 8 \tab  \tab \eqn{[1, \infty)}{[1, Inf)} \cr
   num_steps \tab integer \tab 3 \tab  \tab \eqn{[1, \infty)}{[1, Inf)} \cr
   feature_reusage \tab numeric \tab 1.3 \tab  \tab \eqn{[0, \infty)}{[0, Inf)} \cr
   mask_type \tab character \tab sparsemax \tab sparsemax, entmax \tab - \cr
   virtual_batch_size \tab integer \tab 128 \tab  \tab \eqn{[1, \infty)}{[1, Inf)} \cr
   valid_split \tab numeric \tab 0 \tab  \tab \eqn{[0, 1]}{[0, 1]} \cr
   learn_rate \tab numeric \tab 0.02 \tab  \tab \eqn{[0, 1]}{[0, 1]} \cr
   optimizer \tab untyped \tab adam \tab  \tab - \cr
   lr_scheduler \tab untyped \tab  \tab  \tab - \cr
   lr_decay \tab numeric \tab 0.1 \tab  \tab \eqn{[0, 1]}{[0, 1]} \cr
   step_size \tab integer \tab 30 \tab  \tab \eqn{[1, \infty)}{[1, Inf)} \cr
   checkpoint_epochs \tab integer \tab 10 \tab  \tab \eqn{[1, \infty)}{[1, Inf)} \cr
   cat_emb_dim \tab integer \tab 1 \tab  \tab \eqn{[0, \infty)}{[0, Inf)} \cr
   num_independent \tab integer \tab 2 \tab  \tab \eqn{[0, \infty)}{[0, Inf)} \cr
   num_shared \tab integer \tab 2 \tab  \tab \eqn{[0, \infty)}{[0, Inf)} \cr
   momentum \tab numeric \tab 0.02 \tab  \tab \eqn{[0, 1]}{[0, 1]} \cr
   pretraining_ratio \tab numeric \tab 0.5 \tab  \tab \eqn{[0, 1]}{[0, 1]} \cr
   verbose \tab logical \tab FALSE \tab TRUE, FALSE \tab - \cr
   device \tab character \tab auto \tab auto, cpu, cuda \tab - \cr
   importance_sample_size \tab integer \tab - \tab  \tab \eqn{[0, 1e+05]}{[0, 1e+05]} \cr
}
}

\examples{
\dontrun{
library(mlr3)
library(mlr3torch)
task <- tsk("german_credit")
lrn <- lrn("classif.torch.tabnet")

lrn$param_set$values$epochs <- 10
lrn$param_set$values$attention_width <- 8
lrn$train(task)
}
}
\references{
Arik, S. O. & Pfister, T. TabNet: Attentive Interpretable Tabular Learning. arXiv:1908.07442 [cs, stat] (2020).
}
\seealso{
\itemize{
\item \link[mlr3misc:Dictionary]{Dictionary} of \link[mlr3:Learner]{Learners}: \link[mlr3:mlr_learners]{mlr3::mlr_learners}.
\item \code{as.data.table(mlr_learners)} for a table of available \link[=Learner]{Learners} in the running session (depending on the loaded packages).
\item Chapter in the \href{https://mlr3book.mlr-org.com/}{mlr3book}: \url{https://mlr3book.mlr-org.com/basics.html#learners}
\item \CRANpkg{mlr3learners} for a selection of recommended learners.
\item \CRANpkg{mlr3cluster} for unsupervised clustering learners.
\item \CRANpkg{mlr3pipelines} to combine learners with pre- and postprocessing steps.
\item \CRANpkg{mlr3tuning} for tuning of hyperparameters, \CRANpkg{mlr3tuningspaces} for established default tuning spaces.
}
}
\author{
Lukas Burk
}
\section{Super classes}{
\code{\link[mlr3:Learner]{mlr3::Learner}} -> \code{\link[mlr3:LearnerClassif]{mlr3::LearnerClassif}} -> \code{LearnerClassifTorchTabnet}
}
\section{Methods}{
\subsection{Public methods}{
\itemize{
\item \href{#method-new}{\code{LearnerClassifTorchTabnet$new()}}
\item \href{#method-importance}{\code{LearnerClassifTorchTabnet$importance()}}
\item \href{#method-clone}{\code{LearnerClassifTorchTabnet$clone()}}
}
}
\if{html}{
\out{<details ><summary>Inherited methods</summary>}
\itemize{
\item \out{<span class="pkg-link" data-pkg="mlr3" data-topic="Learner" data-id="base_learner">}\href{../../mlr3/html/Learner.html#method-base_learner}{\code{mlr3::Learner$base_learner()}}\out{</span>}
\item \out{<span class="pkg-link" data-pkg="mlr3" data-topic="Learner" data-id="format">}\href{../../mlr3/html/Learner.html#method-format}{\code{mlr3::Learner$format()}}\out{</span>}
\item \out{<span class="pkg-link" data-pkg="mlr3" data-topic="Learner" data-id="help">}\href{../../mlr3/html/Learner.html#method-help}{\code{mlr3::Learner$help()}}\out{</span>}
\item \out{<span class="pkg-link" data-pkg="mlr3" data-topic="Learner" data-id="predict">}\href{../../mlr3/html/Learner.html#method-predict}{\code{mlr3::Learner$predict()}}\out{</span>}
\item \out{<span class="pkg-link" data-pkg="mlr3" data-topic="Learner" data-id="predict_newdata">}\href{../../mlr3/html/Learner.html#method-predict_newdata}{\code{mlr3::Learner$predict_newdata()}}\out{</span>}
\item \out{<span class="pkg-link" data-pkg="mlr3" data-topic="Learner" data-id="print">}\href{../../mlr3/html/Learner.html#method-print}{\code{mlr3::Learner$print()}}\out{</span>}
\item \out{<span class="pkg-link" data-pkg="mlr3" data-topic="Learner" data-id="reset">}\href{../../mlr3/html/Learner.html#method-reset}{\code{mlr3::Learner$reset()}}\out{</span>}
\item \out{<span class="pkg-link" data-pkg="mlr3" data-topic="Learner" data-id="train">}\href{../../mlr3/html/Learner.html#method-train}{\code{mlr3::Learner$train()}}\out{</span>}
}
\out{</details>}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-new"></a>}}
\if{latex}{\out{\hypertarget{method-new}{}}}
\subsection{Method \code{new()}}{
Creates a new instance of this \link[R6:R6Class]{R6} class.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{LearnerClassifTorchTabnet$new()}\if{html}{\out{</div>}}
}

}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-importance"></a>}}
\if{latex}{\out{\hypertarget{method-importance}{}}}
\subsection{Method \code{importance()}}{
The importance scores are extracted from the slot \code{.$model$fit$importances}.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{LearnerClassifTorchTabnet$importance()}\if{html}{\out{</div>}}
}

\subsection{Returns}{
Named \code{numeric()}.
}
}
\if{html}{\out{<hr>}}
\if{html}{\out{<a id="method-clone"></a>}}
\if{latex}{\out{\hypertarget{method-clone}{}}}
\subsection{Method \code{clone()}}{
The objects of this class are cloneable with this method.
\subsection{Usage}{
\if{html}{\out{<div class="r">}}\preformatted{LearnerClassifTorchTabnet$clone(deep = FALSE)}\if{html}{\out{</div>}}
}

\subsection{Arguments}{
\if{html}{\out{<div class="arguments">}}
\describe{
\item{\code{deep}}{Whether to make a deep clone.}
}
\if{html}{\out{</div>}}
}
}
}
