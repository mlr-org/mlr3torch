<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Lazy Tensor • mlr3torch</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png">
<script src="../lightswitch.js"></script><script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/Roboto-0.4.9/font.css" rel="stylesheet">
<link href="../deps/JetBrains_Mono-0.4.9/font.css" rel="stylesheet">
<link href="../deps/Roboto_Slab-0.4.9/font.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><script src="../deps/MathJax-3.2.2/tex-chtml.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Lazy Tensor">
<meta name="robots" content="noindex">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top " aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">mlr3torch</a>

    <small class="nav-text text-default me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="In-development version">0.2.1-9000</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../reference/index.html"><span class="fa fa-file-alt"></span> Reference</a></li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><h6 class="dropdown-header" data-toc-skip>Tutorials</h6></li>
    <li><a class="dropdown-item" href="../articles/get_started.html">Get Started</a></li>
    <li><a class="dropdown-item" href="../articles/pipeop_torch.html">Defining an Architecture</a></li>
    <li><a class="dropdown-item" href="../articles/callbacks.html">Callbacks</a></li>
    <li><a class="dropdown-item" href="../articles/lazy_tensor.html">Non-Tabular Data</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>Internals</h6></li>
    <li><a class="dropdown-item" href="../articles/internals_pipeop_torch.html">Networks as Graphs</a></li>
  </ul>
</li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-overview" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Overview</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-overview">
<li><a class="dropdown-item" href="../articles/task_list.html">Tasks</a></li>
    <li><a class="dropdown-item" href="../articles/preprocessing_list.html">Preprocessing &amp; Augmentation</a></li>
    <li><a class="dropdown-item" href="../articles/optimizer_list.html">Optimizers</a></li>
    <li><a class="dropdown-item" href="../articles/loss_list.html">Losses</a></li>
    <li><a class="dropdown-item" href="../articles/callback_list.html">Callbacks</a></li>
    <li><a class="dropdown-item" href="../articles/learner_list.html">Learners</a></li>
    <li><a class="dropdown-item" href="../articles/layer_list.html">Network Layers</a></li>
  </ul>
</li>
<li class="nav-item"><a class="external-link nav-link" href="https://mlr3book.mlr-org.com"><span class="fa fa-link"></span> mlr3book</a></li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/mlr-org/mlr3torch"><span class="fa fa-github"></span></a></li>
<li class="nav-item"><a class="external-link nav-link" href="https://lmmisld-lmu-stats-slds.srv.mwn.de/mlr_invite/"><span class="fa fa-comments"></span></a></li>
<li class="nav-item"><a class="external-link nav-link" href="https://stackoverflow.com/questions/tagged/mlr3"><span class="fa fab fa-stack-overflow"></span></a></li>
<li class="nav-item"><a class="external-link nav-link" href="https://mlr-org.com/"><span class="fa fa-rss"></span></a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-lightswitch" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true" aria-label="Light switch"><span class="fa fa-sun"></span></button>
  <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="dropdown-lightswitch">
<li><button class="dropdown-item" data-bs-theme-value="light"><span class="fa fa-sun"></span> Light</button></li>
    <li><button class="dropdown-item" data-bs-theme-value="dark"><span class="fa fa-moon"></span> Dark</button></li>
    <li><button class="dropdown-item" data-bs-theme-value="auto"><span class="fa fa-adjust"></span> Auto</button></li>
  </ul>
</li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.svg" class="logo" alt=""><h1>Lazy Tensor</h1>
            
      
      <small class="dont-index">Source: <a href="https://github.com/mlr-org/mlr3torch/blob/main/vignettes/articles/lazy_tensor.Rmd" class="external-link"><code>vignettes/articles/lazy_tensor.Rmd</code></a></small>
      <div class="d-none name"><code>lazy_tensor.Rmd</code></div>
    </div>

    
    
<p>This vignette introduces the <code>lazy_tensor</code> class, which is
a vector type that can be used to lazily represent torch tensors of
arbitary dimensions. Among other things, it allows
<code>mlr3torch</code> to work with images, which we will illustrate
using the predefined MNIST task, which has one feature
<code>image</code> of class <code>"lazy_tensor"</code>. The images
display the digits 0, …, 9, and the goal is to classify them
correctly.</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://mlr3torch.mlr-org.com/">mlr3torch</a></span><span class="op">)</span></span>
<span></span>
<span><span class="va">mnist</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html" class="external-link">tsk</a></span><span class="op">(</span><span class="st">"mnist"</span><span class="op">)</span></span>
<span><span class="va">mnist</span></span>
<span><span class="co">#&gt; &lt;TaskClassif:mnist&gt; (70000 x 2): MNIST Digit Classification</span></span>
<span><span class="co">#&gt; * Target: label</span></span>
<span><span class="co">#&gt; * Properties: multiclass</span></span>
<span><span class="co">#&gt; * Features (1):</span></span>
<span><span class="co">#&gt;   - lt (1): image</span></span></code></pre></div>
<p>The name <code>"lazy_tensor"</code> stems from the fact, that the
tensors are not necessarily stored in memory, as this is often
impossible when working with large image datasets.<br>
Therefore, we can easily access the data without any expensive
data-loading. We see that the data contains one column <em>label</em>,
which is the target variable, and an <code>image</code> which is the
input feature.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">mnist</span><span class="op">$</span><span class="fu">head</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="co">#&gt; Processing...</span></span>
<span><span class="co">#&gt; Done!</span></span>
<span><span class="co">#&gt;     label           image</span></span>
<span><span class="co">#&gt;    &lt;fctr&gt;   &lt;lazy_tensor&gt;</span></span>
<span><span class="co">#&gt; 1:      5 &lt;tnsr[1x28x28]&gt;</span></span>
<span><span class="co">#&gt; 2:      0 &lt;tnsr[1x28x28]&gt;</span></span>
<span><span class="co">#&gt; 3:      4 &lt;tnsr[1x28x28]&gt;</span></span>
<span><span class="co">#&gt; 4:      1 &lt;tnsr[1x28x28]&gt;</span></span>
<span><span class="co">#&gt; 5:      9 &lt;tnsr[1x28x28]&gt;</span></span>
<span><span class="co">#&gt; 6:      2 &lt;tnsr[1x28x28]&gt;</span></span></code></pre></div>
<p>If we wanted to obtain the actual tensors representing the images, we
can do so by calling <code><a href="../reference/materialize.html">materialize()</a></code>, which will return a
list of <code>torch_tensor</code>s, not necessarily all with the same
shape. Here, we only show a slice of the tensor for readability.</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">lt</span> <span class="op">=</span> <span class="va">mnist</span><span class="op">$</span><span class="fu">data</span><span class="op">(</span>cols <span class="op">=</span> <span class="st">"image"</span><span class="op">)</span><span class="op">[[</span><span class="fl">1L</span><span class="op">]</span><span class="op">]</span></span>
<span><span class="fu"><a href="../reference/materialize.html">materialize</a></span><span class="op">(</span><span class="va">lt</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span><span class="op">)</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span><span class="op">[</span><span class="fl">1</span>, <span class="fl">12</span><span class="op">:</span><span class="fl">16</span>, <span class="fl">12</span><span class="op">:</span><span class="fl">16</span><span class="op">]</span></span>
<span><span class="co">#&gt; torch_tensor</span></span>
<span><span class="co">#&gt;  139  253  190    2    0</span></span>
<span><span class="co">#&gt;   11  190  253   70    0</span></span>
<span><span class="co">#&gt;    0   35  241  225  160</span></span>
<span><span class="co">#&gt;    0    0   81  240  253</span></span>
<span><span class="co">#&gt;    0    0    0   45  186</span></span>
<span><span class="co">#&gt; [ CPUFloatType{5,5} ]</span></span></code></pre></div>
<p>If all elements have the same shape as is the case here, we could
also obtain a single <code>torch_tensor</code> by specifying
<code>rbind = TRUE</code>.</p>
<p>In order to train a <code>Learner</code> on a <code>Task</code>
containing <code>lazy_tensor</code> columns it must support the
<code>lazy_tensor</code> feature type, as is the case for the multi
layer perceptron, which works both with <code>numeric</code> types, as
well as the <code>lazy_tensor</code>.</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">mlp</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html" class="external-link">lrn</a></span><span class="op">(</span><span class="st">"classif.mlp"</span>,</span>
<span>  neurons <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">100</span>, <span class="fl">100</span><span class="op">)</span>,</span>
<span>  epochs <span class="op">=</span> <span class="fl">10</span>, batch_size <span class="op">=</span> <span class="fl">32</span></span>
<span><span class="op">)</span></span>
<span><span class="va">mlp</span></span>
<span><span class="co">#&gt; &lt;LearnerTorchMLP:classif.mlp&gt;: Multi Layer Perceptron</span></span>
<span><span class="co">#&gt; * Model: -</span></span>
<span><span class="co">#&gt; * Parameters: epochs=10, device=auto, num_threads=1,</span></span>
<span><span class="co">#&gt;   num_interop_threads=1, seed=random, jit_trace=FALSE, eval_freq=1,</span></span>
<span><span class="co">#&gt;   measures_train=&lt;list&gt;, measures_valid=&lt;list&gt;, patience=0,</span></span>
<span><span class="co">#&gt;   min_delta=0, batch_size=32, shuffle=TRUE, tensor_dataset=FALSE,</span></span>
<span><span class="co">#&gt;   neurons=100,100, p=0.5, activation=&lt;nn_relu&gt;, activation_args=&lt;list&gt;</span></span>
<span><span class="co">#&gt; * Validate: NULL</span></span>
<span><span class="co">#&gt; * Packages: mlr3, mlr3torch, torch</span></span>
<span><span class="co">#&gt; * Predict Types:  [response], prob</span></span>
<span><span class="co">#&gt; * Feature Types: integer, numeric, lazy_tensor</span></span>
<span><span class="co">#&gt; * Properties: internal_tuning, marshal, multiclass, twoclass,</span></span>
<span><span class="co">#&gt;   validation</span></span>
<span><span class="co">#&gt; * Optimizer: adam</span></span>
<span><span class="co">#&gt; * Loss: cross_entropy</span></span>
<span><span class="co">#&gt; * Callbacks: -</span></span></code></pre></div>
<p>However, because <code>lazy_tensor</code>s also have a specific
shape, we also must ensure that the shape of the
<code>lazy_tensor</code> matches the expected input shape of the
learner. The multi layer perceptron expects a 2d-tensor where the first
dimension is the batch dimension. But above we have seen that this is
not the case for MNIST, where each element has shape
<code>(1, 28, 28)</code>. Therefore, we need to flatten the
<code>lazy_tensor</code>, which we here do using
<code>po("trafo_reshape")</code>:</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">reshaper</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3pipelines.mlr-org.com/reference/po.html" class="external-link">po</a></span><span class="op">(</span><span class="st">"trafo_reshape"</span>, shape <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="op">-</span><span class="fl">1</span>, <span class="fl">28</span> <span class="op">*</span> <span class="fl">28</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">mnist_flat</span> <span class="op">=</span> <span class="va">reshaper</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="va">mnist</span><span class="op">)</span><span class="op">)</span><span class="op">[[</span><span class="fl">1L</span><span class="op">]</span><span class="op">]</span></span>
<span><span class="va">mnist_flat</span><span class="op">$</span><span class="fu">head</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="co">#&gt;     label         image</span></span>
<span><span class="co">#&gt;    &lt;fctr&gt; &lt;lazy_tensor&gt;</span></span>
<span><span class="co">#&gt; 1:      5   &lt;tnsr[784]&gt;</span></span>
<span><span class="co">#&gt; 2:      0   &lt;tnsr[784]&gt;</span></span>
<span><span class="co">#&gt; 3:      4   &lt;tnsr[784]&gt;</span></span>
<span><span class="co">#&gt; 4:      1   &lt;tnsr[784]&gt;</span></span>
<span><span class="co">#&gt; 5:      9   &lt;tnsr[784]&gt;</span></span>
<span><span class="co">#&gt; 6:      2   &lt;tnsr[784]&gt;</span></span></code></pre></div>
<p>Note that this does not <em>actually</em> reshape all the tensors
in-memory, this will again only happen once <code><a href="../reference/materialize.html">materialize()</a></code>
is called.</p>
<p>We can now proceed to train the a simple multi-layer perceptron on
the flattened mnist task:</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">mlp</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html" class="external-link">lrn</a></span><span class="op">(</span><span class="st">"classif.mlp"</span>,</span>
<span>  neurons <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">100</span>, <span class="fl">100</span><span class="op">)</span>,</span>
<span>  epochs <span class="op">=</span> <span class="fl">10</span>, batch_size <span class="op">=</span> <span class="fl">32</span></span>
<span><span class="op">)</span></span>
<span><span class="va">mlp</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span><span class="va">mnist_flat</span><span class="op">)</span></span></code></pre></div>
<div class="section level2">
<h2 id="creating-a-lazy-tensor">Creating a Lazy Tensor<a class="anchor" aria-label="anchor" href="#creating-a-lazy-tensor"></a>
</h2>
<p>Every <code>lazy_tensor</code> is built on top of a
<code><a href="https://torch.mlverse.org/docs/reference/dataset.html" class="external-link">torch::dataset</a></code>, so we here assume that you are familiar
with it. For more information on how to create
<code><a href="https://torch.mlverse.org/docs/reference/dataset.html" class="external-link">torch::dataset</a></code>s, we recommend reading the <a href="https://torch.mlverse.org/" class="external-link">torch package documentation</a>. The
only additional restriction that we impose on the dataset is that it
must have a <code>.getitem</code> or <code>.getbatch</code> method that
returns a list of named tensors.</p>
<p>As an example, we will create a <code>lazy_tensor</code> of length
1000, whose elements are drawn from a uniform distribution over <span class="math inline">\([0, 1]\)</span>. While the data is stored
in-memory in this example, this is not necessary and the
<code>$.getitem()</code> method can e.g. load images from disk.</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">mydata</span> <span class="op">=</span> <span class="fu"><a href="https://torch.mlverse.org/docs/reference/dataset.html" class="external-link">dataset</a></span><span class="op">(</span></span>
<span>  initialize <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="va">self</span><span class="op">$</span><span class="va">x</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/stats/Uniform.html" class="external-link">runif</a></span><span class="op">(</span><span class="fl">1000</span>, <span class="op">-</span><span class="fl">1</span>, <span class="fl">1</span><span class="op">)</span></span>
<span>  <span class="op">}</span>,</span>
<span>  .getbatch <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">i</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>x <span class="op">=</span> <span class="fu"><a href="https://torch.mlverse.org/docs/reference/torch_tensor.html" class="external-link">torch_tensor</a></span><span class="op">(</span><span class="va">self</span><span class="op">$</span><span class="va">x</span><span class="op">[</span><span class="va">i</span><span class="op">]</span><span class="op">)</span><span class="op">$</span><span class="fu">unsqueeze</span><span class="op">(</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span>,</span>
<span>  .length <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="op">)</span> <span class="fl">1000</span></span>
<span><span class="op">)</span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<p>In order to create a <code>lazy_tensor</code> from
<code>mydata</code>, we have to annotate the returned shapes of the
dataset by passing a named list to <code>dataset_shapes</code>. The
first dimension must be <code>NA</code>, as it is the batch dimension.
We can also set a shape to <code>NULL</code> to indicate that it is
unknown, i.e. it varies between elements.</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">lt</span> <span class="op">=</span> <span class="fu"><a href="../reference/as_lazy_tensor.html">as_lazy_tensor</a></span><span class="op">(</span><span class="va">mydata</span>, dataset_shapes <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="cn">NA</span>, <span class="fl">1</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">lt</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">5</span><span class="op">]</span></span>
<span><span class="co">#&gt; &lt;ltnsr[5]&gt;</span></span>
<span><span class="co">#&gt; [1] &lt;tnsr[1]&gt; &lt;tnsr[1]&gt; &lt;tnsr[1]&gt; &lt;tnsr[1]&gt; &lt;tnsr[1]&gt;</span></span></code></pre></div>
<p>Note that in this case, because we implemented the
<code>.getbatch</code> method, we could have even omitted specifying the
<code>dataset_shapes</code> as they could have been auto-inferred.</p>
<p>We can convert this vector to a <code>torch_tensor</code> just like
before:</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/materialize.html">materialize</a></span><span class="op">(</span><span class="va">lt</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">5</span><span class="op">]</span>, rbind <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="co">#&gt; torch_tensor</span></span>
<span><span class="co">#&gt; -0.9852</span></span>
<span><span class="co">#&gt; -0.0672</span></span>
<span><span class="co">#&gt; -0.0044</span></span>
<span><span class="co">#&gt; -0.4205</span></span>
<span><span class="co">#&gt;  0.4658</span></span>
<span><span class="co">#&gt; [ CPUFloatType{5,1} ]</span></span></code></pre></div>
<p>Because we added no preprocessing, this is the same as calling the
<code>$.getbatch()</code> method on <code>mydata</code> and selecting
the element <code>x</code>.</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://torch.mlverse.org/docs/reference/torch_equal.html" class="external-link">torch_equal</a></span><span class="op">(</span></span>
<span>  <span class="fu"><a href="../reference/materialize.html">materialize</a></span><span class="op">(</span><span class="va">lt</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>, rbind <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span>,</span>
<span>  <span class="va">mydata</span><span class="op">$</span><span class="fu">.getbatch</span><span class="op">(</span><span class="fl">1</span><span class="op">)</span><span class="op">$</span><span class="va">x</span></span>
<span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] TRUE</span></span></code></pre></div>
<p>We continue with creating an example task from <code>lt</code>, where
the relationship between the <code>x</code> and <code>y</code> variable
is polynomial. Note that the target variable, both for classification
and regression, cannot be a <code>lazy_tensor</code>, but must be a
<code>factor</code> or <code>numeric</code> respectively.</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://r-datatable.com" class="external-link">data.table</a></span><span class="op">)</span></span>
<span><span class="va">x</span> <span class="op">=</span> <span class="va">mydata</span><span class="op">$</span><span class="va">x</span></span>
<span><span class="va">y</span> <span class="op">=</span> <span class="fl">0.2</span> <span class="op">+</span> <span class="fl">0.1</span> <span class="op">*</span> <span class="va">x</span> <span class="op">-</span> <span class="fl">0.1</span> <span class="op">*</span> <span class="va">x</span><span class="op">^</span><span class="fl">2</span> <span class="op">-</span> <span class="fl">0.3</span> <span class="op">*</span> <span class="va">x</span><span class="op">^</span><span class="fl">3</span> <span class="op">+</span> <span class="fl">0.5</span> <span class="op">*</span> <span class="va">x</span><span class="op">^</span><span class="fl">4</span> <span class="op">+</span> <span class="fl">0.5</span> <span class="op">*</span> <span class="va">x</span><span class="op">^</span><span class="fl">7</span> <span class="op">+</span> <span class="fl">0.6</span> <span class="op">*</span> <span class="va">x</span><span class="op">^</span><span class="fl">11</span> <span class="op">+</span> <span class="fu"><a href="https://rdrr.io/r/stats/Normal.html" class="external-link">rnorm</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/length.html" class="external-link">length</a></span><span class="op">(</span><span class="va">mydata</span><span class="op">)</span><span class="op">)</span> <span class="op">*</span> <span class="fl">0.1</span></span>
<span><span class="va">dt</span> <span class="op">=</span> <span class="fu"><a href="https://rdatatable.gitlab.io/data.table/reference/data.table.html" class="external-link">data.table</a></span><span class="op">(</span>y <span class="op">=</span> <span class="va">y</span>, x <span class="op">=</span> <span class="va">lt</span><span class="op">)</span></span>
<span><span class="va">task_poly</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/as_task_regr.html" class="external-link">as_task_regr</a></span><span class="op">(</span><span class="va">dt</span>, target <span class="op">=</span> <span class="st">"y"</span>, id <span class="op">=</span> <span class="st">"poly"</span><span class="op">)</span></span>
<span><span class="va">task_poly</span></span>
<span><span class="co">#&gt; &lt;TaskRegr:poly&gt; (1000 x 2)</span></span>
<span><span class="co">#&gt; * Target: y</span></span>
<span><span class="co">#&gt; * Properties: -</span></span>
<span><span class="co">#&gt; * Features (1):</span></span>
<span><span class="co">#&gt;   - lt (1): x</span></span></code></pre></div>
<p>Below, we plot the data:</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://ggplot2.tidyverse.org" class="external-link">ggplot2</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html" class="external-link">ggplot</a></span><span class="op">(</span>data <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html" class="external-link">data.frame</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span>, y <span class="op">=</span> <span class="va">y</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html" class="external-link">geom_point</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html" class="external-link">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span>, y <span class="op">=</span> <span class="va">y</span><span class="op">)</span>, alpha <span class="op">=</span> <span class="fl">0.5</span><span class="op">)</span></span></code></pre></div>
<p><img src="lazy_tensor_files/figure-html/unnamed-chunk-14-1.png" width="2187.5" style="display: block; margin: auto;"></p>
<p>In the next section, we will create a custom <code>PipeOp</code> to
fit a polynomial regression model.</p>
</div>
<div class="section level2">
<h2 id="custom-preprocessing">Custom Preprocessing<a class="anchor" aria-label="anchor" href="#custom-preprocessing"></a>
</h2>
<p>In order to create a custom preprocessing operator for a lazy tensor,
we have to create a new <code>PipeOp</code> class. To make this as
convenient as possible, <code>mlr3torch</code> offters a
<code><a href="../reference/pipeop_preproc_torch.html">pipeop_preproc_torch()</a></code> function that we recommend using for
this purpose. Its most important arguments are:</p>
<ul>
<li>
<code>id</code> - Used as the default identifier of the
<code>PipeOp</code>
</li>
<li>
<code>fn</code> - The preprocessing function. By default, the first
argument is assumed to be the <code>torch_tensor</code> and the
remaining arguments will be part of the <code>PipeOp</code>’s parameter
set.</li>
<li>
<code>shapes_out</code> - A function that returns the shapes of the
output tensors given the input shapes. This can also be set to
<code>NULL</code> for an unknown shape or to <code>"infer"</code> for
auto-inference, see <code><a href="../reference/pipeop_preproc_torch.html">?pipeop_preproc_torch</a></code> for more
information.</li>
</ul>
<p>Below, we create a <code>PipeOp</code>, that transforms a vector
<code>x</code> into a matrix <span class="math inline">\((x^{d_1} ...,
x^{d_n})\)</span>, where <span class="math inline">\(d\)</span> is the
<code>degrees</code> parameter of the <code>PipeOp</code>.</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">PipeOpPreprocTorchPoly</span> <span class="op">=</span> <span class="fu"><a href="../reference/pipeop_preproc_torch.html">pipeop_preproc_torch</a></span><span class="op">(</span><span class="st">"poly"</span>,</span>
<span>  fn <span class="op">=</span> <span class="kw">function</span><span class="op">(</span><span class="va">x</span>, <span class="va">degrees</span><span class="op">)</span> <span class="op">{</span></span>
<span>    <span class="fu"><a href="https://torch.mlverse.org/docs/reference/torch_cat.html" class="external-link">torch_cat</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/lapply.html" class="external-link">lapply</a></span><span class="op">(</span><span class="va">degrees</span>, <span class="kw">function</span><span class="op">(</span><span class="va">d</span><span class="op">)</span> <span class="fu"><a href="https://torch.mlverse.org/docs/reference/torch_pow.html" class="external-link">torch_pow</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">d</span><span class="op">)</span><span class="op">)</span>, dim <span class="op">=</span> <span class="fl">2L</span><span class="op">)</span></span>
<span>  <span class="op">}</span>,</span>
<span>  shapes_out <span class="op">=</span> <span class="st">"infer"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>We can now create a new instance of this <code>PipeOp</code> by
calling <code>$new()</code>, and we set the parameter
<code>degrees</code> to those degrees that were used when simulating the
data above. Further, we set the parameter <code>stages</code>, that is
always available, to <code>"both"</code>, which means that the
preprocessing is applied during training and prediction. For data
augmentation this can be set to <code>"train"</code>.</p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">po_poly</span> <span class="op">=</span> <span class="va">PipeOpPreprocTorchPoly</span><span class="op">$</span><span class="fu">new</span><span class="op">(</span><span class="op">)</span></span>
<span></span>
<span><span class="va">po_poly</span><span class="op">$</span><span class="va">param_set</span><span class="op">$</span><span class="fu">set_values</span><span class="op">(</span></span>
<span>  degrees <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">1</span>, <span class="fl">2</span>, <span class="fl">3</span>, <span class="fl">4</span>, <span class="fl">7</span>, <span class="fl">11</span><span class="op">)</span>,</span>
<span>  stages <span class="op">=</span> <span class="st">"both"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>To create our polynomial regression learner, we combine the
polynomial preprocessor with a <code>lrn("regr.mlp")</code> with no
hidden layer (i.e. a linear model) and train the learner on the
task.</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">lrn_poly</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/as_learner.html" class="external-link">as_learner</a></span><span class="op">(</span></span>
<span>  <span class="va">po_poly</span> <span class="op"><a href="https://mlr3pipelines.mlr-org.com/reference/grapes-greater-than-greater-than-grapes.html" class="external-link">%&gt;&gt;%</a></span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html" class="external-link">lrn</a></span><span class="op">(</span><span class="st">"regr.mlp"</span>, batch_size <span class="op">=</span> <span class="fl">256</span>, epochs <span class="op">=</span> <span class="fl">100</span>,</span>
<span>  neurons <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/integer.html" class="external-link">integer</a></span><span class="op">(</span><span class="fl">0</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">lrn_poly</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span><span class="va">task_poly</span><span class="op">)</span></span>
<span><span class="va">pred</span> <span class="op">=</span> <span class="va">lrn_poly</span><span class="op">$</span><span class="fu">predict</span><span class="op">(</span><span class="va">task_poly</span><span class="op">)</span></span></code></pre></div>
<p>Below, we visualize the predictions and see that the model captured
the non-linear relationship reasonably:</p>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">dt</span> <span class="op">=</span> <span class="fu"><a href="https://rdatatable.gitlab.io/data.table/reference/melt.data.table.html" class="external-link">melt</a></span><span class="op">(</span><span class="fu"><a href="https://rdatatable.gitlab.io/data.table/reference/data.table.html" class="external-link">data.table</a></span><span class="op">(</span></span>
<span>  truth <span class="op">=</span> <span class="va">pred</span><span class="op">$</span><span class="va">truth</span>,</span>
<span>  response <span class="op">=</span> <span class="va">pred</span><span class="op">$</span><span class="va">response</span>,</span>
<span>  x <span class="op">=</span> <span class="va">x</span><span class="op">)</span>,</span>
<span>  id.vars <span class="op">=</span> <span class="st">"x"</span>, measure.vars <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"truth"</span>, <span class="st">"response"</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span><span class="va">dt</span><span class="op">$</span><span class="va">variable</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html" class="external-link">factor</a></span><span class="op">(</span><span class="va">dt</span><span class="op">$</span><span class="va">variable</span>, levels <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"truth"</span>, <span class="st">"response"</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html" class="external-link">ggplot</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">dt</span><span class="op">)</span> <span class="op">+</span></span>
<span>  <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html" class="external-link">geom_point</a></span><span class="op">(</span><span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html" class="external-link">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">x</span>, y <span class="op">=</span> <span class="va">value</span>, color <span class="op">=</span> <span class="va">variable</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p><img src="lazy_tensor_files/figure-html/unnamed-chunk-18-1.png" width="2187.5" style="display: block; margin: auto;"></p>
<p>In the next section, we will briefly cover the implementation details
of the <code>lazy_tensor</code>, which is not necessary to work with the
data-type, so feel free to skip this part.</p>
</div>
<div class="section level2">
<h2 id="digging-into-internals">Digging Into Internals<a class="anchor" aria-label="anchor" href="#digging-into-internals"></a>
</h2>
<p>Internally, the <code>lazy_tensor</code> vector uses the
<code>DataDescriptor</code> class to represent the (possibly)
preprocessed data. It is very similar to the
<code>ModelDescriptor</code> class that is used to build up neural
nerworks using <code>PipeOpTorch</code> objects. The
<code>DataDescriptor</code> stores a <code><a href="https://torch.mlverse.org/docs/reference/dataset.html" class="external-link">torch::dataset</a></code>, an
<code><a href="https://mlr3pipelines.mlr-org.com/reference/Graph.html" class="external-link">mlr3pipelines::Graph</a></code> and some metadata.</p>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">desc</span> <span class="op">=</span> <span class="va"><a href="../reference/DataDescriptor.html">DataDescriptor</a></span><span class="op">$</span><span class="fu">new</span><span class="op">(</span></span>
<span>  dataset <span class="op">=</span> <span class="va">mydata</span>,</span>
<span>  dataset_shapes <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span>x <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="cn">NA</span>, <span class="fl">1</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>Per default, the preprocessing graph contains only a single
<code>PipOpNop</code> that does nothing.</p>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">desc</span></span>
<span><span class="co">#&gt; &lt;DataDescriptor: 1 ops&gt;</span></span>
<span><span class="co">#&gt; * dataset_shapes: [x: (NA,1)]</span></span>
<span><span class="co">#&gt; * input_map: (x) -&gt; Graph</span></span>
<span><span class="co">#&gt; * pointer: nop.77752c.x.output</span></span>
<span><span class="co">#&gt; * shape: [(NA,1)]</span></span></code></pre></div>
<p>The printed output of the data descriptor informs us about:</p>
<ul>
<li>The number of <code>PipeOp</code>s contained in the preprocessing
graph</li>
<li>The output shapes of the dataset</li>
<li>The input map, i.e. how the data is passed to the preprocessing
graph, which is important when there are multiple inputs</li>
<li>The <code>pointer</code>, which points to a specific channel of an
output <code>PipeOp</code>. The output of this channel is the tensor
represented by the <code>DataDescriptor</code>. Note that the
<code>id</code> from the input <code>po("nop")</code> is randomly
generated, which is needed to prevent id clashes then there are more
than one input to the preprocessing graph.</li>
<li>The <code>shape</code>, which is the shape of the tensor at position
<code>pointer</code>
</li>
</ul>
<p>A lazy tensor can be constructed from an integer vector and a
<code>DataDescriptor</code>. The integer vector specifies which element
of the <code>DataDescriptor</code> the <code>lazy_tensor</code>
contains. Below, the first two elements of the <code>lazy_tensor</code>
vector represent the same element of the <code>DataDescriptor</code>,
while the third element represents a different element. Note that all
indices refer to the same <code>DataDescriptor</code>.</p>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">lt</span> <span class="op">=</span> <span class="fu"><a href="../reference/lazy_tensor.html">lazy_tensor</a></span><span class="op">(</span><span class="va">desc</span>, ids <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fl">1</span>, <span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="../reference/materialize.html">materialize</a></span><span class="op">(</span><span class="va">lt</span>, rbind <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="co">#&gt; torch_tensor</span></span>
<span><span class="co">#&gt; -0.9852</span></span>
<span><span class="co">#&gt; -0.9852</span></span>
<span><span class="co">#&gt; -0.0672</span></span>
<span><span class="co">#&gt; [ CPUFloatType{3,1} ]</span></span></code></pre></div>
<p>Internally, the lazy tensor is represented as a list of lists, each
element containing an id and a <code>DataDescriptor</code> Currently,
there can only be a single <code>DataDescriptor</code> in a
<code>lazy_tensor</code> vector.</p>
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/class.html" class="external-link">unclass</a></span><span class="op">(</span><span class="va">lt</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span><span class="op">)</span></span>
<span><span class="co">#&gt; [[1]]</span></span>
<span><span class="co">#&gt; [1] 1</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; [[2]]</span></span>
<span><span class="co">#&gt; &lt;DataDescriptor: 1 ops&gt;</span></span>
<span><span class="co">#&gt; * dataset_shapes: [x: (NA,1)]</span></span>
<span><span class="co">#&gt; * input_map: (x) -&gt; Graph</span></span>
<span><span class="co">#&gt; * pointer: nop.77752c.x.output</span></span>
<span><span class="co">#&gt; * shape: [(NA,1)]</span></span></code></pre></div>
<p>What happens during <code>materialize(lt[1])</code> is the
following:</p>
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># get index and data descriptor</span></span>
<span><span class="va">desc</span> <span class="op">=</span> <span class="va">lt</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span><span class="op">[[</span><span class="fl">2</span><span class="op">]</span><span class="op">]</span></span>
<span><span class="va">id</span> <span class="op">=</span> <span class="va">lt</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span></span>
<span></span>
<span><span class="co"># retrieve the batch &lt;id&gt; from the datast</span></span>
<span><span class="va">dataset_output</span> <span class="op">=</span> <span class="va">desc</span><span class="op">$</span><span class="va">dataset</span><span class="op">$</span><span class="fu">.getbatch</span><span class="op">(</span><span class="va">id</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># batch is reorganized according to the input map</span></span>
<span><span class="va">graph_input</span> <span class="op">=</span> <span class="va">dataset_output</span><span class="op">[</span><span class="va">desc</span><span class="op">$</span><span class="va">input_map</span><span class="op">]</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/names.html" class="external-link">names</a></span><span class="op">(</span><span class="va">graph_input</span><span class="op">)</span> <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/names.html" class="external-link">names</a></span><span class="op">(</span><span class="va">desc</span><span class="op">$</span><span class="va">graph</span><span class="op">$</span><span class="va">input</span><span class="op">$</span><span class="va">name</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># the reorganized batch is fed into the preprocessing graph</span></span>
<span><span class="va">graph_output</span> <span class="op">=</span> <span class="va">desc</span><span class="op">$</span><span class="va">graph</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span><span class="va">graph_input</span>, single_input <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># the output pointed to by the pointer is returned</span></span>
<span><span class="va">tensor</span> <span class="op">=</span> <span class="va">graph_output</span><span class="op">[[</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html" class="external-link">paste0</a></span><span class="op">(</span><span class="va">desc</span><span class="op">$</span><span class="va">pointer</span>, collapse <span class="op">=</span> <span class="st">"."</span><span class="op">)</span><span class="op">]</span><span class="op">]</span></span>
<span><span class="va">tensor</span></span>
<span><span class="co">#&gt; torch_tensor</span></span>
<span><span class="co">#&gt; -0.9852</span></span>
<span><span class="co">#&gt; [ CPUFloatType{1,1} ]</span></span></code></pre></div>
<p>Preprocessing a <code>lazy_tensor</code> vector adds new
<code>PipeOp</code>s to the preprocessing graph and updates the
metainformation like the pointer and output shape. To show this, we
create a simple example task, using the <code>lt</code> vector as a
feature.</p>
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">taskin</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/as_task_regr.html" class="external-link">as_task_regr</a></span><span class="op">(</span><span class="fu"><a href="https://rdatatable.gitlab.io/data.table/reference/data.table.html" class="external-link">data.table</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">lt</span>, y <span class="op">=</span> <span class="fl">1</span><span class="op">:</span><span class="fl">3</span><span class="op">)</span>, target <span class="op">=</span> <span class="st">"y"</span><span class="op">)</span></span>
<span></span>
<span><span class="va">taskout</span> <span class="op">=</span> <span class="va">po_poly</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/list.html" class="external-link">list</a></span><span class="op">(</span><span class="va">taskin</span><span class="op">)</span><span class="op">)</span><span class="op">[[</span><span class="fl">1L</span><span class="op">]</span><span class="op">]</span></span>
<span></span>
<span><span class="va">lt_out</span> <span class="op">=</span> <span class="va">taskout</span><span class="op">$</span><span class="fu">data</span><span class="op">(</span>cols <span class="op">=</span> <span class="st">"x"</span><span class="op">)</span><span class="op">$</span><span class="va">x</span></span>
<span></span>
<span><span class="va">descout</span> <span class="op">=</span> <span class="va">lt_out</span><span class="op">[[</span><span class="fl">1</span><span class="op">]</span><span class="op">]</span><span class="op">[[</span><span class="fl">2</span><span class="op">]</span><span class="op">]</span></span>
<span></span>
<span><span class="va">descout</span></span>
<span><span class="co">#&gt; &lt;DataDescriptor: 2 ops&gt;</span></span>
<span><span class="co">#&gt; * dataset_shapes: [x: (NA,1)]</span></span>
<span><span class="co">#&gt; * input_map: (x) -&gt; Graph</span></span>
<span><span class="co">#&gt; * pointer: poly.x.output</span></span>
<span><span class="co">#&gt; * shape: [(NA,7)]</span></span>
<span></span>
<span><span class="va">descout</span><span class="op">$</span><span class="va">graph</span></span>
<span><span class="co">#&gt; Graph with 2 PipeOps:</span></span>
<span><span class="co">#&gt;            ID         State sccssors    prdcssors</span></span>
<span><span class="co">#&gt;        &lt;char&gt;        &lt;char&gt;   &lt;char&gt;       &lt;char&gt;</span></span>
<span><span class="co">#&gt;  nop.77752c.x        &lt;list&gt;   poly.x             </span></span>
<span><span class="co">#&gt;        poly.x &lt;&lt;UNTRAINED&gt;&gt;          nop.77752c.x</span></span></code></pre></div>
<p>We see that the <code>$graph</code> has a new pipeop with id
<code>"poly.x"</code> and the output <code>pointer</code> points to
<code>poly.x</code>. Also we see that the shape of the tensor is now
<code>c(NA, 7)</code> and not <code>c(NA, 1)</code> as before, which we
can verify by calling <code><a href="../reference/materialize.html">materialize()</a></code> again:</p>
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/materialize.html">materialize</a></span><span class="op">(</span><span class="va">lt_out</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">2</span><span class="op">]</span>, rbind <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span>
<span><span class="co">#&gt; torch_tensor</span></span>
<span><span class="co">#&gt;  1.0000 -0.9852  0.9706 -0.9563  0.9421 -0.9009 -0.8487</span></span>
<span><span class="co">#&gt;  1.0000 -0.9852  0.9706 -0.9563  0.9421 -0.9009 -0.8487</span></span>
<span><span class="co">#&gt; [ CPUFloatType{2,7} ]</span></span></code></pre></div>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Sebastian Fischer, Martin Binder.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.1.</p>
</div>

    </footer>
</div>





  </body>
</html>
