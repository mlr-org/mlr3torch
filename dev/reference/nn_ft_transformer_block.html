<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Single Transformer Block for FT-Transformer — nn_ft_transformer_block • mlr3torch</title><!-- favicons --><link rel="icon" type="image/png" sizes="96x96" href="../favicon-96x96.png"><link rel="icon" type="”image/svg+xml”" href="../favicon.svg"><link rel="apple-touch-icon" sizes="180x180" href="../apple-touch-icon.png"><link rel="icon" sizes="any" href="../favicon.ico"><link rel="manifest" href="../site.webmanifest"><!-- mathjax math --><script src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script><script>
  window.MathJax = {
    chtml: {
      fontURL: "https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/output/chtml/fonts/woff-v2"
    }
  };
</script><script src="../lightswitch.js"></script><script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet"><script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/Roboto-0.4.10/font.css" rel="stylesheet"><link href="../deps/JetBrains_Mono-0.4.10/font.css" rel="stylesheet"><link href="../deps/Roboto_Slab-0.4.10/font.css" rel="stylesheet"><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet"><link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet"><script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Single Transformer Block for FT-Transformer — nn_ft_transformer_block"><meta name="description" content="A transformer block consisting of a multi-head self-attention mechanism followed by a feed-forward
network.
This is used in LearnerTorchFTTransformer."><meta property="og:description" content="A transformer block consisting of a multi-head self-attention mechanism followed by a feed-forward
network.
This is used in LearnerTorchFTTransformer."><meta property="og:image" content="https://mlr3torch.mlr-org.com/logo.svg"><meta name="robots" content="noindex"></head><body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top " aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">mlr3torch</a>

    <small class="nav-text text-default me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="In-development version">0.3.1.9000</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto"><li class="active nav-item"><a class="nav-link" href="../reference/index.html"><span class="fa fa-file-alt"></span> Reference</a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles"><li><h6 class="dropdown-header" data-toc-skip>Tutorials</h6></li>
    <li><a class="dropdown-item" href="../articles/get_started.html">Get Started</a></li>
    <li><a class="dropdown-item" href="../articles/pipeop_torch.html">Defining an Architecture</a></li>
    <li><a class="dropdown-item" href="../articles/callbacks.html">Callbacks</a></li>
    <li><a class="dropdown-item" href="../articles/lazy_tensor.html">Non-Tabular Data</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><h6 class="dropdown-header" data-toc-skip>Internals</h6></li>
    <li><a class="dropdown-item" href="../articles/internals_pipeop_torch.html">Networks as Graphs</a></li>
  </ul></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-overview" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Overview</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-overview"><li><a class="dropdown-item" href="../articles/task_list.html">Tasks</a></li>
    <li><a class="dropdown-item" href="../articles/preprocessing_list.html">Preprocessing &amp; Augmentation</a></li>
    <li><a class="dropdown-item" href="../articles/optimizer_list.html">Optimizers</a></li>
    <li><a class="dropdown-item" href="../articles/loss_list.html">Losses</a></li>
    <li><a class="dropdown-item" href="../articles/callback_list.html">Callbacks</a></li>
    <li><a class="dropdown-item" href="../articles/learner_list.html">Learners</a></li>
    <li><a class="dropdown-item" href="../articles/layer_list.html">Network Layers</a></li>
  </ul></li>
<li class="nav-item"><a class="external-link nav-link" href="https://mlr3book.mlr-org.com"><span class="fa fa-link"></span> mlr3book</a></li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
      </ul><ul class="navbar-nav"><li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json"></form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/mlr-org/mlr3torch"><span class="fa fa-github"></span></a></li>
<li class="nav-item"><a class="external-link nav-link" href="https://lmmisld-lmu-stats-slds.srv.mwn.de/mlr_invite/"><span class="fa fa-comments"></span></a></li>
<li class="nav-item"><a class="external-link nav-link" href="https://stackoverflow.com/questions/tagged/mlr3"><span class="fa fab fa-stack-overflow"></span></a></li>
<li class="nav-item"><a class="external-link nav-link" href="https://mlr-org.com/"><span class="fa fa-rss"></span></a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-lightswitch" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true" aria-label="Light switch"><span class="fa fa-sun"></span></button>
  <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="dropdown-lightswitch"><li><button class="dropdown-item" data-bs-theme-value="light"><span class="fa fa-sun"></span> Light</button></li>
    <li><button class="dropdown-item" data-bs-theme-value="dark"><span class="fa fa-moon"></span> Dark</button></li>
    <li><button class="dropdown-item" data-bs-theme-value="auto"><span class="fa fa-adjust"></span> Auto</button></li>
  </ul></li>
      </ul></div>


  </div>
</nav><div class="container template-reference-topic">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.svg" class="logo" alt=""><h1>Single Transformer Block for FT-Transformer</h1>
      <small class="dont-index">Source: <a href="https://github.com/mlr-org/mlr3torch/blob/main/R/PipeOpTorchFTTransformerBlock.R" class="external-link"><code>R/PipeOpTorchFTTransformerBlock.R</code></a></small>
      <div class="d-none name"><code>nn_ft_transformer_block.Rd</code></div>
    </div>

    <div class="ref-description section level2">
    <p>A transformer block consisting of a multi-head self-attention mechanism followed by a feed-forward
network.</p>
<p>This is used in <code><a href="mlr_learners.ft_transformer.html">LearnerTorchFTTransformer</a></code>.</p>
    </div>

    <div class="section level2">
    <h2 id="ref-usage">Usage<a class="anchor" aria-label="anchor" href="#ref-usage"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span><span class="fu">nn_ft_transformer_block</span><span class="op">(</span></span>
<span>  <span class="va">d_token</span>,</span>
<span>  <span class="va">attention_n_heads</span>,</span>
<span>  <span class="va">attention_dropout</span>,</span>
<span>  <span class="va">attention_initialization</span>,</span>
<span>  ffn_d_hidden <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  ffn_d_hidden_multiplier <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  <span class="va">ffn_dropout</span>,</span>
<span>  <span class="va">ffn_activation</span>,</span>
<span>  <span class="va">residual_dropout</span>,</span>
<span>  <span class="va">prenormalization</span>,</span>
<span>  <span class="va">is_first_layer</span>,</span>
<span>  <span class="va">attention_normalization</span>,</span>
<span>  <span class="va">ffn_normalization</span>,</span>
<span>  query_idx <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  <span class="va">attention_bias</span>,</span>
<span>  <span class="va">ffn_bias_first</span>,</span>
<span>  <span class="va">ffn_bias_second</span></span>
<span><span class="op">)</span></span></code></pre></div>
    </div>

    <div class="section level2">
    <h2 id="arguments">Arguments<a class="anchor" aria-label="anchor" href="#arguments"></a></h2>


<dl><dt id="arg-d-token">d_token<a class="anchor" aria-label="anchor" href="#arg-d-token"></a></dt>
<dd><p>(<code>integer(1)</code>)<br>
The dimension of the embedding.</p></dd>


<dt id="arg-attention-n-heads">attention_n_heads<a class="anchor" aria-label="anchor" href="#arg-attention-n-heads"></a></dt>
<dd><p>(<code>integer(1)</code>)<br>
Number of attention heads.</p></dd>


<dt id="arg-attention-dropout">attention_dropout<a class="anchor" aria-label="anchor" href="#arg-attention-dropout"></a></dt>
<dd><p>(<code>numeric(1)</code>)<br>
Dropout probability in the attention mechanism.</p></dd>


<dt id="arg-attention-initialization">attention_initialization<a class="anchor" aria-label="anchor" href="#arg-attention-initialization"></a></dt>
<dd><p>(<code>character(1)</code>)<br>
Initialization method for attention weights. Either "kaiming" or "xavier".</p></dd>


<dt id="arg-ffn-d-hidden">ffn_d_hidden<a class="anchor" aria-label="anchor" href="#arg-ffn-d-hidden"></a></dt>
<dd><p>(<code>integer(1)</code>)<br>
Hidden dimension of the feed-forward network. Multiplied by 2 if using ReGLU or GeGLU activation.</p></dd>


<dt id="arg-ffn-d-hidden-multiplier">ffn_d_hidden_multiplier<a class="anchor" aria-label="anchor" href="#arg-ffn-d-hidden-multiplier"></a></dt>
<dd><p>(<code>numeric(1)</code>)<br>
Alternative way to specify the hidden dimension of the feed-forward network as <code>d_token * d_hidden_multiplier</code>. Also multiplied by 2 if using RegLU or GeGLU activation.</p></dd>


<dt id="arg-ffn-dropout">ffn_dropout<a class="anchor" aria-label="anchor" href="#arg-ffn-dropout"></a></dt>
<dd><p>(<code>numeric(1)</code>)<br>
Dropout probability in the feed-forward network.</p></dd>


<dt id="arg-ffn-activation">ffn_activation<a class="anchor" aria-label="anchor" href="#arg-ffn-activation"></a></dt>
<dd><p>(<code>nn_module</code>)<br>
Activation function for the feed-forward network. Default value is <code>nn_reglu</code>.</p></dd>


<dt id="arg-residual-dropout">residual_dropout<a class="anchor" aria-label="anchor" href="#arg-residual-dropout"></a></dt>
<dd><p>(<code>numeric(1)</code>)<br>
Dropout probability for residual connections.</p></dd>


<dt id="arg-prenormalization">prenormalization<a class="anchor" aria-label="anchor" href="#arg-prenormalization"></a></dt>
<dd><p>(<code>logical(1)</code>)<br>
Whether to apply normalization before attention and FFN (<code>TRUE</code>) or after (<code>TRUE</code>).</p></dd>


<dt id="arg-is-first-layer">is_first_layer<a class="anchor" aria-label="anchor" href="#arg-is-first-layer"></a></dt>
<dd><p>(<code>logical(1)</code>)<br>
Whether this is the first layer in the transformer stack. Default value is <code>FALSE</code>.</p></dd>


<dt id="arg-attention-normalization">attention_normalization<a class="anchor" aria-label="anchor" href="#arg-attention-normalization"></a></dt>
<dd><p>(<code>nn_module</code>)<br>
Normalization module to use for attention. Default value is <code>nn_layer_norm</code>.</p></dd>


<dt id="arg-ffn-normalization">ffn_normalization<a class="anchor" aria-label="anchor" href="#arg-ffn-normalization"></a></dt>
<dd><p>(<code>nn_module</code>)<br>
Normalization module to use for the feed-forward network. Default value is <code>nn_layer_norm</code>.</p></dd>


<dt id="arg-query-idx">query_idx<a class="anchor" aria-label="anchor" href="#arg-query-idx"></a></dt>
<dd><p>(<code><a href="https://rdrr.io/r/base/integer.html" class="external-link">integer()</a></code> or <code>NULL</code>)<br>
Indices of the tensor to apply attention to. Should not be set manually.
If NULL, then attention is applied to the entire tensor.
In the last block in a stack of transformers, this is set to <code>-1</code>
so that attention is applied only to the embedding of the CLS token.</p></dd>


<dt id="arg-attention-bias">attention_bias<a class="anchor" aria-label="anchor" href="#arg-attention-bias"></a></dt>
<dd><p>(<code>logical(1)</code>)<br>
Whether attention has a bias. Default is <code>TRUE</code></p></dd>


<dt id="arg-ffn-bias-first">ffn_bias_first<a class="anchor" aria-label="anchor" href="#arg-ffn-bias-first"></a></dt>
<dd><p>(<code>logical(1)</code>)<br>
Whether the first layer in the FFN has a bias. Default is <code>TRUE</code></p></dd>


<dt id="arg-ffn-bias-second">ffn_bias_second<a class="anchor" aria-label="anchor" href="#arg-ffn-bias-second"></a></dt>
<dd><p>(<code>logical(1)</code>)<br>
Whether the second layer in the FFN has a bias. Default is <code>TRUE</code></p></dd>

</dl></div>
    <div class="section level2">
    <h2 id="references">References<a class="anchor" aria-label="anchor" href="#references"></a></h2>
    <p>Devlin, Jacob, Chang, Ming-Wei, Lee, Kenton, Toutanova, Kristina (2018).
“Bert: Pre-training of deep bidirectional transformers for language understanding.”
<em>arXiv preprint arXiv:1810.04805</em>.
Gorishniy Y, Rubachev I, Khrulkov V, Babenko A (2021).
“Revisiting Deep Learning  for Tabular Data.”
<em>arXiv</em>, <b>2106.11959</b>.</p>
    </div>

  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside></div>


    <footer><div class="pkgdown-footer-left">
  <p>Developed by Sebastian Fischer, Martin Binder.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.3.</p>
</div>

    </footer></div>





  </body></html>

