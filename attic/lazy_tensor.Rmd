---
title: "Lazy Tensor"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Lazy Tensor}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
library(ggplot2)
theme_set(theme_bw())
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

This vignette introduces the `lazy_tensor` class, which is a vector type that can be used to lazily represent torch tensors of arbitary dimensions.
The `lazy_tensor` type can be illustrated using the predefined `"lazy_iris"` task, which is similar to the `"iris"` task, but stores the 4 features in a single `lazy_tensor` column instead of four numeric columns.

```{r}
library(mlr3torch)

tsk_iris = tsk("lazy_iris")
tsk_iris
lt = tsk_iris$data(cols = "x")$x
head(lt)
```

We can convert the `lazy_tensor` column to a `torch_tensor` by calling `materialize()`.
By default, this will return a list of `torch_tensor`s.

```{r}
materialize(lt[1:2])
```

We can also convert the `lazy_tensor` to a single tensor by setting `rbind = TRUE`.

```{r}
materialize(lt[1:2], rbind = TRUE)
```

An important feature of the `lazy_tensor` type is that it can be preprocessed like other (non-lazy) datatypes.
Below, we standardize the data using its mean and standard deviation, which we calculate from the materialized tensor.

```{r}
lt_mat = materialize(lt, rbind = TRUE)
mus = torch_mean(lt_mat, dim = 1L)
sigmas = torch_std(lt_mat, dim = 1L)
po_scale = po("trafo_scale", mean = mus, sd = sigmas)

tsk_iris_preproc = po_scale$train(list(tsk_iris))[[1L]]
tsk_iris_preproc
```

Materializing the (lazily) preprocessed lazy tensor is equivalent to (eagerly) preprocessing the materialized tensor.

```{r}
lt_preproc = tsk_iris_preproc$data(cols = "x")$x
torch_equal(materialize(lt_preproc, rbind = TRUE), (lt_mat - mus) / sigmas)
```

We can combine the preprocessing with a simple multi layer perceptron to create a `GraphLearner`.

```{r}
glrn = as_learner(
  po_scale %>>% lrn("classif.mlp", batch_size = 150, epochs = 10)
)

glrn$train(tsk("iris"))
```


## Creating a Lazy Tensor

Every `lazy_tensor` is built on top of a `torch::dataset`, so we start by creating an dataset containing random data.
The only restriction that we impose on the dataset is that it must have a `.getitem` or `.getbatch` method that returns a list of named tensors.
In this case, the dataset returns a single tensor named `x`.
While the data is stored in-memory in this example, this is not necessary and the `$.getitem()` method can e.g. load images from disk.
For more information on how to create `torch::dataset`s, we recommend reading the [torch package documentation](https://torch.mlverse.org/).

```{r}
mydata = dataset(
  initialize = function() {
    self$x = runif(1000, -1, 1)
  },
  .getbatch = function(i) list(x = torch_tensor(self$x[i])$unsqueeze(2)),
  .length = function() 1000
)()
```

In order to create a `lazy_tensor` from `mydata`, we have to annotate the returned shapes of the dataset by passing a named list to `dataset_shapes`.
The first dimension must be `NA` as it is the batch dimension.
We can also set a shape to `NULL` to indicate that it is unknown (e.g. because it is variable).

```{r}
lt = as_lazy_tensor(mydata, dataset_shapes = list(x = c(NA, 1)))
lt[1:5]
```

We can convert this vector to a `torch_tensor` just like before

```{r}
materialize(lt[1], rbind = TRUE)
```

Because we added no preprocessing, this is the same as calling the `$.getbatch()` method on `mydata` and selecting the element `x`.

```{r}
torch_equal(
  materialize(lt[1], rbind = TRUE),
  mydata$.getbatch(1)$x
)
```

We continue with creating an example task from `lt`.

```{r}
library(data.table)
x = mydata$x
y = 0.2 + 0.1 * x - 0.1 * x^2 - 0.3 * x^3 + 0.5 * x^4 + 0.5 * x^7 + 0.6 * x^11 +  rnorm(length(mydata)) * 0.1
dt = data.table(y = y, x = lt)
task_poly = as_task_regr(dt, target = "y", id = "poly")
```

```{r}
library(ggplot2)
ggplot(data = data.frame(x = x, y = y)) +
  geom_point(aes(x = x, y = y))
```

In the next section, we will create a custom `PipeOp` to fit a polynomial regression model.

## Custom Preprocessing

In order to create a custom preprocessing operator for a lazy tensor, we have to create a new `PipeOp` class.
To make this as convenient as possible, `mlr3torch` offters a `pipeop_preproc_torch()` function that we recommend using for this purpose.
Its most important arguments are:
* `id` - Used as the default identifier of the `PipeOp`
* `fn` - The preprocessing function
* `shapes_out` - A function that returns the shapes of the output tensors given the input shapes


```{r}
PipeOpPreprocTorchPoly = pipeop_preproc_torch("poly",
  fn = function(x, degrees) {
    torch_cat(lapply(degrees, function(d) torch_pow(x, d)), dim = 2L)
  },
  shapes_out = function(shapes_in, param_vals, task) {
    shape = shapes_in[[1L]]
    stopifnot(length(shape) == 2L)
    list(c(NA, length(param_vals$degrees)))
  }
)

po_poly = PipeOpPreprocTorchPoly$new()

# We want to apply the preprocessing during both `"train"` and `"predict"` and set te stages parameter accordingly.
po_poly$param_set$set_values(
  degrees = c(0, 1, 2, 3, 4, 7, 11),
  stages = "both"
)

lrn_poly = as_learner(
  po_poly %>>% lrn("regr.mlp", batch_size = 256, epochs = 20)
)

lrn_poly$train(task_poly)
pred = lrn_poly$predict(task_poly)

dt = melt(data.table(
  truth = pred$truth,
  response = pred$response,
  x = x),
  id.vars = "x", measure.vars = c("truth", "response")
)
dt$variable = factor(dt$variable, levels = c("truth", "response"))


ggplot(data = dt) +
  geom_point(aes(x = x, y = value, color = variable))
```

## Digging Into Internals

Internally, the `lazy_tensor` vector uses the `DataDescriptor` class to represent the (possibly) preprocessed data.
It is very similar to the `ModelDescriptor` class that is used to build up neural nerworks using `PipeOpTorch` objects.
The `DataDescriptor` stores a `torch::dataset`, an `mlr3pipelines::Graph` and some metadata.


```{r}
desc = DataDescriptor$new(
  dataset = mydata,
  dataset_shapes = list(x = c(NA, 1))
)
```

Per default, the preprocessing graph contains only a single `PipOpNop` that doees nothing.

```{r}
desc
```

The printed output of the data descriptor informs us about:

* The number of `PipeOp`s contained in the preprocessing graph
* The output shapes of the dataset
* The input map, i.e. how the data is passed to the preprocessing graph, which is important when there are multiple inputs
* The `pointer`, which points to a specific channel of an output `PipeOp`.
  The output of this channel is the tensor represented by the `DataDescriptor`.


A lazy tensor can be constructed from an integer vector and a `DataDescriptor`.
The integer vector specifies which element of the `DataDescriptor` an element of the `lazy_tensor` vector represents.
Below, the first two elements of the `lazy_tensor` vector represent the same element of the `DataDescriptor`, while the third element represents a different element.
Note that all indices refer to the same `DataDescriptor`.

```{r}
lt = lazy_tensor(desc, ids = c(1, 1, 2))
materialize(lt, rbind = TRUE)
```

Internally, the lazy tensor is represented as a list of lists, containing an id and a `DataDescriptor`
Currently, there can only be a single `DataDescriptor` in a `lazy_tensor` vector.

```{r}
unclass(lt[[1]])
```


What happens during `materialize(lt[1])` is the following:

```{r}
# get index and data descriptor
desc = lt[[1]][[2]]
id = lt[[1]][[1]]
dataset_output = desc$dataset$.getbatch(id)

# batch is reorganized according to the input map
graph_input = dataset_output[desc$input_map]
names(graph_input) = names(desc$graph$input$name)

# the reorganized batch is fed into the preprocessing graph
graph_output = desc$graph$train(graph_input, single_input = FALSE)

# the output pointed to by the pointer is returned
tensor = graph_output[[paste0(desc$pointer, collapse = ".")]]
tensor
```
Preprocessing a `lazy_tensor` vector adds new `PipeOp`s to the preprocessing graph and updates the metainformation like the pointer and output shape.
To show this, we create a simple example task, using the `lt` vector as a feature.

```{r}
taskin = as_task_regr(data.table(x = lt, y = 1:3), target = "y")

taskout = po_poly$train(list(taskin))[[1L]]

lt_out = taskout$data(cols = "x")$x

descout = lt_out[[1]][[2]]

descout$graph
```
