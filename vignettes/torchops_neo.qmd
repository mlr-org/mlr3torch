---
title: "TorchOps Neo"
author: "mb706"
date: '2022-08-13'
output: html_document
---

```r
library("torch")
```

# Intro

The TorchOps Neo project tries to use `mlr3pipelines` as a backet to implement building network architectures from `PipeOp`s. Specific `PipeOp`s, inheriting from [`PipeOpModule`], contain `torch_module` objects. These `PipeOp`s are chained in a `Graph` that passes `torch_tensor` objects along its edges. Another class of `PipeOp`, [`PipeOpTorch`], can be used to build these torch module `Graph`s, they become parts of normal data preprocessing pipelines. These two distinct classes of `PipeOp`s are necessary since they represent different things: The [`PipeOpTorch`] operators correspond to `torch`'s `nn_module_generator` functions, the [`PipeOpModule`] then contain the instantiated `nn_module`-objects.

Some further changes have been made compared to the old code, in particular the handling of different columns / datatypes within `Task`s and how network input is handled. The network training code was also refactored and simplified.

# Torch modules and `PipeOpModule`

## Torch Primer

Some input tensor: 2-batch of 3 units.
```r
input <- torch_randn(2, 3)
input
```

A `nn_module` is constructed from a `nn_module_generator`. `nn_linear` is one of the simpler generators:
```r
module_1 <- nn_linear(3, 4, bias = TRUE)
```

Applying this module gives a 2-batch of 4 units:

```r
output <- module_1(input)
output
```

A neural network with one (4-unit) hidden layer and two outputs needs the following ingredients

```r
activation <- nn_sigmoid()
module_2 <- nn_linear(4, 2, bias = TRUE)
softmax <- nn_softmax(2)
```

Piping a tensor through this:

```r
output <- module_1(input)
output <- activation(output)
output <- module_2(output)
output <- softmax(output)
output
```

Training the neural network then consists of Toch keeping track of what parameters were used to calculate `output` to calculate gradients etc. In particular, this "piping through a series of modules" will happen a lot. An obvious idea here is to do this with `mlr3pipelines`.

## Wrapped Torch modules as `PipeOpModule`

Wrapping `nn_module` objects in a `PipeOp` has the advantage that the network structure can be represented as a `Graph` object where it is made explicit (can be plotted, can be extended or manipulated), compared to e.g. writing a function that pipes input through a series of modules.

A `PipeOpModule` can be used to wrap a module directly, but it is usually constructed by a `PipeOpTorch` (see later). It typically has a single input and a single output, although multiple inputs are possible (module is then called with multiple arguments), and multiple outputs are possible when the module-function returns a list. Number of outputs must be declared during construction, then.

Wrapping the linear `module_1` works like this:
```r
po_module_1 <- PipeOpModule$new("module_1", module_1)
```

It is used in the familiar way:
```r
output <- po_module_1$train(list(input))[[1]]
```

Note we only use the `$train()`, since Torch modules do not have anything that maps to the `state` (it is filled by an empty list).

The single hidden layer neural network can be constructed as a `Graph`, which can then do the training all at once.

```r
po_activation <- PipeOpModule$new("activation", activation)
po_module_2 <- PipeOpModule$new("module_2", module_2)
po_softmax <- PipeOpModule$new("softmax", softmax)

module_graph <- po_module_1 %>>% po_activation %>>% po_module_2 %>>% po_softmax
```

Using the `Graph`'s `$train()` to pipe a tensor through the `Graph`:
```r
output <- module_graph$train(input)[[1]]
output
```

## `Graph` as a Torch module

The native class in `torch` that represents a transformation of tensors is the `nn_module`. It has various advantages to have the `Graph` of `PipeOpModule` also be a 