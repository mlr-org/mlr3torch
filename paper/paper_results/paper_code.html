<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<meta name="generator" content="litedown 0.8">
<title></title>
<style type="text/css">
body {
  font-family: sans-serif;
  max-width: 800px;
  margin: auto;
  padding: 1em;
  line-height: 1.5;
  print-color-adjust: exact;
  -webkit-print-color-adjust: exact;
}
body, .abstract, code, .footnotes, footer, #refs, .caption { font-size: .9em; }
li li { font-size: .95em; }
ul:has(li > input[type="checkbox"]) { list-style: none; padding-left: 1em; }
*, :before, :after { box-sizing: border-box; }
a { color: steelblue; }
pre, img { max-width: 100%; }
pre { white-space: pre-wrap; word-break: break-word; }
pre code { display: block; padding: 1em; overflow-x: auto; }
code { font-family: 'DejaVu Sans Mono', 'Droid Sans Mono', 'Lucida Console', Consolas, Monaco, monospace; }
:not(pre, th) > code, code[class], div > .caption { background: #f8f8f8; }
pre > code:is(:not([class]), .language-plain, .language-none, .plain), .box, .figure, .table { background: inherit; border: 1px solid #eee; }
pre > code {
  &.message { border-color: #9eeaf9; }
  &.warning { background: #fff3cd; border-color: #fff3cd; }
  &.error { background: #f8d7da; border-color: #f8d7da; }
}
.fenced-chunk { border-left: 1px solid #666; }
.code-fence {
  opacity: .4;
  border: 1px dashed #666;
  border-left: 2px solid;
  &:hover { opacity: inherit; }
}
.box, .figure, .table, table { margin: 1em auto; }
div > .caption { padding: 1px 1em; }
.figure { p:has(img, svg), pre:has(svg) { text-align: center; } }
.flex-col { display: flex; justify-content: space-between; }
table {
  &:only-child:not(.table > *) { margin: auto; }
  th, td { padding: 5px; font-variant-numeric: tabular-nums; }
  thead, tfoot, tr:nth-child(even) { background: whitesmoke; }
  thead th { border-bottom: 1px solid #ddd; }
  &:not(.datatable-table) {
    border-top: 1px solid #666;
    border-bottom: 1px solid #666;
  }
}
blockquote {
  color: #666;
  margin: 0;
  padding: 1px 1em;
  border-left: .5em solid #eee;
}
hr, .footnotes::before { border: 1px dashed #ddd; }
.frontmatter { text-align: center; }
#TOC {
  a { text-decoration: none; }
  ul { list-style: none; padding-left: 1em; }
  & > ul { padding: 0; }
  ul ul { border-left: 1px solid lightsteelblue; }
}
.body h2 { border-bottom: 1px solid #666; }
.body .appendix, .appendix ~ h2 { border-bottom-style: dashed; }
.main-number::after { content: "."; }
span[class^="ref-number-"] { font-weight: bold; }
.ref-number-fig::after, .ref-number-tab::after { content: ":"; }
.cross-ref-chp::before { content: "Chapter "; }
.cross-ref-sec::before { content: "Section "; }
.cross-ref-fig::before, .ref-number-fig::before { content: "Figure "; }
.cross-ref-tab::before, .ref-number-tab::before { content: "Table "; }
.cross-ref-eqn::before, .MathJax_ref:has(mjx-mtext > mjx-c + mjx-c)::before { content: "Equation "; }
.abstract, #refs {
  &::before { display: block; margin: 1em auto; font-weight: bold; }
}
.abstract::before { content: "Abstract"; text-align: center; }
#refs::before { content: "Bibliography"; font-size: 1.5em; }
.ref-paren-open::before { content: "("; }
.ref-paren-close::after { content: ")"; }
.ref-semicolon::after { content: "; "; }
.ref-and::after { content: " and "; }
.ref-et-al::after { content: " et al."; font-style: italic; }
.footnote-ref a {
  &::before { content: "["; }
  &::after { content: "]"; }
}
section.footnotes {
  margin-top: 2em;
  &::before { content: ""; display: block; max-width: 20em; }
}
.fade {
  background: repeating-linear-gradient(135deg, white, white 30px, #ddd 32px, #ddd 32px);
  opacity: 0.6;
}

@media print {
  body { max-width: 100%; }
  tr, img { break-inside: avoid; }
}
@media only screen and (min-width: 992px) {
  body:not(.pagesjs) pre:has(.line-numbers):not(:hover) { white-space: pre; }
}
</style>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@xiee/utils@1.14.14/css/prism-xcode.min.css">
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/components/prism-core.min.js" defer></script>
<script src="https://cdn.jsdelivr.net/npm/prismjs@1.29.0/plugins/autoloader/prism-autoloader.min.js" defer></script>
</head>
<body>
<div class="frontmatter">
</div>
<div class="body">
<pre><code class="language-r">Sys.time()
</code></pre>
<pre><code>## [1] &quot;2025-11-07 09:38:56 UTC&quot;
</code></pre>
<pre><code class="language-r">options(mlr3torch.cache = TRUE)
lgr::get_logger('mlr3')$set_threshold('warn')
library(&quot;mlr3&quot;)
set.seed(42)
task &lt;- tsk(&quot;mtcars&quot;)
learner &lt;- lrn(&quot;regr.rpart&quot;)
split &lt;- partition(task, ratio = 2/3)
learner$train(task, split$train)
pred &lt;- learner$predict(task, split$test)
pred$score(msr(&quot;regr.rmse&quot;))
</code></pre>
<pre><code>## regr.rmse 
##  4.736051
</code></pre>
<pre><code class="language-r">library(&quot;mlr3pipelines&quot;)
graph_learner &lt;- as_learner(po(&quot;pca&quot;) %&gt;&gt;% lrn(&quot;regr.rpart&quot;))

resampling &lt;- rsmp(&quot;cv&quot;, folds = 3)
rr &lt;- resample(task, graph_learner, resampling)
rr$aggregate(msr(&quot;regr.rmse&quot;))
</code></pre>
<pre><code>## regr.rmse 
##  4.274766
</code></pre>
<pre><code class="language-r">library(&quot;torch&quot;)
torch_manual_seed(42)
x &lt;- torch_tensor(1, device = &quot;cpu&quot;)
w &lt;- torch_tensor(2, requires_grad = TRUE, device = &quot;cpu&quot;)
y &lt;- w * x
y$backward()
w$grad
</code></pre>
<pre><code>## torch_tensor
##  1
## [ CPUFloatType{1} ]
</code></pre>
<pre><code class="language-r">library(&quot;mlr3torch&quot;)
mnist &lt;- tsk(&quot;mnist&quot;)
mnist
</code></pre>
<pre><code>## 
## ── &lt;TaskClassif&gt; (70000x2): MNIST Digit Classification ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
## • Target: label
</code></pre>
<pre><code>## Dataset &lt;mnist&gt; (~12 MB) will be downloaded and processed if not already available.
## Downloading &lt;mnist&gt; ...
## Processing &lt;mnist&gt;...
## Dataset &lt;mnist&gt; downloaded and extracted successfully.
## Dataset &lt;mnist&gt; loaded with 60000 images.
## Dataset &lt;mnist&gt; loaded with 10000 images.
</code></pre>
<pre><code>## • Target classes: 1 (11%), 7 (10%), 3 (10%), 2 (10%), 9 (10%), 0 (10%), 6 (10%), 8 (10%), 4 (10%), 5 (9%)
## • Properties: multiclass
## • Features (1):
##   • lt (1): image
</code></pre>
<pre><code class="language-r">rows &lt;- mnist$data(1:2)
rows
</code></pre>
<pre><code>##     label           image
##    &lt;fctr&gt;   &lt;lazy_tensor&gt;
## 1:      5 &lt;tnsr[1x28x28]&gt;
## 2:      0 &lt;tnsr[1x28x28]&gt;
</code></pre>
<pre><code class="language-r">str(materialize(rows$image))
</code></pre>
<pre><code>## List of 2
##  $ :Float [1:1, 1:28, 1:28]
##  $ :Float [1:1, 1:28, 1:28]
</code></pre>
<pre><code class="language-r">po_flat &lt;- po(&quot;trafo_reshape&quot;, shape = c(-1, 28 * 28))
mnist_flat &lt;- po_flat$train(list(mnist))[[1L]]
mnist_flat$head(2)
</code></pre>
<pre><code>##     label         image
##    &lt;fctr&gt; &lt;lazy_tensor&gt;
## 1:      5   &lt;tnsr[784]&gt;
## 2:      0   &lt;tnsr[784]&gt;
</code></pre>
<pre><code class="language-r">mlp &lt;- lrn(&quot;classif.mlp&quot;,
 loss = t_loss(&quot;cross_entropy&quot;),
 optimizer = t_opt(&quot;adamw&quot;, lr = 0.001),
 callbacks = t_clbk(&quot;history&quot;))

mlp$param_set$set_values(
  neurons = c(100, 200), activation = torch::nn_relu,
  p = 0.3, opt.weight_decay = 0.01, measures_train = msr(&quot;classif.logloss&quot;),
  epochs = 10, batch_size = 32, device = &quot;cpu&quot;)

mlp$configure(predict_type = &quot;prob&quot;)

mlp$train(mnist_flat, row_ids = 1:1000)

mlp$model$network
</code></pre>
<pre><code>## An `nn_module` containing 100,710 parameters.
## 
## ── Modules ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
## • 0: &lt;nn_linear&gt; #78,500 parameters
## • 1: &lt;nn_relu&gt; #0 parameters
## • 2: &lt;nn_dropout&gt; #0 parameters
## • 3: &lt;nn_linear&gt; #20,200 parameters
## • 4: &lt;nn_relu&gt; #0 parameters
## • 5: &lt;nn_dropout&gt; #0 parameters
## • 6: &lt;nn_linear&gt; #2,010 parameters
</code></pre>
<pre><code class="language-r">head(mlp$model$callbacks$history, n = 2)
</code></pre>
<pre><code>##    epoch train.classif.logloss
##    &lt;num&gt;                 &lt;num&gt;
## 1:     1              4.266895
## 2:     2              1.296983
</code></pre>
<pre><code class="language-r">pred &lt;- mlp$predict(mnist_flat, row_ids = 1001:1100)
pred$score(msr(&quot;classif.ce&quot;))
</code></pre>
<pre><code>## classif.ce 
##       0.21
</code></pre>
<pre><code class="language-r">pth &lt;- tempfile()
mlp$marshal()
saveRDS(mlp, pth)
mlp2 &lt;- readRDS(pth)
mlp2$unmarshal()

set_validate(mlp, validate = 0.3)

nn_simple &lt;- nn_module(&quot;nn_simple&quot;,
  initialize = function(d_in, d_latent, d_out) {
    self$linear1 = nn_linear(d_in, d_latent)
    self$activation = nn_relu()
    self$linear2 = nn_linear(d_latent, d_out)
  },
  forward = function(x) {
    x = self$linear1(x)
    x = self$activation(x)
    self$linear2(x)
  }
)

net &lt;- nn_simple(10, 100, 1)

net(torch_randn(1, 10))
</code></pre>
<pre><code>## torch_tensor
## 0.01 *
## -9.4603
## [ CPUFloatType{1,1} ][ grad_fn = &lt;AddmmBackward0&gt; ]
</code></pre>
<pre><code class="language-r">module_graph &lt;- po(&quot;module_1&quot;, module = nn_linear(10, 100)) %&gt;&gt;%
 po(&quot;module_2&quot;, module = nn_relu()) %&gt;&gt;%
 po(&quot;module_3&quot;, module = nn_linear(100, 1))

net &lt;- nn_graph(module_graph, shapes_in = list(module_1.input = c(NA, 10)))
net(torch_randn(2, 10))
</code></pre>
<pre><code>## torch_tensor
## -0.1218
## -0.0636
## [ CPUFloatType{2,1} ][ grad_fn = &lt;AddmmBackward0&gt; ]
</code></pre>
<pre><code class="language-r">graph &lt;- po(&quot;torch_ingress_ltnsr&quot;) %&gt;&gt;%
  nn(&quot;linear&quot;, out_features = 10) %&gt;&gt;% nn(&quot;relu&quot;) %&gt;&gt;% nn(&quot;head&quot;)

md &lt;- graph$train(mnist_flat)[[1L]]
md
</code></pre>
<pre><code>## &lt;ModelDescriptor: 4 ops&gt;
## * Ingress:  torch_ingress_ltnsr.input: [(NA,784)]
## * Task:  mnist [classif]
## * Callbacks:  N/A
## * Optimizer:  N/A
## * Loss:  N/A
## * pointer:  head.output [(NA,10)]
</code></pre>
<pre><code class="language-r">graph &lt;- graph %&gt;&gt;%
  po(&quot;torch_loss&quot;, t_loss(&quot;cross_entropy&quot;)) %&gt;&gt;%
  po(&quot;torch_optimizer&quot;, t_opt(&quot;adamw&quot;, lr = 0.001))

graph &lt;- graph %&gt;&gt;% po(&quot;torch_model_classif&quot;, epochs = 10, batch_size = 16)

glrn &lt;- as_learner(graph)
glrn$train(mnist_flat, row_ids = 1:1000)

path_lin &lt;- nn(&quot;linear_1&quot;)
path_nonlin &lt;- nn(&quot;linear_2&quot;) %&gt;&gt;% nn(&quot;relu&quot;)

residual_layer &lt;- list(path_lin, path_nonlin) %&gt;&gt;% nn(&quot;merge_sum&quot;)

path_num &lt;- po(&quot;select_1&quot;, selector = selector_type(&quot;numeric&quot;)) %&gt;&gt;%
  po(&quot;torch_ingress_num&quot;) %&gt;&gt;%
  nn(&quot;tokenizer_num&quot;, d_token = 10)
path_categ &lt;- po(&quot;select_2&quot;, selector = selector_type(&quot;factor&quot;)) %&gt;&gt;%
  po(&quot;torch_ingress_categ&quot;) %&gt;&gt;%
  nn(&quot;tokenizer_categ&quot;, d_token = 10)

graph &lt;- list(path_num, path_categ) %&gt;&gt;% nn(&quot;merge_cat&quot;, dim = 2)

blocks &lt;- nn(&quot;block&quot;, residual_layer, n_blocks = 5)

nn_winsorized_mse &lt;- nn_module(c(&quot;nn_winsorized_mse&quot;, &quot;nn_loss&quot;),
  initialize = function(max_loss) {
    self$max_loss &lt;- max_loss
  },
  forward = function(input, target) {
    torch_clamp(nnf_mse_loss(input, target), max = self$max_loss)
  }
)
tloss &lt;- as_torch_loss(nn_winsorized_mse)
tloss
</code></pre>
<pre><code>## &lt;TorchLoss:nn_winsorized_mse&gt; nn_winsorized_mse
## * Generator: nn_winsorized_mse
## * Parameters: list()
## * Packages: torch,mlr3torch
## * Task Types: classif,regr
</code></pre>
<pre><code class="language-r">gradient_clipper &lt;- torch_callback(&quot;gradient_clipper&quot;,
  initialize = function(max_norm, norm_type) {
    self$norms &lt;- numeric()
    self$max_norm &lt;- max_norm
    self$norm_type &lt;- norm_type
  },
  on_after_backward = function() {
    norm &lt;- nn_utils_clip_grad_norm_(self$ctx$network$parameters,
      self$max_norm, self$norm_type)
    self$norms &lt;- c(self$norms, norm$item())
  },
  state_dict = function() {
    self$norms
  },
  load_state_dict = function(state_dict) {
    self$norms = state_dict
  }
)

nn_ffn &lt;- nn_module(&quot;nn_ffn&quot;,
  initialize = function(task, latent_dim, n_layers) {
    dims &lt;- c(task$n_features, rep(latent_dim, n_layers),
      length(task$class_names))
    modules &lt;- unlist(lapply(seq_len(length(dims) - 1), function(i) {
      if (i &lt; length(dims) - 1) {
        list(nn_linear(dims[i], dims[i + 1]), nn_relu())
      } else {
        list(nn_linear(dims[i], dims[i + 1]))
      }
    }), recursive = FALSE)
    self$network &lt;- do.call(nn_sequential, modules)
  },
  forward = function(x) self$network(x)
)

num_input &lt;- list(x = ingress_num())
num_input
</code></pre>
<pre><code>## $x
## Ingress: Task[selector_type(c(&quot;numeric&quot;, &quot;integer&quot;))] --&gt; Tensor()
</code></pre>
<pre><code class="language-r">lrn_ffn &lt;- lrn(&quot;classif.module&quot;,
  module_generator = nn_ffn,
  ingress_tokens = num_input,
  latent_dim = 100, n_layers = 5)

task &lt;- tsk(&quot;mtcars&quot;)
task
</code></pre>
<pre><code>## 
## ── &lt;TaskRegr&gt; (32x11): Motor Trends ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
## • Target: mpg
## • Properties: -
## • Features (10):
##   • dbl (10): am, carb, cyl, disp, drat, gear, hp, qsec, vs, wt
</code></pre>
<pre><code class="language-r">ingress &lt;- po(&quot;torch_ingress_num&quot;)

block &lt;- nn(&quot;linear&quot;, out_features = 32) %&gt;&gt;%
  ppl(&quot;branch&quot;, list(relu = nn(&quot;relu&quot;), sigmoid = nn(&quot;sigmoid&quot;))) %&gt;&gt;%
  nn(&quot;dropout&quot;)

architecture &lt;- nn(&quot;block&quot;, block) %&gt;&gt;% nn(&quot;head&quot;)

config &lt;- po(&quot;torch_loss&quot;, loss = t_loss(&quot;mse&quot;)) %&gt;&gt;%
  po(&quot;torch_optimizer&quot;, optimizer = t_opt(&quot;adamw&quot;))

model &lt;- po(&quot;torch_model_regr&quot;, device = &quot;cpu&quot;, batch_size = 512)

pipeline &lt;- ingress %&gt;&gt;%
  architecture %&gt;&gt;% config %&gt;&gt;% model
learner &lt;- as_learner(pipeline)
learner$id &lt;- &quot;custom_nn&quot;

library(&quot;mlr3tuning&quot;)
</code></pre>
<pre><code>## Loading required package: paradox
</code></pre>
<pre><code class="language-r">learner$param_set$set_values(
  block.linear.out_features = to_tune(20, 500),
  block.n_blocks = to_tune(1, 5),
  block.branch.selection = to_tune(c(&quot;relu&quot;, &quot;sigmoid&quot;)),
  block.dropout.p = to_tune(0.1, 0.9),
  torch_optimizer.lr = to_tune(10^-4, 10^-1, logscale = TRUE))

set_validate(learner, &quot;test&quot;)

learner$param_set$set_values(
  torch_model_regr.patience = 5,
  torch_model_regr.measures_valid = msr(&quot;regr.mse&quot;),
  torch_model_regr.epochs = to_tune(upper = 100, internal = TRUE))

library(&quot;mlr3tuning&quot;)
ti &lt;- tune(
  tuner = tnr(&quot;random_search&quot;),
  resampling = rsmp(&quot;holdout&quot;),
  measure = msr(&quot;internal_valid_score&quot;, minimize = TRUE),
  learner = learner,
  term_evals = 30,
  task = task)
pvals &lt;- ti$result_learner_param_vals[1:6]
cat(paste(&quot;*&quot;, names(pvals), &quot;=&quot;, pvals,
 collapse = &quot;\n&quot;), &quot;\n&quot;)
</code></pre>
<pre><code>## * block.n_blocks = 4
## * block.linear.out_features = 248
## * block.branch.selection = sigmoid
## * block.dropout.p = 0.861211954243481
## * torch_optimizer.lr = 0.00310737353560635
## * torch_model_regr.epochs = 44
</code></pre>
<pre><code class="language-r">library(&quot;torchdatasets&quot;)
dogs_vs_cats_dataset(&quot;data&quot;, download = TRUE)
</code></pre>
<pre><code>## &lt;dataset&gt;
##   Public:
##     .getitem: function (i) 
##     .length: function () 
##     classes: dog cat
##     clone: function (deep = FALSE) 
##     images: data/dogs-vs-cats/train/cat.0.jpg data/dogs-vs-cats/trai ...
##     initialize: function (root, split = &quot;train&quot;, download = FALSE, ..., transform = NULL, 
##     load_state_dict: function (x, ..., .refer_to_state_dict = FALSE) 
##     state_dict: function () 
##     target_transform: NULL
##     targets: 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2  ...
##     transform: NULL
</code></pre>
<pre><code class="language-r">ds &lt;- torch::dataset(&quot;dogs_vs_cats&quot;,
  initialize = function(pths) {
    self$pths &lt;- pths
  },
  .getitem = function(i) {
    image &lt;- torchvision::base_loader(self$pths[i])
    list(image = torch_tensor(image)$permute(c(3, 1, 2)))
  },
  .length = function() {
    length(self$pths)
  }
)

paths &lt;- list.files(file.path(&quot;data&quot;, &quot;dogs-vs-cats/train&quot;),
  full.names = TRUE)
dogs_vs_cats &lt;- ds(paths)

lt &lt;- as_lazy_tensor(dogs_vs_cats, list(image = NULL))

labels &lt;- ifelse(grepl(&quot;dog\\.\\d+\\.jpg&quot;, paths), &quot;dog&quot;, &quot;cat&quot;)
table(labels)
</code></pre>
<pre><code>## labels
##   cat   dog 
## 12500 12500
</code></pre>
<pre><code class="language-r">tbl &lt;- data.table(image = lt, class = labels)
task &lt;- as_task_classif(tbl, target = &quot;class&quot;, id = &quot;dogs_vs_cats&quot;)
task
</code></pre>
<pre><code>## 
## ── &lt;TaskClassif&gt; (25000x2) ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
## • Target: class
## • Target classes: cat (positive class, 50%), dog (50%)
## • Properties: twoclass
## • Features (1):
##   • lt (1): image
</code></pre>
<pre><code class="language-r">augment &lt;- po(&quot;augment_random_vertical_flip&quot;, p = 0.5)

preprocess &lt;- po(&quot;trafo_resize&quot;, size = c(224, 224))

unfreezer &lt;- t_clbk(&quot;unfreeze&quot;,
  starting_weights = select_name(c(&quot;fc.weight&quot;, &quot;fc.bias&quot;)),
  unfreeze = data.table(epoch = 3, weights = select_all()))

resnet &lt;- lrn(&quot;classif.resnet18&quot;,
  pretrained = TRUE, epochs = 5, device = &quot;cpu&quot;, batch_size = 32,
  opt.lr = 1e-4, measures_valid = msr(&quot;classif.acc&quot;),
  callbacks = list(unfreezer, t_clbk(&quot;history&quot;)))

learner &lt;- as_learner(augment %&gt;&gt;% preprocess %&gt;&gt;% resnet)
learner$id &lt;- &quot;resnet&quot;
set_validate(learner, 1 / 3)
learner$train(task, c(12400:12600))
</code></pre>
<pre><code>## Model weights for &lt;resnet18&gt; (~45 MB) will be downloaded and processed if not
## already available.
</code></pre>
<pre><code class="language-r">learner$model$classif.resnet18$model$callbacks$history
</code></pre>
<pre><code>##    epoch valid.classif.acc
##    &lt;num&gt;             &lt;num&gt;
## 1:     1         0.4179104
## 2:     2         0.4477612
## 3:     3         0.8955224
## 4:     4         0.8955224
## 5:     5         0.9253731
</code></pre>
<pre><code class="language-r">task &lt;- tsk(&quot;melanoma&quot;)
task
</code></pre>
<pre><code>## 
## ── &lt;TaskClassif&gt; (32701x5): Melanoma Classification ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────
## • Target: outcome
## • Target classes: malignant (positive class, 2%), benign (98%)
## • Properties: twoclass, groups
## • Features (4):
##   • fct (2): anatom_site_general_challenge, sex
##   • int (1): age_approx
##   • lt (1): image
## • Groups: patient_id
</code></pre>
<pre><code class="language-r">table(task$truth())
</code></pre>
<pre><code>## 
## malignant    benign 
##       581     32120
</code></pre>
<pre><code class="language-r">task$missings(&quot;age_approx&quot;)
</code></pre>
<pre><code>## age_approx 
##         44
</code></pre>
<pre><code class="language-r">block_ffn &lt;- nn(&quot;linear&quot;, out_features = 500) %&gt;&gt;%
  nn(&quot;relu&quot;) %&gt;&gt;% nn(&quot;dropout&quot;)
path_tabular &lt;- po(&quot;select_1&quot;,
    selector = selector_type(c(&quot;integer&quot;, &quot;factor&quot;))) %&gt;&gt;%
  po(&quot;imputehist&quot;) %&gt;&gt;%
  po(&quot;encode&quot;, method = &quot;one-hot&quot;) %&gt;&gt;%
  po(&quot;torch_ingress_num&quot;) %&gt;&gt;%
  nn(&quot;block_1&quot;, block = block_ffn, n_blocks = 3)

path_image &lt;- po(&quot;select_2&quot;, selector = selector_name(&quot;image&quot;)) %&gt;&gt;%
  po(&quot;torch_ingress_ltnsr&quot;, shape = c(NA, 3, 128, 128)) %&gt;&gt;%
  nn(&quot;conv2d_1&quot;, out_channels = 16, kernel_size = 7, stride = 2,
    padding = 3) %&gt;&gt;%
  nn(&quot;batch_norm2d_1&quot;) %&gt;&gt;%
  nn(&quot;relu_1&quot;) %&gt;&gt;%
  nn(&quot;max_pool2d_1&quot;, kernel_size = 3, stride = 2, padding = 1) %&gt;&gt;%
  nn(&quot;conv2d_2&quot;, out_channels = 32, kernel_size = 3, stride = 1,
    padding = 1) %&gt;&gt;%
  nn(&quot;batch_norm2d_2&quot;) %&gt;&gt;%
  nn(&quot;relu_2&quot;) %&gt;&gt;%
  nn(&quot;conv2d_3&quot;, out_channels = 64, kernel_size = 3, stride = 1,
    padding = 1) %&gt;&gt;%
  nn(&quot;batch_norm2d_3&quot;) %&gt;&gt;%
  nn(&quot;relu_3&quot;) %&gt;&gt;%
  nn(&quot;flatten&quot;)

architecture &lt;- list(path_tabular, path_image) %&gt;&gt;%
  nn(&quot;merge_cat&quot;) %&gt;&gt;% nn(&quot;linear_1&quot;, out_features = 500) %&gt;&gt;%
  nn(&quot;relu_4&quot;) %&gt;&gt;% nn(&quot;dropout_2&quot;) %&gt;&gt;% nn(&quot;head&quot;)

model &lt;- architecture %&gt;&gt;%
  po(&quot;torch_loss&quot;,
    t_loss(&quot;cross_entropy&quot;, class_weight = torch_tensor(10))) %&gt;&gt;%
  po(&quot;torch_optimizer&quot;, t_opt(&quot;adamw&quot;, lr = 0.0005)) %&gt;&gt;%
  po(&quot;torch_model_classif&quot;, epochs = 4, batch_size = 32, device = &quot;cpu&quot;,
    predict_type = &quot;prob&quot;)

preprocessing &lt;- po(&quot;classbalancing&quot;, ratio = 4, reference = &quot;minor&quot;,
    adjust = &quot;minor&quot;) %&gt;&gt;%
  po(&quot;augment_random_horizontal_flip&quot;) %&gt;&gt;%
  po(&quot;augment_random_vertical_flip&quot;) %&gt;&gt;%
  po(&quot;augment_random_crop&quot;, size = c(128, 128), pad_if_needed = TRUE)
glrn &lt;- as_learner(preprocessing %&gt;&gt;% model)

library(&quot;mlr3viz&quot;)
glrn$id &lt;- &quot;multimodal&quot;
task_subset &lt;- task$clone(deep = TRUE)
subset &lt;- partition(task_subset, ratio = 0.1)$train
task_subset$filter(subset)
rr &lt;- resample(task_subset, glrn, rsmp(&quot;holdout&quot;))
plt &lt;- autoplot(rr, type = &quot;roc&quot;)
</code></pre>
<pre><code>## Warning in ggplot2::fortify(object, raw_curves = raw_curves, reduce_points = reduce_points): Arguments in `...` must be used.
## ✖ Problematic argument:
## • raw_curves = raw_curves
## ℹ Did you misspell an argument name?
</code></pre>
<pre><code class="language-r">saveRDS(plt, here::here(&quot;paper&quot;, &quot;roc.rds&quot;))
Sys.time()
</code></pre>
<pre><code>## [1] &quot;2025-11-07 09:51:43 UTC&quot;
</code></pre>
<pre><code class="language-r">sessionInfo()
</code></pre>
<pre><code>## R version 4.5.1 (2025-06-13)
## Platform: x86_64-pc-linux-gnu
## Running under: Ubuntu 24.04.3 LTS
## 
## Matrix products: default
## BLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 
## LAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.26.so;  LAPACK version 3.12.0
## 
## locale:
##  [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              
##  [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    
##  [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   
##  [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 
##  [9] LC_ADDRESS=C               LC_TELEPHONE=C            
## [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       
## 
## time zone: Etc/UTC
## tzcode source: system (glibc)
## 
## attached base packages:
## [1] stats     graphics  grDevices utils     datasets  methods   base     
## 
## other attached packages:
## [1] mlr3viz_0.10.1      torchdatasets_0.3.1 mlr3tuning_1.4.0   
## [4] paradox_1.0.1       mlr3torch_0.3.2     torch_0.16.3       
## [7] future_1.67.0       mlr3pipelines_0.9.0 mlr3_1.2.0         
## 
## loaded via a namespace (and not attached):
##  [1] gtable_0.3.6         xfun_0.54            ggplot2_4.0.0       
##  [4] processx_3.8.6       callr_3.7.6          vctrs_0.6.5         
##  [7] tools_4.5.1          ps_1.9.1             safetensors_0.2.0   
## [10] curl_7.0.0           parallel_4.5.1       tibble_3.3.0        
## [13] pkgconfig_2.0.3      data.table_1.17.8    checkmate_2.3.3     
## [16] RColorBrewer_1.1-3   S7_0.2.0             assertthat_0.2.1    
## [19] uuid_1.2-1           lifecycle_1.0.4      compiler_4.5.1      
## [22] farver_2.1.2         stringr_1.6.0        precrec_0.14.5      
## [25] codetools_0.2-20     bbotk_1.7.1          pillar_1.11.1       
## [28] crayon_1.5.3         rpart_4.1.24         parallelly_1.45.1   
## [31] digest_0.6.37        stringi_1.8.7        listenv_0.10.0      
## [34] mlr3measures_1.1.0   rprojroot_2.1.1      grid_4.5.1          
## [37] here_1.0.2           cli_3.6.5            magrittr_2.0.4      
## [40] future.apply_1.20.0  withr_3.0.2          scales_1.4.0        
## [43] backports_1.5.0      rappdirs_0.3.3       bit64_4.6.0-1       
## [46] globals_0.18.0       jpeg_0.1-11          bit_4.6.0           
## [49] evaluate_1.0.5       knitr_1.50           torchvision_0.8.0   
## [52] mlr3misc_0.19.0      rlang_1.1.6          Rcpp_1.1.0          
## [55] zeallot_0.2.0        glue_1.8.0           palmerpenguins_0.1.1
## [58] coro_1.1.0           jsonlite_2.0.0       lgr_0.5.0           
## [61] R6_2.6.1             fs_1.6.6
</code></pre>
</div>
</body>
</html>
