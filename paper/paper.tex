\documentclass[article, nojss]{jss}

%% -- LaTeX packages and custom commands ---------------------------------------

%% recommended packages
\usepackage{thumbpdf,lmodern}

%% another package 
\usepackage{framed}
\usepackage{amsmath,amsthm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{rotating}
\usepackage{orcidlink}
\usepackage{enumerate}
\usepackage{float} % to allow H for figure position
\usepackage{longtable}
\usepackage[tableposition=below]{caption}
\captionsetup[longtable]{skip=0.9em}
\usepackage{subcaption}
\usepackage{cleveref}
\usepackage{todonotes}
\usepackage{enumitem}


%arxiv
\usepackage{lmodern}

% track changes
\usepackage[]{changes}

%Algorithm
\usepackage{algorithm}
\usepackage{algorithmic}

%% new custom commands
\newcommand{\class}[1]{`\code{#1}'}
\newcommand{\fct}[1]{\code{#1()}}
\theoremstyle{definition}
\newtheorem{definition}{Definition}

\input{notation.tex}


%% -- Article metainformation (author, title, ...) -----------------------------

%% - \author{} with primary affiliation
%% - \Plainauthor{} without affiliations
%% - Separate authors by \And or \AND (in \author) or by comma (in \Plainauthor).
%% - \AND starts a new line, \And does not.
\author{Sebastian Fischer \And Lukas Burk \And Florian Pfisterer \And Carson Zhang \AND Bernd Bischl \And Martin Binder}

\Plainauthor{S. Fischer, L. Burk, F. Pfisterer, C. Zhang, B. Bischl, M. Binder}

%% - \title{} in title case
%% - \Plaintitle{} without LaTeX markup (if any)
%% - \Shorttitle{} with LaTeX markup (if any), used as running title
\title{mlr3torch: A Deep Learning Framework in R based on mlr3 and torch}
\Plaintitle{mlr3torch: A Deep Learning Framework in R based on mlr3 and torch}
\Shorttitle{mlr3torch: A Deep Learning Framework in R based on mlr3 and torch}


%% - \Abstract{} almost as usual
\Abstract{
    mlr3torch
}

%% - \Keywords{} with LaTeX markup, at least one required
%% - \Plainkeywords{} without LaTeX markup (if necessary)
%% - Should be comma-separated and in sentence case.
\Keywords{deep learning, machine learning, \proglang{R}}
\Plainkeywords{deep learning, machine learning, R}

%% - \Address{} of at least one author
%% - May contain multiple affiliations for each author
%%   (in extra lines, separated by \emph{and}\$.
%% - May contain multiple authors for the same affiliation
%%   (in the same first line, separated by comma).
\Address{
  Adress \\
  Insitut f\"ur Statistik \\ Ludwig-Maximilians-Universit\"at M\"unchen, Germany \\
  Ludwigstr. 33, 80539 Munich, Germany \\
  Munich Center for Machine Learning (MCML), Germany \\
  E-mail: \email{sebastian.fischer@stat.uni-muenchen.de}
}

% https://www.jstatsoft.org/mission#types-of-papers
% Full reproducibility is mandatory for publication and the source code is published along with the article.
%  A careful comparison with other open-source implementations of similar models or procedures should highlight the capabilities of all implementations and the corresponding advantages or disadvantages.
% A description of the design principles and the actual implementation is at the heart of an JSS article.

% All new submissions must provide the following attachments:

%    PDF manuscript in JSS style,
%    source code for the software and
%    replication materials for all results from the manuscript, preferably via a single, commented standalone replication script.


% Software Preparation: 
% Source code must be submitted in ASCII files. 
% For R packages, we encourage inclusion of JSS submissions as vignettes in the package.
% Maximum Upload Size: 50 MB

% Style guide: https://www.jstatsoft.org/style

% Paper should not be longer than 30 pages

\begin{document}

\begin{itemize}
    \item Should we really add all the things into the appendix? Probably for those things that are accessible online, we can just reference the documentation pages (e.g. all available parameters of a torch learner). We have to hand in the package anyway when submitting to JSS.
    \item Is preprocessing and data augmentation properly explained
    \item Are there enough pretty images?
    \item british or american?
    \item add overviews to appendix and reference them in main part
    \item Ensure consistent <- over = 
    \item ensure that caching is enabled because otherwise temp directory might be deleted for the melanoma dataset on linux
    \item Use nn("linear") instead of po("nn\_linear") everywhere (requires mlr3misc release because of bug)
    \item Cite all datasets
    \item use + in multi-line code (see other submissions)
    \item use proglang and pkg everywhere
    \item Check spelling for programming languages and libraries
    \item device good cursor 
    \item check consistent references: does JSS have recommendations on how to cite github software?
    \item Ensure correct and consistent spelling of programming language and libraries
    \item Also cite the programming languages
    \item deep learning mit DL abkürzen: machine learning mit ML
    \item <- and = consistency
    \item cite R6
    \item all datasets cited?
    \item write code that converts latex into a single reproducible script.
    \item is po("nn\_") and nn("") erklärt?
\end{itemize}




\section{Introduction and related work}

Deep learning has profoundly influenced diverse fields ranging from computer vision and audio to natural language processing (NLP).
In recent years, there has also been an increased focus on the application of deep learning to tabular data \citep{borisov2022deep}.
To train deep neural networks, software is needed that supports hardware-accelerated tensor operations and automatic differentiation.
Together, these two components allow to define neural network architectures and train them using gradient-based optimization.
However, the usage of these toolboxes it not restricted to the training of deep neural networks and can, e.g., also be used as an alternative approach to estimating other statistical models, such as L1-regularized linear regression \citep{kolb2025deep}.

The dominant programming language for deep learning is \proglang{Python}, for which the libraries \pkg{TensorFlow} \citep{abadi2016tensorflow}, \pkg{PyTorch} \citep{paszke2019pytorch}, and \pkg{JAX} \citep{bradbury2018jax} are primarily developed.
Historically, \pkg{TensorFlow} popularized static computation graphs, enabling optimized deployment, while \pkg{PyTorch} gained traction due to its dynamic graph approach, prioritizing flexibility and allowing for easier debugging.
Nowadays, \pkg{TensorFlow} also supports eager execution, while \pkg{PyTorch} also supports static compilation, making it apparent that both approaches have their merit.
More recently, \pkg{JAX} introduced a functional approach that can be described as \pkg{NumPy} \citep{harris2020array} enhanced with hardware acceleration and automatic differentiation \citep{bradbury2018jax} and as such focuses even more on the bare essentials.
Another noteworthy library is \pkg{tinygrad} \citep{tinygrad}, which strives to be a minimal, lightweight deep learning framework designed for simplicity and easy extensibility across multiple hardware platforms.

All of these libraries focus mostly on the core building blocks of deep learning, leaving higher-level abstractions such as data loading, training, validation and testing loops, model management, preprocessing operations, or experiment tracking to other frameworks. 
As such \pkg{Keras} \citep{chollet2018keras} is a high-level API that is both integrated into \pkg{TensorFlow}, but which is also available as a standalone multi backend library with support for \pkg{PyTorch}, \pkg{TensorFlow} and \pkg{JAX}. Furthermore, \pkg{PyTorch Lightning} \citep{falcon2019pytorch} and \pkg{fastai} provide higher level deep learning toolboxes exclusively for \pkg{PyTorch}, and \pkg{Trax} \citep{trax} offers similar capabilities for \pkg{JAX}.

Beyond these generic deep learning toolkits, there are also libraries focussing on offering pre-trained state-of-the-art models for different modalities and task types.
One example for this is the Hugging Face \pkg{transformers} library \citep{wolf-etal-2020-transformers}, or the \pkg{torchvision} \citep{marcel2010torchvision} and \pkg{torchaudio} \citep{yang2022torchaudio} extensions for \pytorch.
Furthermore, \pkg{Detectron2} is a platform for object detection, segmentation and other visual recognition tasks \citep{wu2019detectron2}.

Other language also offer support for deep learning.
These include \proglang{Julia}, which offers a full deep learning stack via \flux{} \citep{innes2018flux}, \proglang{Rust} with its crates \pkg{Burn} \citep{burn} and \pkg{Candle} \citep{candle}, and \proglang{Go} with \pkg{gomlx} \citep{gomlx}.
Furthermore, in \proglang{\cpp{}} there exists \pkg{LibTorch} which is the \cpp backend for \pytorch{}, but which can also be used as a standalone \cpp{} library.

In the \proglang{R} language various deep learning packages are available, including \pkg{keras3} \citep{keras3} and \pkg{tensorflow} \citep{r-tensorflow}, which both wrap the corresponding \pytorch{} libraries via \pkg{reticulate} \citep{reticulate}.
With \pkg{torch} (cite), \proglang{R} also has a native deep learning framework built on top of \pkg{LibTorch}.
Furthermore, \pkg{luz} \citep{luz} and and \pkg{cito} \citep{cito} are two higher level deep learning frameworks for R.

Besides specialized deep learning toolkits, there also are more general purpose ML libraries with mostly a focus on tabular data.
In \python, there is \sklearn{} \citep{pedregosa2011scikit}, in \julia{},  \mlj{} offers such capabilities, and \rlang{} has \tidymodels{} \citep{tidymodels} and \pkg{mlr3} \citep{ref-mlr32019} with their respective predecessors \pkg{caret} \citep{caret} and \pkg{mlr} \citep{bischl2016mlr}.
For the API of these frameworks, deep learning methods are just one type of learning method besides other approaches such as statistical modelding, support vector machines or tree-based systems.
To connect deep learning approaches to these generic APIs, connector packages exist, such as \pkg{skorch} \citep{tietz2017skorch} for integrating \pkg{PyTorch} into \sklearn{}, or similarly \pkg{MLJflux} \citep{MLJFlux} for \flux{} and \mlj, and \pkg{brulee} \citep{brulee} for \torch{} and \tidymodels.

\paragraph{Contributions} The \pkg{mlr3torch} package adds extensive deep learning capabilities to the \mlrt{} machine learning framework, which is a collection of \rlang{} packages providing a unified interface to ML in \rlang{}.
\Cref{fig:mlr3-ecosystem} gives an overview of the complete ecosystem.
Albeit more complete and extensible, it can be seen as the spiritual successor to the \pkg{mlr3keras} \citep{ref-mlr3keras2021} extension.
The package offers a unified learner interface for defining and training neural networks, which by being integrated into the ecosystem allows extends to support for operations like resampling, benchmarking, or hyperparameter tuning \citep{mlr3tuning}.
It currently supports working with tabular data and generic tensors for supervised classification and regression.
Neural network architectures can be defined with different levels of customizability.
First, predefined architectures are available, such as vision learners from the \pkg{torchvision} \rlang{} package \citep{r-torchvision}, or tabular architectures.
Furthermore, \mlrttorch{} allows to represent neural network architectures using the graph language defined in \mlrtpipelines{}, thereby making it possible to represent the whole modelling workflow, including preprocessing, data augmentation, as well as the neural network architecture in a single workflow description, similar to how it is possible in \keras{} \citep{chollet2018keras}.
Finally, it is also possible to easily convert network architectures from \torch{} into an \mlrt{} learner.
The package is designed with extensibility in mind: the training loop is customizable via a callback mechanism and it is easily possible to inherit from predefined classes for full control.
Moreover, the package should offer an intuitive interface for users with prior experience in the \mlrt{} framework.

In the next sections, we give an overview of the package, starting with a brief overview on deep learning and the main \rlang{} packages \mlrttorch{} builds upon in \Cref{sec:background}.
Afterwards, \Cref{sec:building-blocks} will summarize the main building blocks of the package, which includes the \emph{data representation}, the \emph{learner class}, as well as the \emph{representation of neural network learners as graphs}.
In \Cref{sec:extending} we will show how to extend the package, which is followed by some practical use cases in \Cref{sec:use-cases}.
There, we will cover \emph{hyperparameter tuning}, \emph{fine tuning}, and the construction of neural networks for \emph{multi modal problems}.
To give an understanding of the runtime performance of the package, we will present some runtime performance benchmarks in \Cref{sec:benchmarks}.
We summarize the package and future directions in \Cref{sec:summary}.


\begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{figures/mlr3verse.png}
    \caption{Overview of the \mlrt{} ecosystem}
    \label{fig:mlr3-ecosystem}
\end{figure}

\section{Background}\label{sec:background}


\subsection{Deep Learning}\label{sec:background_dl}
% Other JSS publications also have this. This should be relatively short.
% I think mayb

Deep learning is an area of machine learning that uses neural networks to model complex, hierarchical patterns in data.
The impact of neural networks on fields such as computer vision, natural language processing, and speech recognition in the last twelve years can hardly be overstated.

Many neural network architectures can be represented as directed acyclical graphs (DAGs)

\begin{equation}
    \mathcal{G} = (\mathcal{V}, \mathcal{E}),
\end{equation}

where $\mathcal{V} = \{v_1, \ldots, v_n\}$ is the set of vertices, corresponding to parameterized layers or operations, and $\mathcal{E} \subseteq \mathcal{V} \times \mathcal{V}$ is the set of directed edges, representing data flow between nodes.

Each layer's transformation $v^i \in \mathcal{V}$ applies a function $f^i(\cdots; \theta_i)$ that takes as inputs the outputs of the parent layers as defined by the edges $e_{ij} = (v_i, v_j) \in \mathcal{E}$.
Common operations include linear functions, activations, convolutions, recurrent computations, attention mechanisms, normalization layers, pooling operations, or other specialized functions.
With $f(\cdots; \theta)$ denoting the whole network, the goal is to find a parameter vector $\theta$ that minimizes a loss function $\mathcal{L}$ under some distribution $\P$.

\begin{equation}
\theta^* = \arg \min_{\theta} \mathbb{E}_{(x, y) \sim \P} \left[ \mathcal{L}(f_{\theta}(x), y) \right]
\end{equation}

This is done by minimizing the empirical loss for a dataset $\mathcal{D} = \{ (\mathbf{x}_i, \mathbf{y}_i) \}_{i=1}^N$ that was sampled from $\P$.
Optimization is typically performed using gradient-based methods such as stochastic gradient descent (SGD).

\subsection{mlr3, mlr3pipelines and torch}

The \pkg{mlr3torch} R package builds mainly upon the R packages \pkg{mlr3} \citep{ref-mlr32019}, \pkg{mlr3pipelines} \citep{ref-mlr3pipelines2021} and \pkg{torch} \citep{ref-torch} which we will briefly review in this section.

The \pkg{mlr3} package and its associated ecosystem offer a versatile, object-oriented, and extendable framework for various machine learning tasks in the \proglang{R} programming language \citep{ref-RCore2021}, including regression, classification and other tasks.
The ecosystem primarily relies on the \pkg{R6} class system for object orientation \citep{ref-r6}.
% maybe mention dictionary retrieval?
At its core, this integrated interface allows users access, train and evaluate a wide range of machine learning algorithms implemented in the \proglang{R} ecosystem.

The example below shows how it can be used to evaluate a simple regression tree \texttt{Learner} from the \pkg{rpart} package \citep{ref-rpart2019} on the predefined mtcars example \texttt{Task}.
We train the learner on $2/3$ of the training data and make predictions on the remaining observations.
The resulting predictions are then evaluated using the root-mean-square measure.

\begin{CodeInput}
R> library(mlr3)
R> set.seed(42)
R> task <- tsk("mtcars")
R> learner <- lrn("regr.rpart")
R> split <- partition(task, ratio = 2/3)
R> learner$train(task, split$train)
R> pred <- learner$predict(task, split$test)
R> measure <- msr("regr.rmse")
R> pred$score(measure)
\end{CodeInput}
\begin{CodeOutput}
regr.rmse 
 4.736051 
\end{CodeOutput}


The \pkg{mlr3pipelines} extension enables the construction of learning pipelines as computational graphs, which, e.g., facilitates the integration of preprocessing steps into the modelling workflow.
The nodes in such a \texttt{Graph} inherit from class \texttt{PipeOp}, which, just like the \texttt{Learner} class, have a train and predict method.
We can construct such a graph by combining individual components via the graph-concatenation operator \texttt{\%>>\%}.
A \texttt{Graph} can be converted back to a \texttt{GraphLearner} which inherits from \texttt{mlr3::Learner} and can be used as such.
Below, we combine a principal component preprocessing step with the decision tree learner.

\begin{CodeInput}
R> library(mlr3pipelines)
R> graph_learner <- as_learner(po("pca") %>>% lrn("regr.rpart"))
\end{CodeInput}

The \pkg{mlr3} ecosystem also has many other components, including packages for hyperparameter optimization, feature selection or analysis tools.
For a comprehensive overview of the \pkg{mlr3} ecosystem, we refer readers to the freely accessible book\footnote{\url{https://mlr3book.mlr-org.com/}} \citep{ref-mlr3book}.

The \pkg{torch} packages is used as the computational backbone for training neural networks in \pkg{mlr3torch}.
It closely resembles the PyTorch \python{} library \citep{ref-pytorch}.
At its heart the package offers support for GPU accelerated tensor operations, an autograd system, as well as many different tensor operations and optimizers commonly used in deep learning.
In the example below, we perform a simple multiplication and obtain the gradient with respect to one of its inputs.

\begin{CodeInput}
R> library(torch)
R> x <- torch_tensor(1, device = "cpu")
R> w <- torch_tensor(2, requires_grad = TRUE, device = "cpu")
R> y <- w * x
R> y$backward()
R> w$grad
\end{CodeInput}
\begin{CodeOutput}
torch_tensor
 1
[ CPUFloatType{1} ]
\end{CodeOutput}

\section{Building Blocks of mlr3torch}\label{sec:building-blocks}

\subsection{Data Representation}

\subsubsection{Lazy Tensor}

% Lazy Tensor
In the \pkg{mlr3} ecosystem, the machine learning problem and dataset is represented via the \pkg{R6} class \texttt{Task}.
An instance of the class contains the data -- usually represented in tables -- and metadata such as the task type, the target column, and the feature names.
In order to support not only standard tabular features, but also generic torch tensors, \pkg{mlr3torch} defines the \texttt{lazy\_tensor} type, which represents a vector of tensors with possibly heterogeneous shapes.
It is built upon the \texttt{torch::dataset} class, which means that the vector class only needs to define how to construct its elements, but not necessarily store them in memory, hence the name 'lazy'.

We can demonstrate this using the MNIST digit classification example problem \citep{ref-mnist}.

\begin{CodeInput}
R> library(mlr3torch)
R> mnist = tsk("mnist")
R> mnist
\end{CodeInput}
\begin{CodeOutput}
<TaskClassif:mnist> (70000 x 2): MNIST Digit Classification
* Target: label
* Properties: multiclass
* Features (1):
  - lt (1): image
\end{CodeOutput}

The underlying data has two columns, one of which is a \texttt{factor} column representing the class labels and a \texttt{lazy\_tensor} column for the images:

\begin{CodeInput}
R> rows <- mnist$data(1:2)
R> rows
\end{CodeInput}
\begin{CodeOutput}
  label           image
 <fctr>    <lazy_tensor>
1:    5  <tnsr[1x28x28]>
2:    0  <tnsr[1x28x28]>
\end{CodeOutput}

The reason the target column is not represented as a \texttt{lazy\_tensor} is that this allows for interoperability with other parts of the \texttt{mlr3} framework, including the definition of measures, stratification of resamplings or target transformations.

The conversion from a \texttt{lazy\_tensor} to a \texttt{torch\_tensor} is possible via the \texttt{materialize} function.

\begin{CodeInput}
str(materialize(rows[[2]]))
\end{CodeInput}
\begin{CodeOutput}
List of 2
 $ :Float [1:1, 1:28, 1:28]
 $ :Float [1:1, 1:28, 1:28]
\end{CodeOutput}

The construction of a \texttt{lazy\_tensor} from a \texttt{torch::dataset} is in possible through the \texttt{as\_lazy\_tensor} converter.

\subsubsection{Data Preprocessing and Augmentation}

One key feature of the \texttt{lazy\_tensor} is that it can be modified by objects inheriting from \texttt{mlr3pipelines::PipeOp}.
Below, we reshape the two-dimensional images into a dense vector.

\begin{CodeInput}
R> po_flat = po("trafo_reshape", shape = c(-1, 28 * 28))
R> mnist_flat = po_flat$train(list(mnist))[[1L]]
R> mnist_flat$head(2)
\end{CodeInput}
\begin{CodeOutput}
    label         image
   <fctr> <lazy_tensor>
1:      5   <tnsr[784]>
2:      0   <tnsr[784]>
\end{CodeOutput}

The same reshaping would also be applied when calling the \texttt{\$predict()} method of the pipeop.

Analogously, data augmentation steps are also available as \texttt{PipeOp}s, but their identifiers are prefixed with \texttt{"augment\_"} instead of \texttt{"trafo\_"}. 

\begin{CodeInput}
R> po_maybe_flip <- po("augment_random_horizontal_flip")
\end{CodeInput}

Both types of data processing steps inherit from the \texttt{PipeOpTaskPreprocTorch} class.
They only differ with respect to the stages during which they are active: preprocessing operations are applied during both training and prediction, whereas data augmentation is only active during the former.

Just like the data representation itself, these preprocessing steps are not applied eagerly, but lazily.
They build up an internal preprocessing graph that will be executed when the lazy tensor is materialized.

\subsection{LearnerTorch}

The \texttt{LearnerTorch} \pkg{R6} class is the abstract base class that inherits from the \texttt{mlr3::Learner} class and defines the training and prediction logic, currently only for supervised and regression.
Each learner in \pkg{mlr3torch} inherits from this base class and implements the network architecture, supported feature types, and how to construct the \texttt{torch::dataloader} for the input \texttt{Task}.
One simple example is the \texttt{LearnerTorchMLP} which represents simple multi layer perceptron (MLP) with dropout.

\subsubsection{Configuration}

A torch learner is configurable through \emph{construction arguments}, \emph{hyperparameters} and \emph{fields}.

The construction arguments are the
\begin{itemize}
    \item \texttt{loss} :: \texttt{TorchLoss}, which wraps a \texttt{torch::nn\_loss} and defines the loss function for training,
    \item \texttt{optimizer} :: \texttt{TorchOptimizer}, which wraps a \texttt{torch::torch\_optimizer\_generator} and defines how the network parameters are optimized, and 
    \item \texttt{callbacks} :: \texttt{list()} of \texttt{TorchCallback}s, which allow to customize the training process.
\end{itemize}

All three object types are configurable via their own hyperparameters, such as the learning rate of an optimizer.
While \pkg{mlr3torch} comes with predefined and ready-to-use optimizers, loss functions and callbacks (see \Cref{app:loss_opts_cbs}), adding custom versions thereof is also supported and is shown in \Cref{sec:extending}.

Below, we construct a \texttt{LearnerTorchMLP} classifier that uses cross entropy as the loss function, Stochastic Gradient Descent (SGD) as the optimizer and stores the training history:

\begin{CodeInput}
R> library(mlr3torch)
R> mlp = lrn("classif.mlp",
+   loss = t_loss("cross_entropy"),
+   optimizer = t_opt("sgd", lr = 0.01),
+   callbacks = t_clbks("history")
+ )
\end{CodeInput}

The hyperparameters of the learner as well as of the loss, optimizer and callbacks are represented as a \texttt{paradox::ParamSet} \citep{ref-paradox2024}.
The \pkg{paradox} package provides a comprehensive framework for defining parameter spaces.
It supports a variety of parameter types, hierarchical dependencies, and constraint handling, making it a powerful tool for hyperparameter optimization and configuration management.

The parameter set of the composed learner contains the parameter union from the specified loss, optimizer and callbacks, some parameters that are shared among all \texttt{LearnerTorch} classes, as well as learner-specific parameters that, e.g., parameterize the network architecture.
To avoid name clashes, the loss, optimizer and callback parameters are prefixed by \texttt{"loss."}, \texttt{"opt."}  and \texttt{"cb."} respectively.
A detailed description of the parameters shared between all classes inheriting from \texttt{LearnerTorch} is given in \Cref{app:learner_ps}.

In the MLP example, the learner-specific parameters include the latent dimension, the activation function, and the dropout probability.
The parameter values can be set via the \texttt{\$set\_values()} method that mutates the object in-place.

\begin{CodeInput}
R> mlp$param_set$set_values(
+   epochs = 10, batch_size = 16, device = "cpu",
+   neurons = c(100, 200), activation = torch::nn_relu,
+   p = 0.3, opt.nesterov = TRUE
+ )
\end{CodeInput}

Furthermore, torch learners can be configured through \emph{fields}, which distinguish themselves from parameters as they have predefined semantics that are shared between all \texttt{mlr3::Learner}s.
Important ones are the predict type (\texttt{\$predict\_type}), how to construct the validation data (\texttt{\$validate}), whether to conduct the model training in an external process (\texttt{\$encapsulation)}), and which learner to use when the primary learner fails (\texttt{\$fallback}).
Below, we configure the learner to make probability predictions via the setter \texttt{\$configure()} that allows to set both fields and parameters.

\begin{CodeInput}
R> mlp$configure(
+    predict_type = "prob"
+  )
\end{CodeInput}

Finally, \texttt{LearnerTorch} class is designed with extensibility in mind, allowing developers to create specialized torch learners by inheriting from the abstract base class. We show how to do so in section \Cref{sec:extending}.

\subsubsection{Training and Prediction}

The primary interface for using a \texttt{LearnerTorch} is through its \texttt{\$train()} and \texttt{\$predict()} methods.
The former takes as input a \texttt{Task} and stores the trained model in its \texttt{\$model} field.
The \texttt{\$predict()} method takes in an \texttt{mlr3::Task} and outputs an \texttt{mlr3::Prediction} object.

Below, we train the MLP learner on the flattened MNIST task from above:

\begin{CodeInput}
R> mlp$train(mnist_flat, row_ids = 1:60000)
\end{CodeInput}

After training, we can access the training results via the \texttt{\$model} slot of the learner.
Most importantly, the model contains the trained network, a \texttt{nn\_module}, the optimizer state, as well as the states of the callbacks.

\begin{CodeInput}
R> mlp$model$network
\end{CodeInput}

Once a learner is trained, it can be used to make predictions on unseen data.

\begin{CodeInput}
R> pred <- learner$predict(mnist_flat, row_ids = 60001:70000)
R> pred
\end{CodeInput}

\begin{CodeOutput}
\end{CodeOutput}

\subsubsection{Learner Characteristics}\label{sec:learner-characteristics}

Just like any other \texttt{mlr3::Learner}, learners in \pkg{mlr3torch} are annotated with metainformation that characterizes their capabilities.
This allows \pkg{mlr3} to perform correctness and compatibility checks that prevent user errors.
Available options for those characteristics are defined in the \pkg{mlr3} base package but are extendable via a reflection mechanism.
These learner characteristics include the task type, predict types, feature types and properties:

\begin{itemize}
    \item \texttt{task\_type} which defines on which task the learner operates. Currently, \pkg{mlr3torch} only supports regression and classification.
    \item \texttt{predict\_types} indicating the type of predictions the learner can make. Torch regression learners can currently only make response predictions and classifiers can make both response and probability predictions.
    \item \texttt{feature\_types} specify which features the learner can handle. This includes the \texttt{lazy\_tensor} type.
    \item \texttt{properties} are special requirements or abilities of the learner. An example is, e.g., the ability to perform feature selection or to handle observation weights. Important properties of torch learners will be covered below.
\end{itemize}

Three important properties supported by all torch learners are \texttt{"marshal"}, \texttt{"validation"} and \texttt{"internal\_tuning"}.

\paragraph{Marshalling} is the process of converting an in-memory object so that it can be (de)-serialized without loss of information.
Learners that have \texttt{"marshal"} property require such processing before and after (de-)serialization.
This is necessary for torch learners as their weights are \texttt{torch\_tensor}s, which are external pointers.
To save a torch learner, one therefore needs to call \texttt{\$marshal()} beforehand, and \texttt{\$unmarshal()} after saving the object.

\begin{CodeInput}
R> pth <- tempfile()
R> mlp$marshal()
R> saveRDS(mlp, pth)
R> mlp2 <- readRDS(pth)
R> mlp2$unmarshal()
\end{CodeInput}

\paragraph{Validation}

When training iterative learning procedures such as deep neural networks, one often wants to monitor the validation performance of the model during training.
In \texttt{mlr3}, this is possible through a standardized mechanism available for learners with the \texttt{"validation"} property.
Such learners have a \texttt{\$validate} field that allows to configure how to create the validation data.
One possibility is to set this to a ratio representing the proportion of data to use for validation.
™
\begin{CodeInput}
set_validate(mlp, validate = 0.3)
\end{CodeInput}

This validation split is also taken into account by preprocessing \texttt{PipeOp}s, which ensures that the prediction logic of a preprocessing \texttt{PipeOp} is applied to the validation data.

\paragraph{Internal Tuning}

While \pkg{mlr3} has extensive support for standard offline hyperparameter tuning through its extension package \pkg{mlr3tuning} \citep{ref-mlr3tuning}, some learning algorithms, including neural networks, can also internally optimize hyperparameters during training.
For deep neural networks, this is done by early stopping the number of epochs on the validation performance. 
While such internal tuning can be considered as just another hyperparameter of a learner, the standardization of this property in the \texttt{mlr3} base package allows to conveniently combine internal hyperparameter tuning with standard offline tuning.
In section \Cref{sec:tuning}, we we will demonstrate how to use this in the context of neural architecture search.

For an in-depth coverage of the validation and internal tuning mechanism, we refer to the respective chapter from the  \pkg{mlr3} book \citep{ref-mlr3book-valid}.

\subsection{Neural Networks as Graphs}

\subsubsection{Module Graph}

In \pkg{torch}, neural networks are represented by the \texttt{nn\_module} class.
To implement a custom module, a constructor method must be defined that initializes the network weights and sub-modules for the specified configuration, as well as a forward method that defines how the network transforms its inputs.
This allows for great flexibility, but there is little structure to the underlying architecture.

While \pkg{mlr3torch} also allows to create \texttt{Learner}s from generic \texttt{nn\_module}s, it also offers a systematic representation as directed acyclical graphs.
Having the architecture in a more structured way allows to, e.g., easier inspect or change individual components.
Such an architecture can be built by assembling \texttt{PipeOpModule}s in a \texttt{mlr3pipelines::Graph}.
Each \texttt{PipeOpModule} wraps an instantiated \texttt{nn\_module} and the edges of the graph define the data-flow between the layers.

Below, we define a simple feed forward network with an input dimension of $10$, a latent dimension of $100$, a ReLU activation, and an output layer that returns a scalar value.

\begin{CodeInput}
R> module_graph <- po("module_1", nn_linear(10, 100)) %>>%
R>  po("module_2", nn_relu()) %>>%
R>  po("module_3", nn_linear(100, 1))
\end{CodeInput}

To execute a forward graph for this architecture, we can call its \texttt{\$train()} method, which in this case expects one input tensor and returns one output tensor.
The \texttt{\$predict()} phase of the graph currently does nothing.

To convert this \texttt{Graph} into a \texttt{nn\_graph} which inherits from \texttt{nn\_module}, we only need to specify the input shapes.
Here the \texttt{NA} indicates that the first (batch) dimension is of unknown shape.

\begin{CodeInput}
R> nn_graph(module_graph, shapes_in = list(module_1.input = c(NA, 10))
\end{CodeInput}

\subsubsection{Meta Graph}

While the construction of module graphs by hand is possible, it requires the specification of auxiliary parameters that can be inferred, such as the input dimension of a linear layer.
Furthermore, it does not integrate with the rest of the \pkg{mlr3} ecosystem, where learning algorithms are represented via the \texttt{Learner} class.
For these reasons it is possible to construct such a module graph via another \emph{meta} graph.
This graph consists primarily of \texttt{PipeOpTorch} objects but can also contain other components.
The generator graph is for the module graph from above is specified below.
Note that we do not specify the input dimension for the first layer our the output dimension of the final layer.

\begin{CodeInput}
R> graph <- po("torch_ingress_num") %>>%
R>   po("nn_linear", out_features = 10) %>>% 
R>   po("nn_relu") %>>%
R>   po("nn_head")  
\end{CodeInput}

While the module graph operated on tensors, the input type the above graph is a \texttt{Task} during both the training and prediction stage. 
The output type during \texttt{\$train()} is a list containing a \texttt{ModelDescriptor} that is initialized by the ingress operator.
It is the intercommunication object between the different \texttt{PipeOpTorch} operators.
The model descriptor contains among other things the neural network (a graph consisting of \texttt{PipeOpModule}s, just like the one we built manually earlier), the \texttt{Task}, as well as other meta information.
When the graph is trained, the network layers (\texttt{po("nn\_<name>")}) generate a \texttt{PipeOpModule}, attach it to the graph and update the metainformation of the \texttt{ModelDescriptor}.
Auxiliary parameters such as the input dimension of a linear layer are inferred from the meta information which includes the current output shapes of the network.
The actual network that is being built up is, however, not trained.
Different ingress operators exist for different feature types, including for numeric features, categorical features and lazy tensors.
It is also possible to have more than one entry point into the neural network which we will see later.
Below, we train the graph on the flattened MNIST task freom earlier. 

\begin{CodeInput}
R> md <- graph$train(mnist_flat)[[1L]]
R> md
R> net <- md$network
R> net
\end{CodeInput}

We can extend this graph to not only build up the model descriptor but to also actually train the weights of the network for the given input task.
This requires to at least configure the optimizer and loss function in the \texttt{ModelDescriptor}.

\begin{CodeInput}
R> graph <- graph %>>%
R>   po("torch_loss", t_loss("cross_entropy")) %>>%
R>   po("torch_optimizer", t_opt("adamw", lr = 0.001))
\end{CodeInput}

To then actually run the training loop, we can add the \texttt{PipeOpTorchModel} operator to the graph.
It takes in a \texttt{ModelDescriptor}, but instead of outputing a modified version of it, converts it into a \texttt{LearnerTorchModel} and trains it on the input task.
Through its parameter set it also allows to specify the training parameters such as the batch size or the number of epochs.

\begin{CodeInput}
graph <- graph %>>% po("torch_model_classif", epochs = 10, batch_size = 16)
\end{CodeInput}

During the \emph{prediction} phase of the above graph, all \texttt{PipeOpTorch} operators simply forward their input task without modification.
Only the \texttt{PipeOpTorchModel} will call the \texttt{\$predict()} method of the \texttt{LearnerTorchModel} that was stored in its state during the training phase.
The whole process is visualized in \Cref{fig:pipeop-torch}.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/pipeop-torch.pdf}
    \caption{Training and prediction phase for a neural network learner represented as an mlr3pipelines graph.}
    \label{fig:pipeop-torch}
\end{figure}

This graph can be converted to an \texttt{mlr3::Learner} and is thereby interoperable with all other components from the \pkg{mlr3} ecosystem.

\begin{CodeInput}
R> glrn <- as_learner(graph)
\end{CodeInput}

The parameter set of the resulting learner contains the parameters of all individual \texttt{PipeOp}s from which it is build up.
This means they can be altered after construction, but most importantly be tuned over utilizing the \pkg{mlr3tuning} extension, see \Cref{sec:tuning}.

\subsubsection{More Complex Architectures}\label{sec:complex-architectures}

The architectures that can be built are not constrained to linear architectures and more complex operations on the network architecture other than adding a single layer can also be performed.

We will demonstrate how to build non-linear network segments by the use of a residual block.
We define two independent graphs, one for the linear and non-linear path respectively.
The two graphs can be combined by a merge operator that takes a variable number of inputs and combines them via an aggregator, in this case summation.

\begin{CodeInput}
R> path_lin <- po("nn_linear_1")
R> path_nonlin <- po("nn_linear_2") %>>% po("nn_relu")
R> residual_layer <- list(path_lin, path_nonlin) %>>% po("nn_merge_sum")
R> residual_layer
\end{CodeInput}

This residual layer is visualized in~\Cref{fig:residual-layer}.

Networks with more than one input (ingress) are also supported.
This can be used to define multi-modal architectures such as neural networks operating on both tables and images (c.f. \Cref{sec:multimodal}).
Here, we demonstrate how to build a tabular neural network that handles categorical features and numeric features.

We start by implementing two independent entry points into the network.
In the numeric path, we start by selecting only the numeric features of the task.
Note that this is a standard \texttt{PipeOp} from \texttt{mlr3pipelines} and it simply outputs a modified task.
Subsequently, the \texttt{po("torch\_ingress\_num")} initializes the \texttt{ModelDescriptor} that represents the numeric entry point to the network.
The first network layer then embeds the numeric features in a higher-dimensional vector.
The graph for the categorical features is constructed analogously.


\begin{CodeInput}
R> path_num <- po("select", selector_type("numeric"), id = "select_num") %>>%
R>   po("torch_ingress_num") %>>%
R>   po("nn_embed_num")
R> 
R> path_categ <- po("select", selector_type("factor"), id = "select_categ") %>>%
R>   po("torch_ingress_categ") %>>%
R>   po("nn_embed_categ")
R> 
\end{CodeInput}

To combine the two independent branches into a single network, we can merge them again, this time by concatenating the outputs of the input tensors, i.e. the embeddings of the categorical and numeric features.
The resulting network has two inputs, where the first one expects a floating point tensor and the second one an integer values tensor representing the categorical features.
The graph is visualized in~\Cref{fig:multi-inputs}.

\begin{CodeInput}
R> graph <- list(path_num, path_categ) %>>% po("nn_merge_cat") 
\end{CodeInput}

\begin{figure}[h]
    \centering
    \begin{subfigure}{0.38\textwidth}
        \centering
        \includegraphics[width=\textwidth, height = 56]{figures/residual-layer.pdf}
        \caption{Residual Layer}
        \label{fig:residual-layer}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.58\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/multi-input.pdf}
        \caption{Multiple Inputs}
        \label{fig:multi-inputs}
    \end{subfigure}
    \caption{More complex network segments.}
    \label{fig:side-by-side}
\end{figure}

Another common characteristic of neural network architectures is that a specific segment is repreated a given number of times.
This can be achieved by the \texttt{PipeOpTorchBlock} operator.
This pipeop takes in another graph primarily consisting of \texttt{PipeOpTorch} objects.
During training, the pipeop will attach the network segment repeatedly to the neural network.

\begin{CodeInput}
R> blocks <- po("nn_block", graph = residual_layer, n_blocks = 5)
\end{CodeInput}

Because \texttt{PipeOpTorchBlock} exposes both the number of repetitions (\texttt{n\_blocks}) as well as the hyperparameters of the composed graph, both can be tuned over.

\section{Extending the Package}\label{sec:extending}

While the configuration options shown in the previous section already cover a lot of ground, it is also essential to be able to customize the training further.
For this reason, \pkg{mlr3torch} allows to customize losses, optimizers, callbacks and learners.

\subsection{Loss and Optimizer}\label{sec:extending-loss-opt}

The loss and optimizer in are represented via the \texttt{TorchLoss} and \texttt{TorchOptimizer} classes.
They wrap a loss generator (such as \texttt{torch::nn\_mse\_loss}) and an optimizer generator (e.g.~a \texttt{torch::optim\_sgd}) respectively and annotate them with metainformation, most importantly, a \texttt{ParamSet}.
Constructing a custom loss or optimizer therefore requires an underlying class generator that are constructible via torch functions \texttt{torch::nn\_module} and \texttt{torch::optimizer}.
These constructors can be converted to their \texttt{mlr3torch} representation via the functions \texttt{as\_torch\_loss()} and \texttt{as\_torch\_optimizer()}.
Without specifying additional arguments, all the meta information will be inferred as good as possible.
It is also possible to provide more specific details such the types of the hyperparameters and their valid ranges.
We demonstrate this below with help of the already existing L1 loss and Adam optimizer.

\begin{CodeInput}
R> tloss <- as_torch_loss(torch::nn_l1_loss)
R> tloss
R> topt <- as_torch_optimizer(torch::optim_adam) 
R> topt
\end{CodeInput}

\begin{CodeInput}
\end{CodeInput}

\subsection{Callbacks}\label{sec:extending-callbacks}

Callbacks are a way to modify the training loop of a deep neural network without writing everything from scratch.
They are implemented via the \pkg{R6} class \texttt{TorchCallback} that wraps the \texttt{CallbackSet} class.
Analogously to the the optimizer and loss, conversion of the latter to the former is possible via \texttt{as\_torch\_callback()}.
The design of the callback mechanism is similar to how it is implemented in frameworks such as keras (cite) or luz (cite).

A \texttt{CallbackSet} is a collection of callback functions, known as hooks, that are invoked at specific points the training process to extend or customize its behaviour.
Hooks can be triggered before or after certain actions (such as after processing a batch).
Information from the training loop to the callback is communicated a \texttt{ContextTorch} object that, e.g., contains the network and optimizer or the current training or validation batch.
A callback can also store information in the learner after training by implementing the \texttt{state\_dict} and \texttt{load\_state\_dict} methods.

The \texttt{torch\_callback} helper function simplifies the creation of a \texttt{TorchCallback} object.
We demonstrate it below by implementing gradient clipping, which has two hyperparameter representing the maximally allowed norm value and the type of norm.
It only implements the hook that is run after the backward call.
After training, it keeps the history of norms in the learner state.

\begin{CodeInput}
R> gradient_clipper <- torch_callback("gradient_clipper",
R>   initialize = function(max_norm, norm_type) {
R>     self$norms <- numeric()
R>     self$max_norm <- max_norm
R>.    self$norm_type <- norm_type
R>   },
R>   on_after_backward = function() {
R>     norm <- nn_utils_clip_grad_norm_(self$ctx$network$parameters,
R>       self$max_norm, self$norm_type)
R>     self$norms <- c(self$norms, norm$item())
R>   },
R>   state_dict = function() {
R>     self$norms
R>   },
R>   load_state_dict = function(state_dict) {
R>     self$norms = state_dict
R>   }
R> )
R> gradient_clipper
\end{CodeInput}

\subsection{Learners and Task Types}\label{sec:extending-learner-task}

When the construction of custom architectures as shown in \Cref{sec:complex-architectures} is not enough, it also possible to implement custom learners by inheriting from \texttt{LearnerTorch}.
While it is in principle possible to overwrite the whole training and prediction logic (private methods \texttt{\$.train()} and \texttt{\$.predict()}), it is usually sufficient to overwrite one or more of the following methods:
\begin{itemize}
    \item \texttt{\$initialize(id, loss, optimizer, callbacks, ...)}, which needs to define learner-specific hyperparameters, as well as other metadata such as the supported feature types and task type.
    \item \texttt{\$.network(task, param\_vals)}, which is called at the beginning of the training loop to construct an instantiated \texttt{nn\_module} for the specified hyperparameters and tailored to the given input task.
    \item \texttt{\$.dataset(task, param\_vals)} that returns a \texttt{torch::dataset} for the given input task. The dataset must return a named list, where \texttt{x} is a list of tensors, \texttt{y} is the target tensor, and \texttt{.index} contains the indices.
\end{itemize}

The process of constructing such a class is described in the help page of \texttt{LearnerTorch}.

\subsubsection{Custom Learners for Classification and Regression}

In many cases, it is possible to construct a custom learner from a module constructor without implementing a new class.
This is possible via the \texttt{LearnerTorchModule} which takes arguments
\begin{itemize}
    \item \texttt{module\_generator}, which is a \texttt{torch::nn\_module\_generator} that takes as construction argument a \texttt{task} as well as other parameters.
    \item \texttt{ingress\_tokens} which is a named list of \texttt{TorchIngressTokens} that define how the data is loaded from the input task. The names must correspond to the arguments of the module's forward method.
\end{itemize}

We start by defining a simple feed forward network with to architecture parameters, the latent dimension and the number of layers.
The task construction argument is used to infer hyperparameters for the first layer (number of features) and the output layer (number of classes).

\begin{CodeInput}
nn_ffn <- nn_module("nn_ffn",
  initialize = function(task, latent_dim, n_layers) {
    dims <- c(task$n_features, rep(latent_dim, n_layers),
      length(task$class_names))
    modules <- unlist(lapply(seq_len(length(dims) - 1), function(i) {
      if (i < length(dims) - 1) {
        list(nn_linear(dims[i], dims[i + 1]), nn_relu())
      } else {
        list(nn_linear(dims[i], dims[i + 1]))
      }
    }), recursive = FALSE)
    
    self$network <- do.call(nn_sequential, modules)
  },
  forward = function(x) {
    self$network(x)
  }
}
\end{CodeInput}

Next, we need to define how the training task is converted to \texttt{torch\_tensors} that can be fed into the network.
In our case, our model will load all numeric columns (integers and doubles) as a single tensor into the network as argument \texttt{x}.
If our network should also work with categoricals or lazy tensors, we could have also used other ingress tokens such as \texttt{ingress\_categ()} or \texttt{ingress\_ltnsr()}.
The names of the tokens define as which argument they will be passed to the forward method of the module.
In our case, we have a single element \texttt{x} that will be passed as argument \texttt{x} to the \texttt{nn\_ffn} from above.

\begin{CodeInput}
num_input = list(x = ingress_num())
num_input
\end{CodeInput}

Next, we construct the learner, which also includes specification of the architecture's hyperparameters, which are part of the learner's \texttt{ParamSet}.

\begin{CodeInput}
lrn_ffn = lrn("classif.module",
  module_generator = nn_ffn,
  ingress_tokens = num_input,
  latent_dim = 100, layers = 5
)
lrn_ffn
\end{CodeInput}

After specifying the required training arguments, we can train the learner on a tas
We can, e.g., train this learner on the iris example task for which we need to configure the epochs and the batch size.

\begin{CodeInput}
lrn_ffn$configure(
  epochs = 100, batch_size = 64
)

task = tsk("iris")
lrn_ffn$train(task)
\end{CodeInput}

\subsubsection{Other Task Types}

Extending the package to other supervised learning problems such as survival analysis is straightforward and could be achieved by implementing the required loss functions as shown in~\Cref{sec:extending-loss-opt} and implementing custom dataset constructors, network architectures and prediction encoders.
Because the focus of \pkg{mlr3torch} is currently primarily on supervised learning, extending the package to non-supervised task would require more effort, including the definition of a custom \texttt{Task} class, implementation of the respective \texttt{Measure}s and the \texttt{Learner} base class.

\section{Use Cases}\label{sec:use-cases}

We will now demonstrate some of \pkg{mlr3torch}'s features by tuning a torch learner on a tabular regression problem (\Cref{sec:tuning}), finetuning a pre-trained image network from \pkg{torchvision} (\Cref{sec:finetuning}) and by constructing a network architecture for a multi-modal task that contains both tabular features and images (\Cref{sec:multimodal}).

\subsection{Neural Architecture Search}\label{sec:tuning}

In this section, we will construct a tabular neural network learner and simultaneously tune its architecture, optimizer and number of epochs.
This will be done using the predefined \emph{bike sharing} example task where the goal is to predict the number of rented bikes on a given day  \citep{bike_sharing_275}.

\begin{CodeInput}
R> task = tsk("bike_sharing")
R> task
\end{CodeInput}

\subsubsection{Defining the Learning Pipeline}
The input of our network architecture will be a single numeric tensor, so the first step in the learning pipeline is to encode the categorical features as numeric columns using the one-hot method of \texttt{PipeOpEncode}.

\begin{CodeInput}
R> preprocessing = po("encode", method = "one-hot")
\end{CodeInput}

The numeric entry-point is initialized by \texttt{PipeOpTorchIngressNum}.

\begin{CodeInput}
ingress = po("torch_ingress_num")
\end{CodeInput}

The subsequent \texttt{PipeOp}s define the layers of the neural network.
In this example, we are using an architecture that repeats the same \textit{block} multiple times.
One such block will consist of a linear operation, an activation function and a dropout layer.
We can select between different activation functions using the \texttt{ppl("branch")} construct.
This allows us to select between different branches in the graph by setting the \texttt{branch.selection} parameter.
In the example below, we allow for two choices, namely a ReLU and a sigmoid activation function.

\begin{CodeInput}
block = po("nn_linear", out_features = 32) %>>%
  ppl("branch", list(relu = po("nn_relu"), tanh = po("nn_sigmoid"))) %>>%
  po("nn_dropout")
\end{CodeInput}

This also demonstrates one of the strengths of \pkg{mlr3torch}: because \texttt{PipeOpTorch} operators are just \texttt{PipeOp}s, they can be combined with other operators from \pkg{mlr3pipelines} as long as their input and output types are compatible.

To repeat the above defined segment one or more times, we can use \texttt{PipeOpTorchBlock} which we have already covered earlier.

\begin{CodeInput}
architecture = po("nn_block", block)
\end{CodeInput}

Next, we set the loss function to MSE and the optimizer to AdamW (cite):

\begin{CodeInput}
configuration = po("torch_loss", loss = t_loss("mse")) %>>%
  po("torch_optimizer", optimizer = t_opt("adamw"))
\end{CodeInput}

The final step in the graph is the \texttt{PipeOp} that does takes in a \texttt{ModelDescriptor} and does the actual training:

\begin{CodeInput}
model = po("torch_model_regr", device = "cpu", batch_size = 128)
\end{CodeInput}

We can now assemble these pieces into the complete learning pipeline and convert it to a \texttt{Learner}:

\begin{CodeInput}
pipeline = preprocessing %>>%
  ingress %>>%
  architecture %>>%
  configuration %>>%
  model

learner = as_learner(pipeline)
learner$id = "custom_nn"
\end{CodeInput}

The resulting \texttt{GraphLearner} exposes all the hyperparameters of the individual \texttt{PipeOp}s that it was built from and over which we will now tune.

\subsubsection{Defining the Search Space}
In order to tune this learning pipeline, we now configure the search space for the architecture and the optimizer.
We will tune the latent dimension between $20$ and $500$, repeat the layer between $1$ to $10$ times with either the ReLU or sigmoid activation and use a dropout ratio in $[0.1, 0.9]$.
For the optimizer, we search for the learning rate within $[10^{-4}, 10^{-1}]$ on a logarithmic scale.

\begin{CodeInput}
learner$param_set$set_values(
  nn_block.nn_linear.out_features = to_tune(20, 500),
  nn_block.n_blocks = to_tune(1, 10),
  nn_block.branch.type = to_tune(c("relu", "tanh")),
  nn_block.nn_dropout = to_tune(0.1, 0.9),
  torch_model_regr.opt.lr = to_tune(10^-4, 10^-1, logscale = TRUE)
)
\end{CodeInput}

All the above parameters will be tuned in the standard offline way using Bayesian optimization.
In contrast, we will tune  \texttt{epoch} hyperparameter using the learner-internal early stopping procedure that was covered in \Cref{sec:learner-characteristics}.
For this start with configuring the validation data.
By setting the \texttt{\$validate} field to \texttt{"test"}, we will early stop on the test set of the resampling used for the hyperparameter tuning, i.e. the validation data.

\begin{CodeInput}
set_validate(learner, "test") 
\end{CodeInput}

Finally, we set the patience parameter, the maximum number of epochs to train for, and the validation measure.
By setting \texttt{internal = TRUE}, we specify that the epochs are tuned using early stopping.

\begin{CodeInput}
learner$param_set$set_values(
  torch_model_regr.patience = 10,
  measures_valid = msr("regr.mse"),
  torch_model_regr.epochs = to_tune(upper = 1000, internal = TRUE)
)
\end{CodeInput}

\subsubsection{Running the Tuning Procedure}

We employ bayesian optimization via the \texttt{mlr3mbo} package to explore the hyperparameter space efficiently.
The tuning process is configured to perform three-fold cross-validation and use the internal validation score, i.e. the value of the \texttt{measures\_valid} hyperparameter from above, also as the tuning measure.
We run the bayesian optimization loop for 100 iterations.

\begin{CodeInput}
ti = tune(
  tuner = tnr("mbo"),
  resampling = rsmp("cv", folds = 3),
  measure = msr("internal_valid_score", minimize = TRUE),
  learner = learner,
  term_evals = 100,
  task = task
)
ti
\end{CodeInput}

\subsection{Fine-tuning an Image Network}\label{sec:finetuning}

% What is finetuning
Instead of training a neural network from scratch, fine-tuning is a common approach in deep learning that leverages the pre-trained weights of large neural networks on new, task-specific data.
This technique allows models to adapt quickly to new domains by transferring the learned representations from large, generic datasets, to more focused applications.
Fine-tuning not only reduces training time but also improves performance, especially in scenarios where the available labelled data for the downstream task is limited.

% Summary of what we do in this section:
In this section, we demonstrate how the \pkg{mlr3torch} framework can be used to fine-tune a pre-trained image classification network from the \pkg{torchvision} package on a downstream task.
We will fine-tune the \emph{ResNet-18} model \citep{he2015deepresiduallearningimage} on the \emph{dogs vs. cats} dataset \citep{dogs-vs-cats}, where the goal is to classify images into dogs and cats.

\subsubsection{Task Definition}

As a first step, we will show how to construct the classification task, the data for which we will download using with the help of the \pkg{torchdatasets} R package \citep{torchdatasets}.

\begin{CodeInput}
library(torchdatasets)
data_dir = here::here("data")
dogs_vs_cats_dataset(data_dir, download = TRUE)
\end{CodeInput}

Next, we will create a \texttt{lazy\_tensor} vector that represents these images.
To do so, we create a custom \texttt{torch::dataset} class that stores the image paths and loads them from disk..
\pkg{mlr3torch} requires that the \texttt{\$.getitem()} method returns a named list of \texttt{torch\_tensor}s.

\begin{CodeInput}
ds = torch::dataset("dogs_vs_cats",
  initialize = function(pths) {
    self$pths = pths
  },
  .getitem = function(i) {
    list(image = torchvision::base_loader(self$pths[i]))
  },
  .length = function() {
    length(self$pths)
  }
)
\end{CodeInput}

In order to initialize this dataset, we need to create the vector containing the image paths.

\begin{CodeInput}
paths = list.files(file.path(data_dir, "dogs-vs-cats/train"), full.names = TRUE)
dogs_vs_cats = ds(paths)
\end{CodeInput}

To convert this into a \texttt{lazy\_tensor}, we can use the \texttt{as\_lazy\_tensor} converter.
Here we specify the output shapes of the images to be \texttt{NULL}, which denotes that they the images have different shapes.

\begin{CodeInput}
lt = as_lazy_tensor(ds, list(image = NULL))
head(lt)
\end{CodeInput}

Next, we construct the vector containing the true labels.
The dataset has a total of $25000$ observation with half of them being dogs and the other half being cats.
In our case, the information is encoded in the names of the files.

\begin{CodeInput}
R> labels = ifelse(grepl("dog\\.\\d+\\.jpg", paths), "dog", "cat")
R> table(labels)
\end{CodeInput}

To define the actual \texttt{Task}, we now construct a \texttt{data.table} \citep{dowle2019package} containing both the images and labels and then convert it to a task by specifying the target variable and its identifier.

\begin{CodeInput}
tbl = data.table(image = lt, class = label)
task = as_task_classif(tbl, target = "class", id = "dogs_vs_cats")
task
\end{CodeInput}

\subsubsection{Learning Pipeline}

% What we are doing in this section
The pipeline we will use to learn this task will consist of \emph{data augmentation}, \emph{preprocessing} and the \emph{neural network}.

% Note about preprocessing and data augmentation in mlr3torch
While \texttt{mlr3pipelines} already offers many preprocessing operators for tabular features, \pkg{mlr3torch} provides \texttt{PipeOp}s that operate on \texttt{lazy\_tensor} features.
These are divided into augmentation operators (prefix \texttt{"augment\_"}) and preprocessing operators (starts with \texttt{"trafo\_}). 
The full list of currently available operators is listed on the package website.\footnote{\url{https://mlr3torch.mlr-org.com/articles/preprocessing\_list.html}}
For the problem at hand, we will vertically flip the images using a probability of $0.5$.

\begin{CodeInput}
augment = po("augment_random_vertical_flip", p = 0.5)
\end{CodeInput}

For preprocessing, we will reshape each image to size $(3, 224, 224)$, which is expected by the ResNet-18 architecture.
By default this is done using bilinear interpolation.
TODO: Also apply other preprocessing such as normalization etc.

\begin{CodeInput}
preprocess = po("trafo_reshape", shape = c(NA, 3, 224, 224))
\end{CodeInput}

The next step is the ResNet-18 learner which has a parameter \texttt{pretrained} that determines whether to use the pretrained weights.
Furthermore, we instruct the learner to use one third of the data for validation and train for 10 epochs:

\begin{CodeInput}
resnet = lrn("classif.resnet18",
  pretrained = TRUE,
  epochs = 10,
  validate = 1 / 3,
  measures_valid = msr("classif.acc")
)
\end{CodeInput}



By instructing the learner to use the pretrained weights, \texttt{torchvision} will download (and cache) those weights upon first use and the torch learner will replace the output layer of the network with a new head with the correct output dimension for the task at hand, in this case two.

For better training performance, we will freeze all but the head of the network for the first three epochs and only then train the whole network.
This is possible using the \emph{unfreeze} callback, where we specify which parameters to train from the beginning and then when to unfreeze other parameters.

\begin{CodeInput}
unfreezer = t_clbk("unfreeze",
  starting_weights = c("fc.weights", "fc.bias"),
  unfreeze = data.table(
    epochs = 3, weights = select_all()
  )
)
learner$set_values(
  callbacks = list(unfreezer, t_clbk("history"))
)
\end{CodeInput}

\subsubsection{Training}

Now, we combine the three steps into a single new \texttt{GraphLearner}, train it on the task and plot the validation loss.

\begin{CodeInput}
library(ggplot2)
learner = as_learner(augment %>>% preprocess %>>% resnet)
learner$id = "resnet"
learner$train(task)
history = learner$model$classif.alexnet$callbacks$history
ggplot(history, aes(x = epoch, y = valid.classif.acc)) + 
  geom_point()
\end{CodeInput}

\subsection{Multi Modal Data}\label{sec:multimodal}

Because the graph representation of neural networks in \texttt{mlr3torch} allows for multiple inputs, it naturally handles multi modal data.
In this section, we will define a custom network architecture for the \emph{melanoma} task, where the goal is to predict whether a tumor is malignant or benign \citep{international2020siim}.
The features include an image of the melanoma, as well as the approximate age, location, and the sex of the patient.
Furthermore, the data is grouped by \texttt{patient\_id}, as each patient is on average contained around 15 times in the data.

\begin{CodeInput}
R> task <- tsk("melanoma")
R> task
<TaskClassif:melanoma> (32701 x 5): Melanoma Classification
* Target: outcome
* Properties: twoclass, groups
* Features (4):
  - fct (2): anatom_site_general_challenge, sex
  - int (1): age_approx
  - lt (1): image
* Groups: patient_id
\end{CodeInput}

Furthermore, as malignant tumors are much rarer then benign ones, the target distribution is highly unbalanced.

\begin{CodeInput}
R> table(task$truth())
malignant    benign 
      581     32120 
\end{CodeInput}

Furthermore, for some observations, the approximate age is missing

\begin{CodeInput}
R> task$missings("age_approx")
age_approx 
        44 
\end{CodeInput}


The network architecture we will create for this dataset will take one tensor input for the tabular features and one for the image, similarly to \Cref{fig:multi-inputs}.

We start by defining the graph segment for the tabular features, which consists of first selecting them, followed by sampling the mising values from their empirical distribution and then encoding them one-hot.
The three features are then fed as a single tensor into the network, where we repeat a block consisting of a linear layer, a ReLU activiation and a dropout layer, three times.

\begin{CodeInput}
R> block_ffn <- nn("linear", out_features = 500) %>>% nn("relu") %>>% nn("dropout")
R> path_tabular <- po("select_1", selector = selector_type(c("integer", "factor"))) %>>%
R>   po("imputehist") %>>%
R>   po("encode", method = "one-hot") %>>%
R>   po("torch_ingress_num") %>>%
R>   nn("block_1", block = block_ffn, n_blocks = 3)
\end{CodeInput}

For the image column, less preprocessing is required and we simply select the feature and then define three convolutional layers with batch normalization, ending in a flattening operation.

\begin{CodeInput}
R> path_image <- po("select_2", selector = selector_name("image")) %>>%
R>   po("torch_ingress_ltnsr") %>>%
R>   nn("conv2d_1", out_channels = 64, kernel_size = 7, stride = 2, padding = 3) %>>%
R>   nn("batch_norm2d_1") %>>%
R>   nn("relu_1") %>>%
R>   nn("max_pool2d_1", kernel_size = 3, stride = 2, padding = 1) %>>%
R>   nn("conv2d_2", out_channels = 128, kernel_size = 3, stride = 1, padding = 1) %>>%
R>   nn("batch_norm2d_2") %>>%
R>   nn("relu_2") %>>%
R>   nn("conv2d_3", out_channels = 256, kernel_size = 3, stride = 1, padding = 1) %>>%
R>   nn("batch_norm2d_3") %>>%
R>   nn("relu_3") %>>%
R>   nn("flatten")
\end{CodeInput}

Next, we merge the two branches by concatenation and afterwards add one more dense layer with dropout, followed by the classification head.

\begin{CodeInput}
R> architecture <- list(path_tabular, path_image) %>>%
R>   nn("merge_cat") %>>%
R>   nn("linear_1", out_features = 500) %>>%
R>   nn("relu_4") %>>%
R>   nn("dropout_2") %>>%
R>   nn("head")
\end{CodeInput}

To finalize the model configuration, we specify the loss, optimizer and remaining training arguments.
As a loss function, we use cross-entropy, but increase the weight for the positive class (malignant tumor) by a factor of 10 due to the imbalanced class distribution.§
For the optimizer, we use again AdamW, train for 4 epochs with a batch size of $32$.

\begin{CodeInput}
R> model <- architecture %>>%
R>   po("torch_loss", t_loss("cross_entropy", weight = torch_tensor(c(10, 1)))) %>>%
R>   po("torch_optimizer", t_opt("adamw", lr = 0.0005)) %>>%
R>   po("torch_model_classif", epochs = 4, batch_size = 32, device = "cuda",
R>     predict_type = "prob")
\end{CodeInput}

As further measures to address the unbalanced class distribution, we upsample the minority class by a factor, use random vertical and horizontal flipping and cropping for data augmentation.

\begin{CodeInput}
R> preprocessing <- po("classbalancing", ratio = 4, reference = "minor",
  adjust = "minor") %>>%
R>   po("augment_random_horizontal_flip") %>>%
R>   po("augment_random_vertical_flip") %>>%
R>   po("augment_random_crop", size = c(128, 128), pad_if_needed = TRUE)
R> postprocessing = po("threshold")
R> glrn = as_learner(preprocessing %>>% model)
\end{CodeInput}

Finally, we change the ID of the learner and resample it on the task using five-fold cross validation.

\begin{CodeInput}
R> glrn$id = "multimodal"
R> rr <- resample(glrn, task, rsmp("cv", folds = 5))
\end{CodeInput}

Below, we display the resulting ROC curve:

\begin{CodeInput}
autoplot(rr, type = "roc")
\end{CodeInput}




\section{Performance Evaluation mlr3torch vs. (R) torch vs. PyTorch}\label{sec:benchmarks}

Besides functionality, one important consideration of a deep learning framework is its performance, both in terms of runtime and memory usage.
To evaluate this, we ran the following benchmark experiment:


\subsection*{Experiment Setup}

The data consists of $n = 2000$ observations with $p = 1000$ features. The batch size is set to 32.
Features are drawn from a standard normal distribution, and the target variable is generated as $Y = X\beta + \epsilon$ with $\epsilon \sim \mathcal{N}(0, 0.1)$.
Models are trained using the configurations in Table~\ref{tab:config}.
The network architecture is a simple feed-forward network with ReLU (need to cite relu and tanh) activation and a latent dimension of 1000.
The training is done for 15 epochs and each experiment is repeated 4 (???) times.
We run the experiments on Linux and vary the optimizer, the implementation (different approaches and R and one in \python{} using PyTorch), as well as the number of layer and the device. (TODO: Give versions and code etc.)
For the baseline in R, we use a standard training loop and the default optimizers as initially implemented in \pkg{torch}.
The \textit approach uses R wrappers around LibTorch \cpp{} optimizers that were initially implemented in the \pkg{ignite} package (link) and then contributed to the \pkg{torch} package.
Finally, \textit{ignite} uses an approach that is only available for models that can be jit-compiled using tracing, via the \texttt{torch::jit_trace} function.
A disadvantage of this approach is that it does not work for all architectures and e.g. cannot be used for networks with variable sized inputs.


\begin{table}[h]
\centering
\caption{Parameterization of the Experiment}
\label{tab:config}
\begin{tabular}{ll}
\toprule
\textbf{Parameter} & \textbf{Values} \\
\midrule
Optimizers & \texttt{sgd}, \texttt{adam} \\
Devices & \texttt{cpu}, \texttt{cuda} \\
Frameworks & \texttt{torchoptx}, \texttt{ignite} \\
Latent Dimensions & 1000 \\
Number of Layers & 2, 4, 8, 16, 32 \\
\bottomrule
\end{tabular}
\end{table}



Each configuration is run in a separate R session to ensure isolation. Training loss is computed using Mean Squared Error (MSE), and training time per epoch is recorded. Results, including loss and timings, are saved as a CSV for further analysis.

\section{Summary and Discussion}\label{sec:summary}

\begin{itemize}
    \item Better support for text processing
    \item Performance improvements.
    \item Support for survival analysis
    \item Adding book chapters to the mlr3 book 

    
\end{itemize}

\section*{Acknowledgements}

Sebastian Fischer is supported by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) – 460135501 (NFDI project MaRDI).

\begin{itemize}
    \item Summary
    \item Strengths
    \item Current limitations
    \item Future work: Survival analysis
    \item Thank Lukas and Flo
\end{itemize}

\listoftodos

\section{Availability and Documentation}

\begin{itemize}
    \item Vignettes, but should maybe become book chapters before submission
    \item mlr-org website for an overview of available learners, layers, callbacks etc.
\end{itemize}

\subsection{Software and Code}

\begin{itemize}
    \item is available on GitHub
    \item is on CRAN
\end{itemize}

\subsection{Reproducibility}

\begin{itemize}
    \item The code for this paper must also be reproducible!
\end{itemize}

\bibliography{refs}

\begin{appendix}

\section{LearnerTorch}

\subsection{Parameter Set}\label{app:learner_ps}
\subsection{Fields}\label{app:learner_fields}

\section{Loss, Optimizers and Callbacks}\label{app:loss_opts_cbs}

\end{appendix}


\end{document}