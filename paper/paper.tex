\documentclass[article]{jss}

%% -- LaTeX packages and custom commands ---------------------------------------

%% recommended packages
\usepackage{thumbpdf,lmodern}

%% another package
\usepackage{framed}
\usepackage{dsfont}
\usepackage{amsmath,amsthm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{rotating}
\usepackage{orcidlink}
\usepackage{enumerate}
\usepackage{float} % to allow H for figure position
\usepackage{longtable}
\usepackage[tableposition=below]{caption}
\captionsetup[longtable]{skip=0.9em}
\usepackage{subcaption}
\usepackage{cleveref}
\usepackage{todonotes}
\usepackage{enumitem}
\usepackage{comment}


% track changes
\usepackage[]{changes}

%Algorithm
\usepackage{algorithm}
\usepackage{algorithmic}

%% new custom commands
\newcommand{\class}[1]{`\code{#1}'}
\newcommand{\fct}[1]{\code{#1()}}
\theoremstyle{definition}
\newtheorem{definition}{Definition}

\input{notation.tex}


%% -- Article meta-information (author, title, ...) -----------------------------

%% - \author{} with primary affiliation
%% - \Plainauthor{} without affiliations
%% - Separate authors by \And or \AND (in \author) or by comma (in \Plainauthor).
%% - \AND starts a new line, \And does not.
%\author{Sebastian Fischer \And Lukas Burk \And Florian Pfisterer \And Carson Zhang \AND Bernd Bischl \And Martin Binder}
\author{Sebastian Fischer~\orcidlink{0000-0002-9609-3197} \\
    LMU Munich \\
    MCML \\
    \And Lukas Burk~\orcidlink{0000-0001-7528-3795} \\
    LMU Munich, MCML \\
    Leibniz Institute for\\Prevention Research and\\Epidemiology - BIPS \\
    University of Bremen
    \AND Carson Zhang \\
    LMU Munich \\
    \And Bernd Bischl~\orcidlink{0000-0001-6002-6980} \\
    LMU Munich \\
    MCML \\
    \And Martin Binder~\orcidlink{0009-0008-2578-2869} \\
    LMU Munich \\
    MCML \\
}

\Plainauthor{Sebastian Fischer, Lukas Burk, Florian Pfisterer, Carson Zhang, Bernd Bischl, Martin Binder}

%% - \title{} in title case
%% - \Plaintitle{} without LaTeX markup (if any)
%% - \Shorttitle{} with LaTeX markup (if any), used as running title
\title{mlr3torch: A Deep Learning Framework in R based on mlr3 and torch}
\Plaintitle{mlr3torch: A Deep Learning Framework in R based on mlr3 and torch}
\Shorttitle{\pkg{mlr3torch}: A Deep Learning Framework in \proglang{R} based on \pkg{mlr3} and \pkg{torch}}


%% - \Abstract{} almost as usual
\Abstract{
    Deep learning (DL) has become a fundamental approach in modern machine learning (ML) with applications across various domains.
    We introduce the \proglang{R} package \pkg{mlr3torch}, a comprehensive and extensible DL framework that integrates seamlessly into the \pkg{mlr3} ecosystem.
    Built upon the \pkg{torch} package, it simplifies the creation, training and evaluation of neural networks for both tabular data and generic tensors (such as images) for classification and regression.
    While the package comes with predefined architectures and allows to easily convert \torch{} models into \mlrt{} learners, it also allows to define neural networks as graphs.
    This representation is built upon the graph language defined in the \pkg{mlr3pipelines} \proglang{R} package and allows the user to define the whole deep learning workflow, including preprocessing, data augmentation and network architecture, in a single graph.
    By being fully integrated into the \pkg{mlr3} ecosystem, the package allows for standardized resampling, benchmarking, preprocessing, hyperparameter tuning and more.
    We explain the basic functionality of the package and show how it can be customized or extended to new domains.
    Furthermore, we demonstrate the package's capabilities through applications such as neural architecture search, fine-tuning of pretrained models, and defining architectures for multi-modal data.
    Finally, we present some runtime benchmarks for \pkg{mlr3torch}.
}

%% - \Keywords{} with LaTeX markup, at least one required
%% - \Plainkeywords{} without LaTeX markup (if necessary)
%% - Should be comma-separated and in sentence case.
\Keywords{deep learning, machine learning, \rlang{}}
\Plainkeywords{deep learning, machine learning, R}

%% - \Address{} of at least one author
%% - May contain multiple affiliations for each author
%%   (in extra lines, separated by \emph{and\$.
%% - May contain multiple authors for the same affiliation
%%   (in the same first line, separated by comma).
\Address{
  Sebastian Fischer \\
  Insitut f\"ur Statistik \\ Ludwig-Maximilians-Universit\"at M\"unchen, Germany \\
  Ludwigstr. 33, 80539 Munich, Germany \\
  Munich Center for Machine Learning (MCML), Germany \\
  E-mail: \email{sebastian.fischer@stat.uni-muenchen.de}
}

% https://www.jstatsoft.org/mission#types-of-papers
% Full reproducibility is mandatory for publication and the source code is published along with the article.
%  A careful comparison with other open-source implementations of similar models or procedures should highlight the capabilities of all implementations and the corresponding advantages or disadvantages.
% A description of the design principles and the actual implementation is at the heart of an JSS article.

% All new submissions must provide the following attachments:

%    PDF manuscript in JSS style,
%    source code for the software and
%    replication materials for all results from the manuscript, preferably via a single, commented standalone replication script.


% Software Preparation:
% Source code must be submitted in ASCII files.
% For R packages, we encourage inclusion of JSS submissions as vignettes in the package.
% Maximum Upload Size: 50 MB

% Style guide: https://www.jstatsoft.org/style

% Paper should not be longer than 30 pages

\begin{document}

\begin{comment}

Fullfilled:

\begin{itemize}
    \item Code distributed with JSS articles uses the GNU General Public License (GPL) version 2 or version 3 or a GPL-compatible license. JSS does NOT consider software distributed under other licenses.
\end{itemize}

Final submission:

\begin{itemize}
    \item [ ] british or american english?
    \item [ ] Size limit: Upload not more than 50 MB
    \item [ ] submission must contain: pdf manuscript, source code, replication materials
    \item [ ] the manuscript must be fully and exactly reproducible on at least one platform
    \item [ ] We need to provide a simplified version of the script reproducing the results that can run on a regular PC. (https://www.jstatsoft.org/authors#manuscript-preparation) -> maybe I can just run this on GitHub actions CI?
    \item [ ] to facilitate review, authors are strongly encouraged to provide an output file that shows the results from running the single standalone replication script so that this can be compared against the results presented in the manuscript. For R submissions, this should be done by providing a file "code.html" created by running knitr::spin("code.R") on the replication script "code.R" which should include a call to sessionInfo() at the end.
    \item [ ] Source code must be submitted in ASCII files (but need to read software preparation guidelines again after submission: https://www.jstatsoft.org/authors#manuscript-preparation)
    \item [ ] manuscript can be compiled using pdflatex
    \item [ ] Special naming for programming languages and journals (don't think it applies here)
    \item [ ] Ensure consistent <- over =
    \item [ ] Cite all datasets
    \item [ ] use proglang and pkg everywhere
    \item [ ] Check spelling for programming languages and libraries
    \item [ ] check consistent references: does JSS have recommendations on how to cite github software?
    \item [ ] Also cite the programming languages
    \item [ ] CodeInput and CodeOutput used correctly everywhere?
    \item [ ] Run code at the end twice for reproducibility
    \item [ ] Check that we are citing the correct R packages for those packages without a paper
    \item [ ] AbkÃ¼rzungen konsistent verwenden: DL, ML, NLP etc.
    \item [ ] Use nn("linear") instead of po("nn\_linear") everywhere (requires mlr3misc release because of bug)
    \item [ ] Nothing goes beyond left and right margin
    \item [ ] max 30 pages
\end{itemize}

JSS Guides:

\begin{itemize}
    \item Should read https://stats-devguide.ropensci.org/ (more as general guidance, not a requirement for packages reviewed by JSS)
    \item !!!discuss the advantages and disadvantages of the new contribution compared to existing implementations, when possible and also applicable using empirical illustrations.
    \item Page limit: not longer than 30 pages
\end{itemize}

JSS Style guide:

\begin{itemize}
    \item [ ] Abbreviations should be spelled in upper-case letters without additional formatting (i.e., without periods, without small caps, italics, etc.). All abbreviations should be introduced with their expansion where the expansion should not be capitalized. (Exceptions are, of course, when the expansion contains proper names or the first word is the first word of a sentence.) Examples would include:
    \item [ ] never use (\cite{...}). (use \citep{} for that)
    \item [ ] \proglang, \pkg and \code have been used for highlighting throughout the paper (including titles and references), except where explicitly escaped.
    \item [ ] In order to use markup in section headers, you can use: \section[Calling C++ from R]{Calling \proglang{C++} from \proglang{R}}
    \item [ ] \title in title style
    \item [ ] \section, \subsection, etc. in sentence style (in sentence style, only first word of sentence and forst word after - or : is capitalized)
    \item [ ] annotations of figures/tables (including captions) in sentence style
    \item [ ] Figures, tables and equations are marked with a \label and referred to by \ref, e.g., Figure~\ref{...}
    Software packes are \cite{}d properly.
    \item [ ] Use \code{...} instead of \texttt{...} for inline code
    \item [ ] No comments in Code, this information should be presented in the normal latex text
    \item [ ] In all cases, code input/output must fit within the normal textwidth of the manuscript.
    \item [ ] Text in figures: In particular, this means that annotations should not be too large or too small. As a rough guidance, graphics annotation should be a about the size of the figure caption or a little bit smaller.
    \item [ ] For books with an edition, this should be indicated as 2nd, 3rd, etc.
    \item [ ] For referring to subsections, do not use Subsection x.y, just Section x.y.
    \item [ ] Add comma after e.g., and i.e., to prevent latex from interpreting as end of sentence
    \item [ ] use \top instead of ^T
    \item [ ] To refer to equations, one can use either Equation~\ref{...} (with capitalization) or (\ref{...}) with the former being preferred if the number of equation references is not too large.
\end{itemize}

References:

\begin{itemize}
    \item all titles in the BibTeX file in title style.
    \item Use \proglang{} and \pkg{} also there
    \item If software does not say provide way for citation, use  the following:
    \@Manual{SAS-STAT,
        author  = {{\proglang{SAS} Institute Inc.}},
        title   = {\proglang{SAS/STAT} Software, Version~9.1},
        year    = {2003},
        address = {Cary, NC},
        url     = {https://www.sas.com/}
    }
    \item For R packages that don't have an official citation, use citation("foo"), but make sure that \proglang{} and \pkg{} are used correctly
\end{itemize}


Captions:

\begin{itemize}
    \item All captions should appear below the corresponding figure/table.
    \item The captions should be in sentence style and end with a period. No additional formatting (such as \emph, \bf or \it) should be used for the caption.
    \item All table row/column headers should also be in sentence style. There should not be further footnote-style annotations in tables; these should all be placed in the caption.
\end{itemize}

Susanne's review:

\begin{itemize}
    \item For bullet lists/itemized lists please use either a comma, semi-colon, or period at the end of
each item
    \item Do not use additional formatting for specific words unless explicitly required by the JSS style
guide, e.g., --> remove emph and textit etc.
\end{itemize}

Non-text TODOs:

\begin{itemize}
    \item [ ] I should provide all the documentation as part of the mlr3 book before submission
    \item [ ] ensure that caching is enabled in code because otherwise temp directory might be deleted for the melanoma dataset on linux
    \item [ ] Check that networks / learners that are not trained here still can be trained and are correctly constructed
    \item [ ] check that everything is working (like created callback) when it is not explicitly run in the code
    \item [ ] Should include JIT-improvements in torch and make a release, otherwise benchmark result is a little misleading
\end{itemize}

Text TODOs:

\begin{itemize}
    \item Appendix: What should be there? And if we add it there, reference it in the main text.
    \item Upkeep with mlr3torch: different output dim
\end{itemize}

\end{comment}

%\input{1-introduction}
%\input{2-building-blocks}
%\input{3-extending}
%\input{4-use-cases}
%\input{5-runtime-evaluation}

\section{Introduction and related work}

Deep learning (DL) has profoundly influenced diverse fields ranging from computer vision and audio to natural language processing (NLP).
In recent years, there has also been an increased focus on the application of DL to tabular data \citep{ref-borisov2022deep}.
To train deep neural networks, software is needed that supports hardware-accelerated tensor operations and automatic differentiation.
Together, these two components enable the definition of neural network architectures and train them using gradient-based optimization.
However, the usage of such DL frameworks it not restricted to the training of deep neural networks and can, e.g., also be used for gaussian processes as is done in \pkg{GPyTorch} \citep{gardner2018gpytorch}.

The dominant programming language for DL is \proglang{Python} \citep{ref-van1995python}, for which the libraries \pkg{TensorFlow} \citep{ref-abadi2016tensorflow}, \pkg{PyTorch} \citep{ref-pytorch}, and \pkg{JAX} \citep{ref-jax2018github} are primarily developed.
Historically, \pkg{TensorFlow} popularized static computation graphs, enabling optimized deployment, while \pkg{PyTorch} gained traction due to its dynamic graph approach, prioritizing flexibility and allowing for easier debugging.
Nowadays, \pkg{TensorFlow} also supports eager execution, while \pkg{PyTorch} also allows for static compilation, making it apparent that both approaches have their merit.
More recently, \pkg{JAX} introduced a functional approach that can be described as \pkg{NumPy} \citep{ref-harris2020array} enhanced with hardware acceleration and automatic differentiation.
Another noteworthy \python{} library is \pkg{tinygrad} \citep{ref-tinygrad}, which strives to be a minimal, lightweight DL framework designed for simplicity and easy extensibility across multiple hardware platforms.

All of these libraries focus mostly on the core building blocks of DL, leaving higher-level abstractions such as data loading, training loops, model management, preprocessing and data augmentation, as well as experiment tracking to other frameworks.
One example for such a framework is \pkg{Keras} \citep{ref-chollet2018keras}, which offers a high-level API that is both integrated into \pkg{TensorFlow}, but which is also available as a standalone multi-backend library with support for \pkg{PyTorch}, \pkg{TensorFlow} and \pkg{JAX}. Furthermore, \pkg{PyTorch Lightning} \citep{ref-lightning2019} and \pkg{fastai} provide higher level DL toolboxes exclusively for \pkg{PyTorch}, and \pkg{Trax} \citep{ref-trax} offers similar capabilities for \pkg{JAX}.

Beyond these generic DL toolkits, there are also libraries focusing on pretrained state-of-the-art models for different modalities and task types.
One example for this is the Hugging Face \pkg{transformers} library \citep{ref-wolf-etal-2020-transformers}, or the \pkg{torchvision} \citep{ref-marcel2010torchvision} and \pkg{torchaudio} \citep{ref-yang2022torchaudio} extensions for \pytorch.
Furthermore, \pkg{Detectron2} is a platform for object detection, segmentation and other visual recognition tasks \citep{ref-wu2019detectron2}.

Languages other than \proglang{Python} also have DL libraries of their own.
These include \proglang{Julia} \citep{ref-bezanson2017julia}, which offers a full DL stack via \flux{} \citep{ref-innes2018flux}, \rust{} \citep{ref-matsakis2014rust} with its crates \pkg{Burn} \citep{ref-burn} and \pkg{Candle} \citep{ref-candle}, and \proglang{Go} \citep{ref-go} with \pkg{gomlx} \citep{ref-gomlx}.
Furthermore in \cpp{} there is \pkg{LibTorch}, the \cpp{} backend for \pytorch{}, which can also be used as a standalone \cpp{} library.

In the \proglang{R} language \citep{ref-R-base}, various DL packages are available, including \pkg{torch} \citep{ref-torch2025}, a native DL framework which is built directly on top of \pkg{LibTorch}.
The \pkg{torchvision} \rlang{} package extends this with a collection of pretrained models, datasets, and preprocessing operation for computer vision tasks \citep{ref-r-torchvision}.
Furthermore, \pkg{luz} \citep{ref-luz2023} and \pkg{cito} \citep{ref-cito2024} are two higher level DL frameworks for \rlang{} built on top of \torch{}.
Furthermore, other backends are supported via \pkg{keras3} \citep{ref-keras32025} and \pkg{tensorflow} \citep{ref-r-tensorflow2024}, which both wrap the corresponding \pyt{} libraries via \pkg{reticulate} \citep{ref-reticulate2025}.

Besides specialized DL toolkits, there are also other, more general-purpose ML libraries that mostly focus on tabular data.
In \python, there is \sklearn{} \citep{ref-pedregosa2011scikit-learn}, in \julia{} there is  \mlj{} \citep{ref-blaom2020mlj}, and \rlang{} has \tidymodels{} \citep{ref-kuhn2020tidymodels} and \pkg{mlr3} \citep{ref-mlr32019} with their respective predecessors \pkg{caret} \citep{ref-kuhn2021caret} and \pkg{mlr} \citep{ref-bischl2016mlr}, both of which are still in wide use.
From the perspective of such general ML frameworks, neural networks are just one type of learning method besides other approaches such as statistical models, support vector machines, or tree-based systems.
To connect DL methods to these generic APIs, connector packages exist, such as \pkg{skorch} \citep{ref-skorch} for integrating \pkg{PyTorch} into \sklearn{},  \pkg{MLJflux} \citep{ref-MLJFlux} for \flux{} and \mlj, and \pkg{brulee} \citep{ref-brulee2025} for \torch{} and \tidymodels.

\paragraph{Contributions} The \pkg{mlr3torch} package adds extensive DL capabilities to the \mlrt{} machine learning (ML) framework, which is a collection of \rlang{} packages providing a unified interface to ML in \rlang{}.
The package offers a unified learner interface for defining and training neural networks.
Being integrated into the \mlrt{} ecosystem facilitates easy resampling, benchmarking, or hyperparameter tuning \citep{ref-mlr3tuning2024}.
It currently supports working with tabular data and generic tensors for supervised classification and regression.
Neural network architectures can be defined with different levels of customizability.
First, predefined architectures are available, such as vision learners from the \pkg{torchvision} \rlang{} package \citep{ref-r-torchvision}, or tabular architectures \citep{gorishniy2021revisiting}.
For full control, it is possible to define custom neural network modules in \proglang{R} \torch{} and easily convert them to \mlrt{} \code{Learner}s.
Furthermore, \mlrttorch{} allows to represent neural network architectures using the graph language defined in \mlrtpipelines{}, thereby making it possible to represent the whole modelling workflow, including preprocessing, data augmentation, as well as the neural network architecture, in a single workflow description, similar to how it is possible in \keras{} \citep{ref-chollet2018keras}.
Finally, the package is designed with extensibility in mind: the training loop is customizable via a callback mechanism and it is easily possible to inherit from predefined classes for full control.

In the next sections, we give an overview of the package, starting with a brief overview on DL and the main \rlang{} packages \mlrttorch{} builds upon in \Cref{sec:background}.
Afterwards, \Cref{sec:building-blocks} will summarize the main building blocks of the package, which includes the data representation, the learner class, as well as the representation of neural networks as graphs.
In \Cref{sec:extending} we will show how to extend the package, which is followed by some practical use cases in \Cref{sec:use-cases}.
There, we will cover hyperparameter tuning, fine-tuning, and the construction of neural networks for multimodal problems.
To give an understanding of the runtime performance of the package, we will present benchmarks in \Cref{sec:benchmarks}.
We summarize the package and future directions in \Cref{sec:conclusion}.

\section{Background}\label{sec:background}


\subsection{Deep learning}\label{sec:background_dl}
% Other JSS publications also have this. This should be relatively short.
% I think maybe we should also include a section on the history of DL, i.e., how it came to be, who did what, etc.

Deep learning is an area of machine learning that uses neural networks to model complex patterns in data.
The impact of neural networks on fields such as computer vision, text generation, and speech recognition in the last years can hardly be overstated.
The simplest architecture is the multi-layer perceptron (MLP), which combines (affine-)linear transformations with non-linear activation functions:

\begin{align}
f_{\boldsymbol{\theta}}(\mathbf{x}) &=
  \phi_{L}\!\bigl(
    W_{L}\,\phi_{L-1}\!\bigl(
      W_{L-1}\,\dotsm\phi_{1}(W_{1}\mathbf{x}+b_{1}) + b_{L - 1}
    \bigr)+b_{L}
  \bigr),
\end{align}

\noindent
with \(W_{\ell}\in\mathbb{R}^{d_{\ell}\times d_{\ell-1}}\), \(b_{\ell}\in\mathbb{R}^{d_{\ell}}\) and \(\theta = \{W_1, \ldots, W_L, b_1, \ldots, b_L\}\) for \(\ell=1,\dots,L\).

More generally, almost all neural network architectures can be represented as directed acyclic graphs (DAGs).
Each node in the graph represents a possible parametrized differentiable transformation that takes in one or more tensors and outputs one or more tensors.
The edges go from layer outputs to layer inputs and define the data-flow between the operations.

One such graph -- a simple transformer-based neural network -- is depicted in \Cref{fig:intro-network}.
First, the input (e.g., a text string) is tokenized into subwords, and these tokens are then converted into vector representations through embedding.
This output tensor is then fed into an attention module and subsequently added to the original tokenized input, in a technique which is commonly called a skip-connection.
Note that the addition layer has no learnable parameters.
The output is then processed using layer normalization and followed by a linear layer, a ReLU activation, and a dropout layer, the output of which is again added to the output of the normalization layer, creating another skip-connection.
Finally, a second linear layer represents the network's output head.

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{figures/intro-network.pdf}
    \caption{Transformer-based neural network architecture represented as a graph.}
    \label{fig:intro-network}
\end{figure}

With $f_\theta$ denoting the whole network, where $\theta$ represents the union of all the parameters of the individual layers, the goal when training a neural network is to find a parameter vector $\theta^*$ that minimizes a loss function $\mathcal{L}$ under some data generating process $\P$.

\begin{equation}
\theta^* \in \operatorname{argmin}_{\theta \in \Theta} \mathbb{E}_{(x, y) \sim \P} \left[ \mathcal{L}(f_{\theta}(x), y) \right]
\end{equation}

This is done by minimizing the empirical loss for a dataset $\mathcal{D} = \left\{ (\mathbf{x}^{(i)}, y^{(i)} \right\}_{i=1}^n$ that was sampled from $\P$.
Optimization is typically performed using gradient-based methods such as stochastic gradient descent (SGD).

We do not provide an extensive treatment of the theoretical foundations of deep learning, as the topic is too broad for adequate coverage here, and most readers will already be familiar with its core concepts. For a comprehensive discussion, we refer the reader to \cite{bishop2023deep}.


\subsection{Main dependencies}

The \pkg{mlr3torch} \rlang{} package builds mainly upon the \rlang{} packages \pkg{mlr3} \citep{ref-mlr32019}, \pkg{mlr3pipelines} \citep{ref-mlr3pipelines2021} and \pkg{torch} \citep{ref-torch2025} which we will briefly review in this section.

The \pkg{mlr3} framework offer a versatile, object-oriented, and extendable framework for various ML tasks in the \proglang{R} programming language, including regression, classification and other tasks.
The ecosystem primarily relies on the \pkg{R6} class system for object orientation \citep{ref-r6chang}.
% maybe mention dictionary retrieval?
At its core, \pkg{mlr3} allows users to access, train and evaluate a wide range of machine learning algorithms implemented in the \proglang{R} language through a unified interface.

The example below demonstrates how to evaluate a simple regression tree \code{Learner} from the \pkg{rpart} package \citep{ref-rpart2025} on the predefined \code{mtcars} example \code{Task} \citep{ref-henderson1981building}, where the goal is to predict the miles per gallon (mpg) of a car based on its other features.
We train the learner on $2/3$ of the complete dataset and predict the remaining third.
The resulting predictions are then evaluated using the RMSE \code{Measure}.

\begin{CodeInput}
R> library("mlr3")
R> set.seed(42)
R> task <- tsk("mtcars")
R> learner <- lrn("regr.rpart")
R> split <- partition(task, ratio = 2/3)
R> learner$train(task, split$train)
R> pred <- learner$predict(task, split$test)
R> pred$score(msr("regr.rmse"))
\end{CodeInput}
\begin{CodeOutput}
regr.rmse
 4.736051
\end{CodeOutput}

The \pkg{mlr3pipelines} extension enables the construction of learning pipelines as computational graphs, which, among other things, facilitates the integration of preprocessing steps into the modelling workflow.
The nodes in such a \code{Graph} inherit from class \code{PipeOp}, which, just like the \code{Learner} class, have a train and predict method.
Each \code{PipeOp} has one or more typed input channels and one or more typed output channels. Input channels can be connected to output channels of preceding \code{PipeOp}s, and the types of these channels may differ between training and prediction.
We can construct such a graph by combining individual components via the graph-concatenation operator \code{\%>>\%}.
A \code{Graph} can be converted to a \code{GraphLearner}, which inherits from \code{mlr3::Learner} and can be used as such.
Below, we combine a principal component preprocessing step with the decision tree learner.

\begin{CodeInput}
R> library("mlr3pipelines")
R> graph_learner <- as_learner(po("pca") %>>% lrn("regr.rpart"))
\end{CodeInput}

To evaluate the performance of the above pipeline, we can \code{resample} it using a cross-validation \code{Resampling} scheme with 3 folds and evaluate it again using RMSE.

\begin{CodeInput}
R> resampling <- rsmp("cv", folds = 3)
R> rr <- resample(task, graph_learner, resampling)
R> rr$aggregate(msr("regr.rmse"))
\end{CodeInput}
\begin{CodeOutput}
regr.rmse
 4.274766
\end{CodeOutput}

The \pkg{mlr3} ecosystem also has many other components, including packages for hyperparameter optimization, feature selection or analysis tools.
For a comprehensive overview of the \pkg{mlr3} ecosystem, we refer readers to the freely accessible book \citep{ref-mlr3book}.

The \pkg{torch} package is used as the computational backbone for training neural networks in \pkg{mlr3torch}.
It closely resembles the \pytorch{} \python{} library \citep{ref-pytorch}.
At its heart the package offers support for GPU accelerated tensor operations, an autograd system, as well as many different tensor operations and optimizers commonly used in DL.
In the example below, we perform a simple multiplication and obtain the gradient with respect to one of its inputs.

\begin{CodeInput}
R> library("torch")
R> torch_manual_seed(42)
R> x <- torch_tensor(1, device = "cpu")
R> w <- torch_tensor(2, requires_grad = TRUE, device = "cpu")
R> y <- w * x
R> y$backward()
R> w$grad
\end{CodeInput}
\begin{CodeOutput}
torch_tensor
 1
[ CPUFloatType{1} ]
\end{CodeOutput}

\section{Building blocks}\label{sec:building-blocks}

\subsection{Data representation}

\subsubsection{Lazy tensor}

% Lazy Tensor
In the \pkg{mlr3} ecosystem, the machine learning problem and dataset is represented via the \pkg{R6} class \code{Task}.
Such a task contains the data itself, as well as metadata, which includes the task type (e.g., regression or classification), the target column, or the feature names.
In order to support not only standard tabular features, but also generic torch tensors, \pkg{mlr3torch} defines the \code{lazy\_tensor} type, which represents a vector of tensors with possibly heterogeneous shapes.
It is built upon the \code{torch::dataset} class, and therefore only needs to define how to obtain its elements, but not necessarily store them in memory, hence the name 'lazy'.
We will demonstrate the datatype using the MNIST digit classification example problem \citep{ref-mnist-2012}, a supervised learning task where the goal is to assign an image of a handwritten digit to one of the ten digit classes (0-9).

\begin{CodeInput}
R> library("mlr3torch")
R> mnist <- tsk("mnist")
R> mnist
\end{CodeInput}
\begin{CodeOutput}
<TaskClassif:mnist> (70000 x 2): MNIST Digit Classification
* Target: label
* Properties: multiclass
* Features (1):
  - lt (1): image
\end{CodeOutput}

The underlying data includes two columns, one of which is a \code{factor} vector with the class labels and the other one a \code{lazy\_tensor} containing the greyscale images with $28\times28$ pixels.

\begin{CodeInput}
R> rows <- mnist$data(1:2)
R> rows
\end{CodeInput}
\begin{CodeOutput}
  label           image
 <fctr>    <lazy_tensor>
1:    5  <tnsr[1x28x28]>
2:    0  <tnsr[1x28x28]>
\end{CodeOutput}

To convert a \code{lazy\_tensor} to a \code{torch\_tensor}, we can call \code{materialize()}.

\begin{CodeInput}
R> str(materialize(rows$image))
\end{CodeInput}
\begin{CodeOutput}
List of 2
 $ :Float [1:1, 1:28, 1:28]
 $ :Float [1:1, 1:28, 1:28]
\end{CodeOutput}

The construction of a \code{lazy\_tensor} from a \code{torch::dataset} is possible through the \code{as\_lazy\_tensor} converter, which will be shown in action in \Cref{sec:finetuning}.

\subsubsection{Data preprocessing and augmentation}

A core feature of \code{lazy\_tensor} is that it can be processed by \code{PipeOp}s.
Below, we reshape the images into dense vectors, so we can later train a simple feed forward network on the flattened images.

\begin{CodeInput}
R> po_flat <- po("trafo_reshape", shape = c(-1, 28 * 28))
R> mnist_flat <- po_flat$train(list(mnist))[[1L]]
R> mnist_flat$head(2)
\end{CodeInput}
\begin{CodeOutput}
    label         image
   <fctr> <lazy_tensor>
1:      5   <tnsr[784]>
2:      0   <tnsr[784]>
\end{CodeOutput}

The same reshaping would also be applied when calling the \code{\$predict()} method of the \code{PipeOp}.
Just like loading the data, these preprocessing steps are not applied eagerly, but lazily.
They build up an internal preprocessing graph that will be executed when the lazy tensor is materialized.

Analogously, data augmentation steps are also available as \code{PipeOp}s, but their identifiers are prefixed with \code{"augment\_"} instead of \code{"trafo\_"}.
The full list of currently available operators is listed on the package website.\footnote{\url{https://mlr3torch.mlr-org.com/articles/preprocessing\_list.html}}
Both types of \code{PipeOp}s inherit from the \code{PipeOpTaskPreprocTorch} class.
They only differ with respect to the stages during which they are active: preprocessing operations are applied during both training and prediction, whereas data augmentation is only active during training.


\subsection{LearnerTorch}

The \code{LearnerTorch} \pkg{R6} class is the abstract base class that inherits from the \code{mlr3::Learner} class and defines the train and predict logic, currently only for supervised classification and regression.
Each learner in \pkg{mlr3torch} inherits from this base class and additionally defines the network architecture, the hyperparameters, supported feature types, and other meta-information, as well as how to construct the \code{torch::dataset} from the input \code{Task} during training.
One simple example is the \code{LearnerTorchMLP}, which represents a simple multi layer perceptron with dropout.

\subsubsection{Configuration}

A torch learner is configurable through construction arguments, hyperparameters, and fields.
The construction arguments are

\begin{itemize}
    \item \code{loss} :: \code{TorchLoss}, which wraps a \code{torch::nn\_loss} and represents a loss function for training,
    \item \code{optimizer} :: \code{TorchOptimizer}, which wraps a \code{torch::torch\_optimizer\_generator} and defines how the network parameters are optimized, and
    \item \code{callbacks} :: \code{list()} of \code{TorchCallback}s, which customize the training process.
\end{itemize}

All three object types are configurable via their own hyperparameters, such as the learning rate of an optimizer.
While \pkg{mlr3torch} comes with predefined and ready-to-use losses, optimizers, and callbacks, adding custom versions is also supported and is shown in \Cref{sec:extending}.

Below, we construct a \code{LearnerTorchMLP} classifier that uses cross-entropy as the loss function, AdamW as the optimizer \citep{ref-loshchilov2017decoupled} and stores the training history:

\begin{CodeInput}
R> mlp <- lrn("classif.mlp",
+   loss = t_loss("cross_entropy"),
+   optimizer = t_opt("adamw", lr = 0.01),
+   callbacks = t_clbk("history"))
\end{CodeInput}

The hyperparameters of the learner are represented as a \code{ParamSet} as defined in the \pkg{paradox} package, which provides a comprehensive framework for defining parameter spaces \citep{ref-paradox2024}.
It supports a variety of parameter types, hierarchical dependencies, and constraint handling, making it a powerful tool for hyperparameter optimization and configuration management.

The hyperparameter space of the constructed learner contains the parameter union from the specified loss, optimizer and callbacks, some parameters that are shared among all \code{LearnerTorch} classes, as well as learner-specific parameters that, e.g., parameterize the network architecture.
To avoid name clashes, the loss, optimizer and callback parameters are prefixed by \code{"loss."}, \code{"opt."}  and \code{"cb."} respectively.

For the MLP learner, the learner-specific hyperparameters include the latent dimensions (\code{neurons}), the activation function (\code{activation}), and the dropout probability (\code{p}).
The parameter values can be set via the \code{\$set\_values()} method of the \code{ParamSet}.
Additionally, we configure several other hyperparameters, including the device, the number of epochs, and the batch size.
We also specify the weight decay for the optimizer and monitor the log loss throughout the training process.

\begin{CodeInput}
R> mlp$param_set$set_values(
+    neurons = c(100, 200), activation = torch::nn_relu,
+    p = 0.3, opt.weight_decay = 0.01, measures_train = msr("classif.logloss"),
+    epochs = 5, batch_size = 32, device = "cpu")
\end{CodeInput}

Furthermore, \torch{} learners can be configured through fields, which distinguish themselves from hyperparameters as they have predefined semantics that are shared between all \code{mlr3::Learner}s.
These include the predict type (\code{\$predict\_type}), how to construct the validation data (\code{\$validate}), whether to conduct the model training in an external process (\code{\$encapsulation}), and which learner to use when the primary learner fails (\code{\$fallback}).
Below, we configure the learner to make probability predictions via the \code{\$configure()} setter, which can set fields and hyperparameters simultaneously.

\begin{CodeInput}
R> mlp$configure(
+    predict_type = "prob",
+    epochs = 10)
\end{CodeInput}

\subsubsection{Training and prediction}

The primary interface for using a \code{LearnerTorch} is through its \code{\$train()} and \code{\$predict()} methods.
The former takes as input a \code{Task} and stores the trained model in the \code{\$model} field.
The \code{\$predict()} method also takes in a \code{Task}, but outputs a \code{mlr3::Prediction} object.
Below, we train the MLP learner on the first 60,000 rows of the flattened MNIST task from above:

\begin{CodeInput}
R> mlp$train(mnist_flat, row_ids = 1:60000)
\end{CodeInput}

After training, we can access the training results via the \code{\$model} slot of the learner.
Most importantly, the model contains the trained network, a \code{nn\_module}, the optimizer state, as well as the states of the callbacks.

\begin{CodeInput}
R> mlp$model$network
\end{CodeInput}
\begin{CodeOutput}
An `nn_module` containing 100,710 parameters.

  Modules
â¢ 0: <nn_linear> #78,500 parameters
â¢ 1: <nn_relu> #0 parameters
â¢ 2: <nn_dropout> #0 parameters
â¢ 3: <nn_linear> #20,200 parameters
â¢ 4: <nn_relu> #0 parameters
â¢ 5: <nn_dropout> #0 parameters
â¢ 6: <nn_linear> #2,010 parameters
\end{CodeOutput}
\begin{CodeInput}
R> head(mlp$model$callbacks$history, n = 2)
\end{CodeInput}
\begin{CodeOutput}
   epoch train.classif.logloss
   <num>                 <num>
1:     1              2.299611
2:     2              2.307922>
\end{CodeOutput}

Once a learner is trained, it can be used to make predictions on unseen data, which we here evaluate using the classification error \code{Measure}.

\begin{CodeInput}
R> pred <- mlp$predict(mnist_flat, row_ids = 60001:70000)
R> pred$score(msr("classif.ce"))
\end{CodeInput}

\begin{CodeOutput}
classif.ce
     0.095
\end{CodeOutput}

\subsubsection{Learner characteristics}\label{sec:learner-characteristics}

Like any other \code{Learner}, learners in \pkg{mlr3torch} are annotated with metadata that characterizes their capabilities.
This allows for correctness and compatibility checks that prevent user errors and give informative error messages.
Available options for those characteristics are defined in the \pkg{mlr3} base package but are extendable via a reflection mechanism.
These learner characteristics include the task type, predict types, feature types and properties.

\begin{itemize}
    \item The \code{task\_type} defines on which task the learner operates. Currently, \pkg{mlr3torch} only supports regression and classification.
    \item The \code{predict\_types} indicate the type of predictions the learner can make. \torch{} regression learners can currently only make response predictions, and classifiers can make both response and probability predictions.
    \item The \code{feature\_types} specify which features the learner can handle. Here, \mlrttorch{} extends the available types with the \code{lazy\_tensor}.
    \item The \code{properties} are special requirements or abilities of the learner. An example is the ability to perform feature selection or to handle observation weights. We will cover the selected properties \code{"marshal"}, \code{"validation"} and \code{"internal_tuning"} below.
\end{itemize}


\paragraph{Marshalling} is the process of converting an in-memory object so that it can be (de)-serialized without loss of information.
Learners that have this property require such processing before and after (de-)serialization.
This is necessary for \torch{} learners as their weights are \code{torch\_tensor}s, which are external pointers that are not natively handled by \rlang{}'s serialization mechanism.
To save an \mlrttorch{} learner, one therefore needs to call \code{\$marshal()} before saving it, and \code{\$unmarshal()} after reading it.

\begin{CodeInput}
R> pth <- tempfile()
R> mlp$marshal()
R> saveRDS(mlp, pth)
R> mlp2 <- readRDS(pth)
R> mlp2$unmarshal()
\end{CodeInput}

\paragraph{Validation}

When training iterative learning procedures such as deep neural networks, one often wants to monitor the generalization performance of the model during training.
In \mlrt{}, this is possible through a standardized mechanism available for learners with the \code{"validation"} property.
Such learners have a \code{\$validate} field to configure how to create the validation data.
One possibility is to set this to a ratio, e.g. below, we will only use $70\%$ of the data for training.

\begin{CodeInput}
R> set_validate(mlp, validate = 0.3)
\end{CodeInput}

This setting is also taken into account by preprocessing \code{PipeOp}s, which ensures that the prediction logic of a preprocessing \code{PipeOp} is applied to the validation data during training.

\paragraph{Internal Tuning}

While \mlrt{} has extensive support for standard offline hyperparameter tuning through its extensions such as \pkg{mlr3tuning} \citep{ref-mlr3tuning2024}, some learning algorithms can also internally optimize hyperparameters during training.
For deep neural networks, this is possible for the \code{epochs} hyperparameter and done by early stopping based on the validation performance.
While such internal tuning can be considered as just another hyperparameter of a learner, the standardization of this property in the \mlrt{} base package allows to conveniently combine such learner-internal hyperparameter tuning with standard offline tuning.
In \Cref{sec:tuning} we will demonstrate how to do so in the context of neural architecture search.

For an in-depth coverage of the validation and internal tuning mechanism, we refer the reader to the respective chapter from the \pkg{mlr3} book \citep{ref-mlr3book-valid}.

\subsection{Neural networks as graphs}

In \pkg{torch}, neural networks are represented by the \code{nn\_module} class.
To implement a custom module, a constructor method must be defined that initializes the network weights and sub-modules, as well as a forward method that defines how the network transforms its inputs.
This allows for great flexibility, but there is little structure to the underlying architecture.
Below, we create a simple neural network using one latent layer and a ReLU activation.

\begin{CodeInput}
R> nn_simple <- nn_module("nn_simple",
R>   initialize = function(d_in, d_latent, d_out) {
R>     self$linear1 = nn_linear(d_in, d_latent)
R>     self$activation = nn_relu()
R>     self$linear2 = nn_linear(d_latent, d_out)
R>   },
R>   forward = function(x) {
R>     x = self$linear1(x)
R>     x = self$activation(x)
R>     self$linear2(x)
R>   }
R> )
\end{CodeInput}

The resulting \code{nn\_simple} is a module generator, and we can create a specific module instance by calling it and providing the required constructor arguments.

\begin{CodeInput}
R> net <- nn_simple(10, 100, 1)
\end{CodeInput}

To use this neural network, we can simply call it on a tensor, which will apply the \code{\$forward} method of the module.

\begin{CodeInput}
R> net(torch_randn(1, 10))
\end{CodeInput}

\begin{CodeOutput}

\end{CodeOutput}

\subsubsection{Module graph}


While \pkg{mlr3torch} also allows the construction of \code{Learner}s from generic \code{nn\_module}s (c.f.~\Cref{sec:extending-learner}), it also offers a systematic representation via directed acyclic graphs, which, e.g., allows to easier inspect or change individual components.
Such an architecture can be built by assembling \code{PipeOpModule}s in an \code{mlr3pipelines::Graph}.
Each \code{PipeOpModule} wraps an instantiated \code{nn\_module} and the edges of the graph define the data-flow between the layers.

It is important to realize that this uses \code{Graph} objects in a slightly unusual way:
\pkg{mlr3pipelines} \code{Graph}s are usually used to define preprocessing or stacking pipelines that are executed exactly once per model training run, on the entire dataset.
The \code{Graph}s built from \code{PipeOpModule}s, on the other hand, encode only a single neural network forward pass and are often executed many times per training run on individual batches of samples.

Below, we define a simple feed forward network with an input dimension of $10$, a latent dimension of $100$, a ReLU activation, and an output layer that returns a scalar value.
Note that \code{po("module\_1", ...)} is shorthand for \code{po("module", id = "module\_1", ...)} and can represent an arbitrary torch module that is passed via the argument \code{module}.

\begin{CodeInput}
R> module_graph <- po("module_1", module = nn_linear(10, 100)) %>>%
+   po("module_2", module = nn_relu()) %>>%
+   po("module_3", module = nn_linear(100, 1))
\end{CodeInput}

To execute a forward graph for this architecture, we can call its \code{\$train()} method, which in this case expects one input tensor and returns one output tensor.
The \code{\$predict()} phase of the graph currently does nothing.

To work with this graph via a more familar interface, we can convert it into a \code{nn\_graph} which inherits from \code{nn\_module}.
When doing so, we only need to specify the input shapes, where \code{NA} indicates that the first (batch) dimension is of unknown shape.
Below, we evaluate the resulting module on 2 randomly generated observations.

\begin{CodeInput}
R> net <- nn_graph(module_graph, shapes_in = list(module_1.input = c(NA, 10)))
R> net(torch_randn(2, 10))
\end{CodeInput}
\begin{CodeOutput}
torch_tensor
-0.2020
 0.1729
[ CPUFloatType{2,1} ][ grad_fn = <AddmmBackward0> ]
\end{CodeOutput}

\subsubsection{Generating graph}

While the construction of module graphs by hand is possible, it requires the specification of auxiliary parameters that can be inferred, such as the input dimension of a linear layer.
Therefore, \pkg{mlr3torch} makes it convenient to construct such a module graph via another generating graph.
This graph consists primarily of \code{PipeOpTorch} objects but can also contain other components.
The generating graph for the module graph from above is specified below.
Note that we do not need to specify the input dimension for the first layer or the output dimension of the final layer.

\begin{CodeInput}
R> graph <- po("torch_ingress_ltnsr") %>>%
+    nn("linear", out_features = 10) %>>% nn("relu") %>>% nn("head")
\end{CodeInput}

While the module graph operates on tensors, which usually represent individual batches during a training run, the input type of the generating graph is a \code{Task} during both the training and prediction stage.
The output type during \code{\$train()} is a list containing a \code{ModelDescriptor} that is initialized by the \code{"torch_ingress_ltnsr"} ingress operator.
It is the intercommunication object between the different \code{PipeOpTorch} operators.
The model descriptor contains among other things, the neural network (a graph consisting of \code{PipeOpModule}s, just like the one we built manually earlier), the \code{Task}, specifications for data loading, as well as other meta-information.
When the graph is trained, the network layers (\code{nn("<key>")}) generate a \code{PipeOpModule}, attaches it to the graph and update the meta-information stored inside the \code{ModelDescriptor}.
Auxiliary parameters such as the input dimension of a linear layer are inferred from the meta-information, which includes the current output shapes of the network.
Note that the weights of the neural network that is being built up are not yet fit to the data at this point.

Different ingress operators exist for different feature types, including for numeric features, categorical features and lazy tensors.
Such an ingress operator is an entry point into the neural network, but also specifies how the input tensor to the respective network entry point is generated from the \code{Task} during training.
For example, \code{po("torch\_ingress\_categ")} will load the categorical features from the task and convert them into a label-encoded integer-valued tensor.
It is also possible to have more than one entry point into the neural network, which we will see later.
Below, we train the graph on the flattened MNIST task from earlier.

\begin{CodeInput}
R> md <- graph$train(mnist_flat)[[1L]]
R> md
\end{CodeInput}
\begin{CodeOutput}
<ModelDescriptor: 4 ops>
* Ingress:  torch_ingress_ltnsr.input: [(NA,784)]
* Task:  mnist [classif]
* Callbacks:  N/A
* Optimizer:  N/A
* Loss:  N/A
* pointer:  nn_head.output [(NA,10)]
\end{CodeOutput}

We can extend this graph to not only build up the model descriptor but to also actually fit the weights of the network to the given input task.
This requires configuring at least the optimizer and loss function in the \code{ModelDescriptor}, but we could also set some callbacks using \code{po("torch\_callbacks")}.

\begin{CodeInput}
R> graph <- graph %>>%
+    po("torch_loss", t_loss("cross_entropy")) %>>%
+    po("torch_optimizer", t_opt("adamw", lr = 0.001))
\end{CodeInput}

By adding the \code{PipeOpTorchModel} operator to the graph, we specify that the neural network, as stored in the \code{ModelDescriptor}, should be trained in a training loop.
It takes in the \code{ModelDescriptor} as an input, but instead of outputting a modified version of it, it converts it into a \code{LearnerTorchModel} and trains it on the input task.
Through its hyperparameters, it also allows the specification of training parameters such as the batch size or the number of epochs.

\begin{CodeInput}
R> graph <- graph %>>% po("torch_model_classif", epochs = 10, batch_size = 16)
\end{CodeInput}

During the prediction phase of the above graph, all \code{PipeOpTorch} operators simply forward their input task without modification.
Only the \code{PipeOpTorchModel} will call the \code{\$predict()} method of the \code{LearnerTorchModel} that was stored in its state during the training phase.
The whole process is visualized in \Cref{fig:pipeop-torch}.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{figures/pipeop-torch.pdf}
    \caption{Training and prediction phase for a neural network learner represented as an \code{mlr3pipelines::Graph}.}
    \label{fig:pipeop-torch}
\end{figure}

Because this graph can be converted to an \code{mlr3::Learner}, it is interoperable with all other components from the \pkg{mlr3} ecosystem.

\begin{CodeInput}
R> glrn <- as_learner(graph)
R> glrn$train(mnist_flat)
\end{CodeInput}

The \code{ParamSet} of the resulting learner contains the hyperparameters of all individual \code{PipeOp}s from which it is built up.
This means they can be altered after construction, but most importantly, they can be tuned over by utilizing the \pkg{mlr3tuning} extension, see \Cref{sec:tuning}.

\subsubsection{Non-linear graphs}\label{sec:complex-architectures}

In the previous example, we were building a simple linear network with one input, where each \code{PipeOpTorch} added one layer (i.e., a \code{PipeOpModule}) to the underlying module graph.
In this section we will show how to go beyond these restrictions.

To do so, we will define a residual neural network architecture.
We start by defining two independent graphs for the linear and non-linear path respectively.

\begin{CodeInput}
R> path_lin <- nn("linear_1")
R> path_nonlin <- nn("linear_2") %>>% nn("relu")
\end{CodeInput}

The two graphs can be combined by a merge operator that takes a variable number of inputs and combines them via an aggregator, in this case summation.
This residual layer is visualized in~\Cref{fig:residual-layer}.

\begin{CodeInput}
R> residual_layer <- list(path_lin, path_nonlin) %>>% nn("merge_sum")
\end{CodeInput}

\subsubsection{Multi-input architectures}

Networks with more than one input (ingress) are also supported.
This can be used to define multi-modal architectures such as neural networks operating on both tables and images (c.f.~\Cref{sec:multimodal}).
Here, we demonstrate how to build a tabular neural network that accepts both categorical and numeric features.

We start by implementing two independent entry points into the network.
In the numeric path, we start by selecting only the numeric features of the task.
Note that this is a standard \code{PipeOp} from \code{mlr3pipelines} and it simply outputs a modified task.
Subsequently, the \code{po("torch\_ingress\_num")} initializes the \code{ModelDescriptor} that represents the numeric entry point to the network.
The \code{"tokenizer_num"} \code{PipeOpTorch} \citep{gorishniy2021revisiting} then adds a first network layer, which will embed the numeric features in a 10-dimensional vector.
The graph for the categorical features is constructed analogously.

\begin{CodeInput}
R> path_num <- po("select_1", selector = selector_type("numeric")) %>>%
+    po("torch_ingress_num") %>>%
+    nn("tokenizer_num", d_token = 10)
R> path_categ <- po("select_2", selector = selector_type("factor")) %>>%
+    po("torch_ingress_categ") %>>%
+    nn("tokenizer_categ", d_token = 10)
\end{CodeInput}

To combine the two independent branches into a single network, we can merge them again, this time by concatenating the outputs of the input tensors, i.e., the embeddings of the categorical and numeric features along the second dimension.
The resulting network has two inputs, where the first one expects a floating point tensor and the second one an integer valued tensor representing the categorical features.
The graph is visualized in~\Cref{fig:multi-inputs}.

\begin{CodeInput}
R> graph <- list(path_num, path_categ) %>>% nn("merge_cat", dim = 2)
\end{CodeInput}

\begin{figure}[h]
    \centering
    \begin{subfigure}{0.38\textwidth}
        \centering
        \includegraphics[width=\textwidth, height = 56pt]{figures/residual-layer.pdf}
        \caption{Residual layer.}
        \label{fig:residual-layer}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.58\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/multi-input.pdf}
        \caption{Neural network graph with multiple inputs.}
        \label{fig:multi-inputs}
    \end{subfigure}
    \caption{More complex network segments.}
    \label{fig:side-by-side}
\end{figure}

Another common characteristic of neural network architectures is that a specific segment is repeated a given number of times.
This can be achieved by the \code{PipeOpTorchBlock} operator.
This \code{PipeOp} takes in another graph primarily consisting of \code{PipeOpTorch} objects.
During training, the \code{PipeOp} will attach the same network segment repeatedly to the neural network.

\begin{CodeInput}
R> blocks <- nn("block", residual_layer, n_blocks = 5)
\end{CodeInput}

Because \code{PipeOpTorchBlock} exposes both the number of repetitions (\code{n\_blocks}) as well as the hyperparameters of the wrapped graph, both can be tuned over.

\section{Extending the package}\label{sec:extending}

While the configuration options shown in the previous section already cover a lot of ground, it is also essential to be able to customize the training further.
For this reason, \pkg{mlr3torch} allows users to customize losses, optimizers, callbacks, as well as full learners.

\subsection{Loss and optimizer}\label{sec:extending-loss-opt}

The loss and optimizer in \mlrttorch{} are represented via the \code{TorchLoss} and \code{TorchOptimizer} classes.
They wrap a loss generator (such as \code{torch::nn\_mse\_loss}) and an optimizer generator (e.g., \code{torch::optim\_sgd}) respectively and annotate them with meta-information, most importantly, a \code{ParamSet}.
Constructing a custom loss or optimizer therefore requires underlying class generators that are constructible via \code{torch::nn\_module} and \code{torch::optimizer}.
The resulting constructors can be converted to their \mlrttorch{} representation via \code{as\_torch\_loss} and \code{as\_torch\_optimizer}.
Without specifying additional arguments, all the meta-information will be inferred as well as possible.
It is also possible to provide more specific details such as the types of the hyperparameters and their valid ranges.
Below, we create a custom \pkg{torch} loss function -- the winsorized MSE -- and convert it to its \mlrttorch{} counterpart:

\begin{CodeInput}
R> nn_winsorized_mse <- nn_module(c("nn_winsorized_mse", "nn_loss"),
R>   initialize = function(max_loss) {
R>     self$max_loss <- max_loss
R>   },
R>   forward = function(input, target) {
R>     torch_clamp(nnf_mse_loss(input, target), max = self$max_loss)
R>   }
R> )
R> tloss <- as_torch_loss(nn_winsorized_mse)
R> tloss
\end{CodeInput}
\begin{CodeOutput}
<TorchLoss:nn_l1_loss> nn_l1_loss
* Generator: nn_l1_loss
* Parameters: list()
* Packages: torch,mlr3torch
* Task Types: classif,regr
\end{CodeOutput}

This object could now be passed as construction argument \code{loss} when initializing a new \code{LearnerTorch}.

\subsection{Callbacks}\label{sec:extending-callbacks}

Callbacks are a way to modify the training loop of a deep neural network without writing everything from scratch.
They are implemented via the \pkg{R6} class \code{TorchCallback} that wraps the \code{CallbackSet} class.
Analogously to the optimizer and loss, conversion of the latter to the former is possible via \code{as\_torch\_callback()}.
The design of the callback mechanism is similar to how it is implemented in frameworks such as \keras{} or \luz{} \citep{ref-chollet2018keras, ref-luz2023}.

A \code{CallbackSet} is a collection of callback functions, known as hooks, which are invoked at specific points the training process to extend or customize its behaviour.
Hooks can be triggered before or after certain actions (such as after processing a batch).
Information from the training loop is communicated to the callback via a \code{ContextTorch} object that, e.g., contains the network and optimizer or the current training or validation batch.
A callback can also store information in the learner after training by implementing the \code{\$state\_dict} and \code{\$load\_state\_dict} methods.

A \code{TorchCallback} object is created via the \code{torch\_callback} function.
We demonstrate it below by implementing gradient clipping, which has two hyperparameters representing the maximally allowed norm value and the type of norm.
It only implements the hook that is run after the backward call.
After training, it keeps the history of norms in the learner state.

\begin{CodeInput}
R> gradient_clipper <- torch_callback("gradient_clipper",
+    initialize = function(max_norm, norm_type) {
+      self$norms <- numeric()
+      self$max_norm <- max_norm
+      self$norm_type <- norm_type
+    },
+    on_after_backward = function() {
+      norm <- nn_utils_clip_grad_norm_(self$ctx$network$parameters,
+        self$max_norm, self$norm_type)
+      self$norms <- c(self$norms, norm$item())
+    },
+    state_dict = function() {
+      self$norms
+    },
+    load_state_dict = function(state_dict) {
+      self$norms = state_dict
+    }
+  )
\end{CodeInput}

This object could now be passed via the \code{callbacks} construction argument to a \code{Learner} and would ensure that the norms of the gradients used for the weight updates do not exceed the specified value \code{max\_norm}.


\subsection{Learners and task types}\label{sec:extending-learner-task}

When the construction of custom architectures as shown in \Cref{sec:complex-architectures} is not enough, it also possible to implement custom learners by inheriting from \code{LearnerTorch}.
While it is in principle possible to overwrite the whole training and prediction logic (private methods \code{\$.train()} and \code{\$.predict()}), it is usually sufficient to overwrite one or more of the following methods:
\begin{itemize}
    \item \code{\$initialize(id, loss, optimizer, callbacks, ...)}, which needs to define learner-specific hyperparameters, as well as other metadata such as the supported feature types and task type.
    \item \code{\$.network(task, param\_vals)}, which is called at the beginning of the training loop to construct an instantiated \code{nn\_module} for the specified hyperparameters and tailored to the given input task.
    \item \code{\$.dataset(task, param\_vals)}, which must create a \code{torch::dataset} for the given input task and parameter values. The dataset must return a named list, where \code{x} is a list of tensors, \code{y} is the target tensor, and \code{.index} contains the indices.
\end{itemize}

The process of constructing such a class is described in the help page of \code{LearnerTorch}.

\subsubsection{Custom learners for classification and regression}\label{sec:extending-learner}

In many cases, it is possible to construct a custom learner from a module constructor without defining a new class.
This is possible via \code{LearnerTorchModule} class, which has two construction arguments:
\begin{itemize}
    \item \code{module\_generator}, which must be a \code{torch::nn\_module\_generator} that takes as construction argument a \code{task} and possibly other parameters.
    \item \code{ingress\_tokens}, which is a named list of \code{TorchIngressToken}s that define how the data is loaded from the input task. The names must correspond to the arguments of the \code{module\_generator}'s \code{\$forward} method.
\end{itemize}

We demonstrate this by defining a simple feed forward network with two architecture parameters, the latent dimension and the number of layers.
The \code{task} argument is used to infer hyperparameters for the first layer (number of features) and the output layer (number of classes).

\begin{CodeInput}
R> nn_ffn <- nn_module("nn_ffn",
+    initialize = function(task, latent_dim, n_layers) {
+      dims <- c(task$n_features, rep(latent_dim, n_layers),
+        length(task$class_names))
+      modules <- unlist(lapply(seq_len(length(dims) - 1), function(i) {
+        if (i < length(dims) - 1) {
+          list(nn_linear(dims[i], dims[i + 1]), nn_relu())
+        } else {
+          list(nn_linear(dims[i], dims[i + 1]))
+        }
+      }), recursive = FALSE)
+      self$network <- do.call(nn_sequential, modules)
+    },
+    forward = function(x) self$network(x)
+  )
\end{CodeInput}

Next, we need to define how the training task is converted to \code{torch\_tensor}s during training.
In our case, our model will load all numeric columns (integers and doubles) as a single tensor into the network as argument \code{x}.
The names of the tokens specify as which argument they will be passed to the \code{$forward} method of the module.
In our case, we have a single element \code{x} that will be passed as argument \code{x} to the \code{nn\_ffn} from above.

\begin{CodeInput}
R> num_input <- list(x = ingress_num())
R> num_input
\end{CodeInput}

\begin{CodeOutput}
$x
Ingress: Task[selector_type(c("numeric", "integer"))] --> Tensor()
\end{CodeOutput}

Next, we construct the learner, which also includes specification of the architecture's hyperparameters, which are part of the learner's \code{ParamSet}.

\begin{CodeInput}
R> lrn_ffn <- lrn("classif.module",
+    module_generator = nn_ffn,
+    ingress_tokens = num_input,
+    latent_dim = 100, n_layers = 5)
\end{CodeInput}

We could now train this learner on any \mlrt{} classification task, but the network would only make use of the numeric features.

\subsubsection{Other task types}

Extending the package to other supervised learning problems such as survival analysis is straightforward and could be achieved by implementing the required loss functions as shown in~\Cref{sec:extending-loss-opt} and implementing custom dataset constructors, network architectures and prediction encoders as is documented in the help page of \code{LearnerTorch}.
Because the focus of \pkg{mlr3torch} is currently  supervised learning, extending the package to non-supervised task would require more effort, including the definition of a custom \code{Task} class, implementation of the respective \code{Measure}s, as well as a new \code{LearnerTorchUnsupervised} base class.

\section{Use cases}\label{sec:use-cases}

We will now demonstrate some of \pkg{mlr3torch}'s features by tuning a \torch{} learner on a tabular regression problem (\Cref{sec:tuning}), finetuning a pretrained image network from \pkg{torchvision} (\Cref{sec:finetuning}) and by constructing a network architecture for a multi-modal task that contains both tabular features and images (\Cref{sec:multimodal}).

\subsection{Simple neural architecture search}\label{sec:tuning}

In this section, we will construct a tabular neural network learner and simultaneously tune its architecture, optimizer and number of epochs.
This will be done using the predefined \code{"california\_housing"} example task \citep{ref-pace1997sparse}, where the goal is to predict the median house value in California.

\begin{CodeInput}
R> task <- tsk("california_housing")
R> task
\end{CodeInput}
\begin{CodeOutput}
<TaskRegr:california_housing> (20640 x 10): California House Value
* Target: median_house_value
* Properties: -
* Features (9):
  - dbl (8): households, housing_median_age, latitude, longitude,
    median_income, population, total_bedrooms, total_rooms
  - fct (1): ocean_proximity
\end{CodeOutput}

\subsubsection{Defining the learning pipeline}
The input of our network architecture will be a single numeric tensor, so the first step in the learning pipeline is to encode the categorical features as numeric columns using the one-hot method of \code{PipeOpEncode}.
We also replace missing values by sampling from the empirical histogram.

\begin{CodeInput}
R> preprocessing <- po("encode", method = "one-hot") %>>%
+    po("imputehist")
\end{CodeInput}

The numeric entry-point is initialized by \code{PipeOpTorchIngressNum}.

\begin{CodeInput}
R> ingress <- po("torch_ingress_num")
\end{CodeInput}

The subsequent \code{PipeOp}s define the layers of the neural network.
In this example, we are using an architecture that repeats the same block multiple times.
One such block will consist of a linear operation, an activation function and a dropout layer.
We can select between different activation functions using the \code{ppl("branch")} construct.
This allows us to select between different branches in the graph by setting the \code{branch.selection} parameter.
In the example below, we allow for two choices, namely a ReLU and a sigmoid activation function.

\begin{CodeInput}
R> block <- nn("linear", out_features = 32) %>>%
+    ppl("branch", list(relu = nn("relu"), sigmoid = nn("sigmoid"))) %>>%
+    nn("dropout")
\end{CodeInput}

This also demonstrates one of the strengths of \pkg{mlr3torch}: because \code{PipeOpTorch} operators are just \code{PipeOp}s, they can be combined with other operators from \pkg{mlr3pipelines} as long as their input and output types are compatible.

To repeat the above defined segment one or more times, we can use \code{PipeOpTorchBlock} which we have already covered earlier.

\begin{CodeInput}
R> architecture <- nn("block", block) %>>% nn("head")
\end{CodeInput}

Next, we set the loss function to MSE and the optimizer to AdamW:

\begin{CodeInput}
R> config <- po("torch_loss", loss = t_loss("mse")) %>>%
+    po("torch_optimizer", optimizer = t_opt("adamw"))
\end{CodeInput}

The final step in the graph is the \code{PipeOpTorchModelRegr} that takes in a \code{ModelDescriptor} and does the actual training:

\begin{CodeInput}
R> model <- po("torch_model_regr", device = "cuda", batch_size = 512)
\end{CodeInput}

We can now assemble these pieces into the complete learning pipeline and convert it to a \code{Learner}:

\begin{CodeInput}
R> pipeline <- preprocessing %>>% ingress %>>%
+    architecture %>>% config %>>% model
R> learner = as_learner(pipeline)
R> learner$id = "custom_nn"
\end{CodeInput}

The resulting \code{GraphLearner} exposes all the hyperparameters of the individual \code{PipeOp}s that it was built from and over which we will now tune.

\subsubsection{Defining the search space}
In order to tune this learning pipeline, we define the search space for the architecture and the optimizer.
We will tune the latent dimension between $20$ and $500$, repeat the layer between $1$ to $5$ times with either the ReLU or sigmoid activation and use a dropout ratio in $[0.1, 0.9]$.
For the optimizer, we search for the learning rate within $[10^{-4}, 10^{-1}]$ on a logarithmic scale.

\begin{CodeInput}
R> library("mlr3tuning")
R> learner$param_set$set_values(
+    block.linear.out_features = to_tune(20, 500),
+    block.n_blocks = to_tune(1, 5),
+    block.branch.selection = to_tune(c("relu", "sigmoid")),
+    block.dropout.p = to_tune(0.1, 0.9),
+    torch_optimizer.lr = to_tune(10^-4, 10^-1, logscale = TRUE))
\end{CodeInput}

All the above parameters will be tuned in the standard offline way using Bayesian optimization.
In contrast, we will tune the \code{epochs} hyperparameter using the learner-internal early stopping procedure that was covered in \Cref{sec:learner-characteristics}.
By setting the \code{\$validate} field to \code{"test"}, we will early stop on the test set of the resampling used for the offline hyperparameter tuning, i.e., the validation data of the tuning process.

\begin{CodeInput}
R> set_validate(learner, "test")
\end{CodeInput}

Finally, we set the patience parameter for the early stopping, the maximum number of epochs to train for, and the validation measure.
By configuring \code{internal = TRUE}, we specify that the epochs are tuned using early stopping.

\begin{CodeInput}
R> learner$param_set$set_values(
+    torch_model_regr.patience = 5,
+    torch_model_regr.measures_valid = msr("regr.mse"),
+    torch_model_regr.epochs = to_tune(upper = 100, internal = TRUE))
\end{CodeInput}

\subsubsection{Running the tuning procedure}

We employ Bayesian optimization via the \code{mlr3mbo} package to explore the hyperparameter space efficiently \citep{ref-mlr3mbo}.
The tuning process is configured to perform holdout resampling and use the internal validation score, i.e., the value of the \code{measures\_valid} hyperparameter from above, also as the tuning measure.
We run the Bayesian optimization loop for $40$ iterations.

\begin{CodeInput}
R> library("mlr3mbo")
R> ti <- tune(
+    tuner = tnr("mbo"),
+    resampling = rsmp("holdout"),
+    measure = msr("internal_valid_score", minimize = TRUE),
+    learner = learner,
+    term_evals = 40,
+    task = task)
+  pvals <- ti$result_learner_param_vals[2:7]
R> cat("*", paste(names(pvals), "=", pvals,
+   collapse = "\n"), "\n")
\end{CodeInput}
\begin{CodeOutput}
* block.n_blocks = 4
* block.linear.out_features = 335
* block.branch.selection = relu
* block.dropout.p = 0.449124
* torch_optimizer.lr = 0.001817172
* torch_model_regr.epochs = 80
\end{CodeOutput}

We could now re-fit our learner on the whole data with the configuration found above to obtain our final model.

\subsection{Fine-tuning an image network}\label{sec:finetuning}

% What is finetuning
Instead of training a neural network from scratch, fine-tuning is a common approach in DL that leverages the pretrained weights of large neural networks on new, task-specific data.
This technique allows models to adapt quickly to new domains by transferring the learned representations from large, generic datasets to more focused applications.
Fine-tuning not only reduces training time but also improves performance, especially in scenarios where the available labelled data for the downstream task is limited.

% Summary of what we do in this section:
In this section, we demonstrate how the \pkg{mlr3torch} framework can be used to fine-tune a pretrained image classification network from the \pkg{torchvision} package on a downstream task.
We will fine-tune the ResNet-18 model \citep{ref-he2015deepresiduallearningimage} on the dogs vs. cats dataset \citep{ref-dogs-vs-cats2013}, where the goal is to classify images into these two categories.

\subsubsection{Task definition}

As a first step, we will show how to construct the classification task, the data for which we will download using with the help of the \pkg{torchdatasets} \rlang{} package \citep{torchdatasets}.

\begin{CodeInput}
R> library("torchdatasets")
R> dogs_vs_cats_dataset("data", download = TRUE)
\end{CodeInput}

Next, we will create a \code{lazy\_tensor} vector that represents these images.
To do so, we create a custom \code{torch::dataset} class that stores the image paths and loads them from disk.
\pkg{mlr3torch} requires that the \code{\$.getitem()} method returns a named list of \code{torch\_tensor}s.
Note that we permute the first and third dimension of the tensor as \pkg{torchvision} models expect the RGB channels to be the first dimension.

\begin{CodeInput}
R> ds <- torch::dataset("dogs_vs_cats",
+    initialize = function(pths) {
+      self$pths <- pths
+    },
+    .getitem = function(i) {
+      image <- torchvision::base_loader(self$pths[i])
+      list(image = torch_tensor(image)$permute(c(3, 1, 2)))
+    },
+    .length = function() {
+      length(self$pths)
+    }
+  )
\end{CodeInput}

In order to initialize this dataset, we need to create the vector containing the image paths.

\begin{CodeInput}
R> paths <- list.files(file.path("data", "dogs-vs-cats/train"),
+    full.names = TRUE)
R> dogs_vs_cats <- ds(paths)
\end{CodeInput}

To convert this into a \code{lazy\_tensor}, we can use the \code{as\_lazy\_tensor} converter.
Here we specify the output shapes of the images to be \code{NULL}, which denotes that the items have varying shapes.

\begin{CodeInput}
R> lt <- as_lazy_tensor(dogs_vs_cats, list(image = NULL))
\end{CodeInput}

Next, we construct the vector containing the true labels.
The dataset has a total of $25000$ observations, with half of them being dogs and the other half being cats.
For this dataset, the information is encoded in the names of the files.

\begin{CodeInput}
R> labels <- ifelse(grepl("dog\\.\\d+\\.jpg", paths), "dog", "cat")
R> table(labels)
\end{CodeInput}
\begin{CodeOutput}
labels
  cat   dog
12500 12500
\end{CodeOutput}

Next, we construct a \code{data.table} \citep{ref-datatable2024} containing both the images and labels and then convert it to an \code{mlr3::Task} by specifying the target variable and its identifier.

\begin{CodeInput}
R> tbl <- data.table(image = lt, class = labels)
R> task <- as_task_classif(tbl, target = "class", id = "dogs_vs_cats")
R> task
\end{CodeInput}
\begin{CodeOutput}
<TaskClassif:dogs_vs_cats> (25000 x 2)
* Target: class
* Properties: twoclass
* Features (1):
  - lt (1): image
\end{CodeOutput}

\subsubsection{Learning pipeline}

The pipeline we will use to solve this task will consist of data augmentation, preprocessing, and the neural network.
For the problem at hand, we will vertically flip the images using a probability of $0.5$.

\begin{CodeInput}
R> augment <- po("augment_random_vertical_flip", p = 0.5)
\end{CodeInput}

For preprocessing, we will reshape each image to size $(3, 224, 224)$, which is expected by the ResNet-18 architecture.
By default this is done using bilinear interpolation.

\begin{CodeInput}
R> preprocess <- po("trafo_resize", size = c(224, 224))
\end{CodeInput}

For better training performance, we will freeze all but the head of the network for the first three epochs and only then train the remaining parameters.
This is possible using the \code{"unfreeze"} callback, where we specify which parameters to train from the beginning and then when to unfreeze other parameters.

\begin{CodeInput}
R> unfreezer <- t_clbk("unfreeze",
+    starting_weights = select_name(c("fc.weight", "fc.bias")),
+    unfreeze = data.table(epoch = 3, weights = select_all()))
\end{CodeInput}

The final step is the construction of the ResNet-18 learner which has a parameter \code{pretrained} that determines whether to use the pretrained weights.

\begin{CodeInput}
R> resnet <- lrn("classif.resnet18",
+    pretrained = TRUE, epochs = 5, device = "cuda", batch_size = 32,
+    opt.lr = 1e-4, measures_valid = msr("classif.acc"),
+    callbacks = list(unfreezer, t_clbk("history")))
\end{CodeInput}

By instructing the learner to use the pretrained weights, \pkg{torchvision} will download (and optionally cache) those weights upon first use and the torch learner will replace the output layer of the network with a new head with the correct output dimension for the task at hand.

\subsubsection{Training}

Now, we combine the three steps into a single new \code{GraphLearner}, configure its validation data, train it on the task, and plot the validation loss using \pkg{ggplot2} \citep{ref-ggplot2}.

\begin{CodeInput}
R> library("ggplot2")
R> learner <- as_learner(augment %>>% preprocess %>>% resnet)
R> learner$id <- "resnet"
R> set_validate(learner, 1 / 3)
R> learner$train(task)
R> learner$model$classif.resnet18$model$callbacks$history
\end{CodeInput}
\begin{CodeOutput}
TODO
\end{CodeOutput}

\subsection{Multi-modal data}\label{sec:multimodal}

Because the graph representation of neural networks in \code{mlr3torch} allows for multiple inputs, it naturally handles multimodal data.
In this section, we will define a custom network architecture for the \code{"melanoma"} task, where the goal is to predict whether a tumor is malignant or benign \citep{ref-international2020siim}.
The features include an image of the melanoma, as well as the approximate age, location, and the sex of the patient.
Furthermore, the data is grouped by \code{patient\_id}, as each patient on average appears around $15$ times in the data.

\begin{CodeInput}
R> task <- tsk("melanoma")
R> task
\end{CodeInput}
\begin{CodeOutput}
<TaskClassif:melanoma> (32701 x 5): Melanoma Classification
* Target: outcome
* Properties: twoclass, groups
* Features (4):
  - fct (2): anatom_site_general_challenge, sex
  - int (1): age_approx
  - lt (1): image
* Groups: patient_id
\end{CodeOutput}

Because malignant tumors are much rarer than benign ones, the target distribution is highly imbalanced.

\begin{CodeInput}
R> table(task$truth())
\end{CodeInput}
\begin{CodeOutput}
malignant    benign
      581     32120
\end{CodeOutput}

Also, for some observations, the approximate age is missing

\begin{CodeInput}
R> task$missings("age_approx")
\end{CodeInput}
\begin{CodeOutput}
age_approx
        44
\end{CodeOutput}


The network architecture we will create for this dataset will take one tensor input for the tabular features and one for the image, similarly to \Cref{fig:multi-inputs}.

We start by defining the graph segment for the tabular features, which consists of first subsetting the task to contain only those features, followed by sampling the mising values from their empirical distribution and then encoding them one-hot.
The three features are then fed as a single tensor into the network, where we repeat a block consisting of a linear layer, a ReLU activiation and a dropout layer, three times.

\begin{CodeInput}
R> block_ffn <- nn("linear", out_features = 500) %>>%
+    nn("relu") %>>% nn("dropout")
R> path_tabular <- po("select_1",
+      selector = selector_type(c("integer", "factor"))) %>>%
+    po("imputehist") %>>%
+    po("encode", method = "one-hot") %>>%
+    po("torch_ingress_num") %>>%
+    nn("block_1", block = block_ffn, n_blocks = 3)
\end{CodeInput}

For the image column, less preprocessing is required and we simply select the feature and then define three convolutional layers with batch normalization, ending in a flattening operation.

\begin{CodeInput}
R> path_image <- po("select_2", selector = selector_name("image")) %>>%
+    po("torch_ingress_ltnsr", shape = c(NA, 3, 128, 128)) %>>%
+    nn("conv2d_1", out_channels = 64, kernel_size = 7, stride = 2,
+      padding = 3) %>>%
+    nn("batch_norm2d_1") %>>%
+    nn("relu_1") %>>%
+    nn("max_pool2d_1", kernel_size = 3, stride = 2, padding = 1) %>>%
+    nn("conv2d_2", out_channels = 128, kernel_size = 3, stride = 1,
+      padding = 1) %>>%
+    nn("batch_norm2d_2") %>>%
+    nn("relu_2") %>>%
+    nn("conv2d_3", out_channels = 256, kernel_size = 3, stride = 1,
+      padding = 1) %>>%
+    nn("batch_norm2d_3") %>>%
+    nn("relu_3") %>>%
+    nn("flatten")
\end{CodeInput}

Next, we merge the two branches by concatenation and afterwards add one more dense layer with dropout, followed by the classification head.

\begin{CodeInput}
R> architecture <- list(path_tabular, path_image) %>>%
+    nn("merge_cat") %>>% nn("linear_1", out_features = 500) %>>%
+    nn("relu_4") %>>% nn("dropout_2") %>>% nn("head")
\end{CodeInput}

To finalize the model configuration, we specify the loss, optimizer and remaining training arguments.
As a loss function, we use cross-entropy, but increase the weight for the positive class (malignant tumor) by a factor of 10 due to the imbalanced class distribution.
For the optimizer we use again AdamW and train for 4 epochs with a batch size of $32$.

\begin{CodeInput}
R> model <- architecture %>>%
+    po("torch_loss",
+      t_loss("cross_entropy", class_weight = torch_tensor(10))) %>>%
+    po("torch_optimizer", t_opt("adamw", lr = 0.0005)) %>>%
+    po("torch_model_classif", epochs = 4, batch_size = 32, device = "cuda",
+      predict_type = "prob")
\end{CodeInput}

As further measures to address the imbalanced class distribution, we upsample the minority class by a factor of 4 and use random vertical and horizontal flipping, as well as random cropping for data augmentation.

\begin{CodeInput}
R> preprocessing <- po("classbalancing", ratio = 4, reference = "minor",
+      adjust = "minor") %>>%
+    po("augment_random_horizontal_flip") %>>%
+    po("augment_random_vertical_flip") %>>%
+    po("augment_random_crop", size = c(128, 128), pad_if_needed = TRUE)
+  glrn <- as_learner(preprocessing %>>% model)
\end{CodeInput}

Finally, we change the ID of the learner, resample it on the task using five-fold cross validation (which will respect the grouping variable), and display the resulting ROC curve using \pkg{mlr3viz} \citep{ref-mlr3viz2025}.

\begin{CodeInput}
R> library("mlr3viz")
R> glrn$id <- "multimodal"
R> rr <- resample(task, glrn, rsmp("cv", folds = 5))
R> autoplot(rr, type = "roc")
\end{CodeInput}

\begin{figure}[H]
    \centering
    % Placeholder for ROC curve output
    \includegraphics[width=0.6\textwidth]{figures/roc.png}
    \caption{ROC curve for multi-modal neural network evaluated using 5-fold cross-validation.}
    \label{fig:roc-curve}
\end{figure}

\section{Runtime evaluation}\label{sec:benchmarks}

Besides functionality, another important consideration of DL frameworks is their runtime performance.
In this section, we present some benchmark results comparing \mlrttorch{} with \torch{} and \pytorch{} code.
The difference between \mlrttorch{} and \torch{} is only the implementation of the training loop, as \mlrttorch{} relies on \torch{} as a backend.
We train a simple multi-layer perceptron with ReLU activation and different number of layers and latent dimension on a synthetically generated dataset for 20 epochs and present the time per batch.
We perform a 5 epoch warmup phase to mitigate the impact of fixed overheads on timing measurements, ensuring that our reported runtimes more accurately reflect realistic, long-duration training scenarios.
Beyond the architecture characteristics, we also vary the device (CPU and GPU), the optimizer (SGD and AdamW), and whether we JIT compile the network.
The computational details are presented in \Cref{app:comp-details}.
For the optimizers in \rlang{}, we use \texttt{optim\_ignite\_adamw} and \texttt{optim\_ignite\_sgd} which wrap the \libtorch{} \cpp{} implementation of these optimizers.
The runtime for the \rlang{} implementation of the optimizers in \torch{} is considerably higher.
For this reason, \mlrttorch{} uses the ignite versions by default.
The number of threads for the CPU measurements was set to $1$.
We also evaluate the effect of JIT compiling the neural network architecture.
This is a feature of \pkg{torch} that allows to lift out the computation of the forward path from \proglang{R} to \cpp{}, by converting the model into \proglang{TorchScript} â a statically typed intermediate representation â which is then executed by a dedicated \cpp{} interpreter for improved performance and portability.
This is either possible by directly writing \proglang{TorchScript} code (using \code{torch::jit\_compile}), which is a subset of \proglang{Python}, or by tracing a module (\code{torch::jit\_trace}) that was defined in \proglang{R}.
The latter works by executing the module using an example input and recording all torch operations that were applied.
Dynamic control flow causes issues for tracing because it records a fixed execution path and cannot capture data-dependent branching.
Therefore, it is not applicable to all architectures.

\begin{figure}[H]
    \centering
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/plot_adamw.png}
        \label{fig:adamw-benchmark}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.48\textwidth}
        \centering
        \includegraphics[width=\textwidth]{figures/plot_sgd.png}
        \label{fig:sgd-benchmark}
    \end{subfigure}
    \caption{Runtime results for AdamW (left) and SGD (right). The facets correspond to different latent dimensions.}
    \label{fig:optimizer-benchmark}
\end{figure}

\Cref{fig:adamw-benchmark} shows the results for the AdamW optimizer on both the CPU and GPU.
The difference between \torch{} and \mlrttorch{} is relatively small and can be attributed to typical framework overhead.
Especially for larger networks, the difference is negligible.
On the GPU, \pytorch{} is the fastest for all tested configurations, but the performance of the \rlang{} versions is still acceptable.
Interestingly, the \rlang{} versions are faster on the CPU for larger latent dimensions, which is presumably because of the use of the \libtorch{} implementation of AdamW.

% Now talk about jit compiling
Especially for smaller latent dimensions, JIT compiling does improve the performance in \rlang{}, but this effect is not as pronounced for larger dimensions where the actual GPU-heavy computations dominate the runtime.
In \pytorch{}, JIT compiling barely has an effect on the runtime.
Interestingly, JIT compiling does not only seem to reduce runtime by a constant factor, but also change the slope of the scaling of the runtime with the latent dimension.



Next, \Cref{fig:sgd-benchmark} presents the same runtime benchmarks when using the SGD optimizer.
In this scenario, \pytorch{} achieves the best performance for all configurations.
Because the SGD optimizer is simpler than AdamW, using the \libtorch{} implementation has less of an effect on the runtime.
Apart from that, the results are similar to those for the AdamW optimizer.


In summary, we recommend using the ignite versions of the optimizer to obtain good runtime performance and, if possible, JIT compiling the network.

\section{Conclusion}\label{sec:conclusion}

In this work we introduced \pkg{mlr3torch} --- an extensive DL framework in \rlang{} that integrates with the \mlrt{} ecosystem.
This enables seamless access to operations like resampling, benchmarking, and hyperparameter tuning.

The package offers a unified interface for defining, training, and evaluating neural networks, with support for both tabular data and generic tensors in supervised classification and regression tasks.
Neural network architectures are available in different levels of control: predefined architectures are readily available, \torch{} modules can be easily converted to \mlrttorch{} learners, and custom architectures can be defined as graphs.
Enabled by the lazy tensor data type and the integration with \mlrtpipelines{}, the whole modelling workflow (including preprocessing, data augmentation, and the definition of the neural network itself) can be represented as a single graph.

A key strength of \pkg{mlr3torch} is its extensibility and flexibility, be it in defining architectures, customizing the training, e.g., via the provided callback mechanism, or the possiblity to extend the package.
We demonstrated the package's capabilities through three practical use cases. First, we showed how to perform neural architecture search on a tabular regression problem.
Second, we fine-tuned a pretrained ResNet-18 model to a binary image classification task.
Finally, we showcased the framework's ability to define architectures with multiple inputs for a multi-modal task that contained both tabular data and images.

Our runtime benchmarks demonstrate minimal framework overhead of \mlrttorch{} compared to pure \torch{} implementations and that the right configuration yields good runtime performance compared to \pytorch{}.

Future development will focus on extending support to other learning tasks such as survival analysis, implementing additional architectures, further improving runtime performance, and extending the support to other modalities like natural language.



%\input{6-conclusion}

\section*{Acknowledgements}

Sebastian Fischer is supported by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) â 460135501 (NFDI project MaRDI).

\bibliography{refs}

\begin{appendix}

\section{Computational details}\label{app:comp-details}

\begin{itemize}
    \item CUDA Version
    \item R version
    \item torch, mlr3torchm pytorch version
    \item GPU
    \item OS
    \item VRAM
\end{itemize}

\end{appendix}


\end{document}
