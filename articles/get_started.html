<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="mlr3torch">
<title>Get Started • mlr3torch</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png">
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.2.2/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.2.2/bootstrap.bundle.min.js"></script><link href="../deps/Roboto-0.4.7/font.css" rel="stylesheet">
<link href="../deps/JetBrains_Mono-0.4.7/font.css" rel="stylesheet">
<link href="../deps/Roboto_Slab-0.4.7/font.css" rel="stylesheet">
<!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Get Started">
<meta property="og:description" content="mlr3torch">
<meta property="og:image" content="https://mlr3torch.mlr-org.com/logo.svg">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-light navbar-expand-lg bg-light"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">mlr3torch</a>

    <small class="nav-text text-default me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="Unreleased version">0.0.0.9000</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../reference/index.html">
    <span class="fa fa fa fa-file-alt"></span>
     
    Reference
  </a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../news/index.html">Changelog</a>
</li>
<li class="active nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-articles">Articles</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-articles">
    <a class="dropdown-item" href="../articles/get_started.html">Get Started</a>
    <a class="dropdown-item" href="../articles/image_classification.html">Image Classification</a>
    <a class="dropdown-item" href="../articles/pipeop_torch.html">Neural Networks as Graphs</a>
  </div>
</li>
<li class="nav-item">
  <a class="external-link nav-link" href="https://mlr3book.mlr-org.com">
    <span class="fa fa fa fa-link"></span>
     
    mlr3book
  </a>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/mlr-org/mlr3torch">
    <span class="fa fa fa fa-github"></span>
     
  </a>
</li>
<li class="nav-item">
  <a class="external-link nav-link" href="https://lmmisld-lmu-stats-slds.srv.mwn.de/mlr_invite/">
    <span class="fa fa fa fa-comments"></span>
     
  </a>
</li>
<li class="nav-item">
  <a class="external-link nav-link" href="https://stackoverflow.com/questions/tagged/mlr3">
    <span class="fab fa fab fa-stack-overflow"></span>
     
  </a>
</li>
<li class="nav-item">
  <a class="external-link nav-link" href="https://mlr-org.com/">
    <span class="fa fa-rss"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.svg" class="logo" alt=""><h1>Get Started</h1>
            
      
      
      <div class="d-none name"><code>get_started.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="quickstart">Quickstart<a class="anchor" aria-label="anchor" href="#quickstart"></a>
</h2>
<p>In this vignette we will show how to get started with
<code>mlr3torch</code> by training a simple neural network on tabular
data. We assume that you are familiar with the <code>mlr3</code>
framework, for a detailed description see e.g. the <a href="https://mlr3book.mlr-org.com/" class="external-link">mlr3 book</a>. As a first example,
we will train a simple multi-layer perceptron (MLP) on the well-known
“mtcars” task. We first set a seed for reproducibility, load the library
and construct the task.</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">314</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va">mlr3torch</span><span class="op">)</span></span>
<span><span class="va">task</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html" class="external-link">tsk</a></span><span class="op">(</span><span class="st">"mtcars"</span><span class="op">)</span></span>
<span><span class="va">task</span><span class="op">$</span><span class="fu">head</span><span class="op">(</span><span class="op">)</span></span>
<span><span class="co">#&gt;     mpg am carb cyl disp drat gear  hp  qsec vs    wt</span></span>
<span><span class="co">#&gt; 1: 21.0  1    4   6  160 3.90    4 110 16.46  0 2.620</span></span>
<span><span class="co">#&gt; 2: 21.0  1    4   6  160 3.90    4 110 17.02  0 2.875</span></span>
<span><span class="co">#&gt; 3: 22.8  1    1   4  108 3.85    4  93 18.61  1 2.320</span></span>
<span><span class="co">#&gt; 4: 21.4  0    1   6  258 3.08    3 110 19.44  1 3.215</span></span>
<span><span class="co">#&gt; 5: 18.7  0    2   8  360 3.15    3 175 17.02  0 3.440</span></span>
<span><span class="co">#&gt; 6: 18.1  0    1   6  225 2.76    3 105 20.22  1 3.460</span></span></code></pre></div>
<p>Learners in <code>mlr3torch</code> work very similary to other
<code>mlr3</code> learners. Below, we construct a simple multi layer
perceptron for regression. We do this as usual using the
<code><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html" class="external-link">lrn()</a></code> function and specify its parameters: The hidden
dimension is set to 50 and the number of layers to 2. For training, we
set the batch size to 32, the number of training epochs to 30 and the
device to <code>"cpu"</code>. For a complete description of the
available parameters see <code><a href="../reference/mlr_learners.mlp.html">?mlr3torch::LearnerTorchMLP</a></code>.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">mlp</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html" class="external-link">lrn</a></span><span class="op">(</span><span class="st">"regr.mlp"</span>,</span>
<span>  <span class="co"># architecture parameters</span></span>
<span>  d_hidden <span class="op">=</span> <span class="fl">50</span>, layers <span class="op">=</span> <span class="fl">2</span>,</span>
<span>  <span class="co"># training arguments</span></span>
<span>  batch_size <span class="op">=</span> <span class="fl">32</span>, epochs <span class="op">=</span> <span class="fl">30</span>, device <span class="op">=</span> <span class="st">"cpu"</span></span>
<span><span class="op">)</span></span>
<span><span class="va">mlp</span></span>
<span><span class="co">#&gt; &lt;LearnerTorchMLP[regr]:regr.mlp&gt;: My Little Powny</span></span>
<span><span class="co">#&gt; * Model: -</span></span>
<span><span class="co">#&gt; * Optimizer: adam</span></span>
<span><span class="co">#&gt; * Loss: mse</span></span>
<span><span class="co">#&gt; * Callbacks: -</span></span>
<span><span class="co">#&gt; * Parameters: layers=2, d_hidden=50, p=0.5, activation=&lt;nn_relu&gt;,</span></span>
<span><span class="co">#&gt;   activation_args=&lt;list&gt;, batch_size=32, epochs=30, device=cpu,</span></span>
<span><span class="co">#&gt;   measures_train=&lt;list&gt;, measures_valid=&lt;list&gt;, drop_last=FALSE,</span></span>
<span><span class="co">#&gt;   shuffle=TRUE, num_threads=1, seed=random</span></span>
<span><span class="co">#&gt; * Packages: mlr3, mlr3torch, torch</span></span>
<span><span class="co">#&gt; * Predict Types:  [response]</span></span>
<span><span class="co">#&gt; * Feature Types: integer, numeric</span></span>
<span><span class="co">#&gt; * Properties: -</span></span></code></pre></div>
<p>We can use this learner for training and prediction just like any
other regression learner. Below, we split the observations into a
training and test set, train the learner on the training set and predict
the test set. Finally, we compute the mean squared error (MSE) of the
predictions.</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Split the obersevations into training and test set</span></span>
<span><span class="va">splits</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/partition.html" class="external-link">partition</a></span><span class="op">(</span><span class="va">task</span><span class="op">)</span></span>
<span><span class="co"># Train the learner on the train set</span></span>
<span><span class="va">mlp</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span><span class="va">task</span>, row_ids <span class="op">=</span> <span class="va">splits</span><span class="op">$</span><span class="va">train</span><span class="op">)</span></span>
<span><span class="co"># Predict the test set</span></span>
<span><span class="va">prediction</span> <span class="op">=</span> <span class="va">mlp</span><span class="op">$</span><span class="fu">predict</span><span class="op">(</span><span class="va">task</span>, row_ids <span class="op">=</span> <span class="va">splits</span><span class="op">$</span><span class="va">test</span><span class="op">)</span></span>
<span><span class="co"># Compute the mse</span></span>
<span><span class="va">prediction</span><span class="op">$</span><span class="fu">score</span><span class="op">(</span><span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html" class="external-link">msr</a></span><span class="op">(</span><span class="st">"regr.mse"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; regr.mse </span></span>
<span><span class="co">#&gt; 181.2989</span></span></code></pre></div>
</div>
<div class="section level2">
<h2 id="configuring-a-learner">Configuring a Learner<a class="anchor" aria-label="anchor" href="#configuring-a-learner"></a>
</h2>
<p>Although torch learners are quite like other <code>mlr3</code>
learners, there are some differences. One is that all
<code>LearnerTorch</code> classes have <em>construction arguments</em>,
i.e. torch learners are more modular then other learners. While learners
are free to implement their own construction arguments, there are some
that are common to all torch learners, namely the <code>loss</code>,
<code>optimizer</code> and <code>callbacks</code>.</p>
<p>As these are construction arguments (and not part of the learner’s
<code>ParamSet</code>) they cannot be changed afterwards. The reason for
this is that these construction arguments can themselves have
parameters, which are exposed through the learner’s hyperparameter set.
In the previous example, we did not specify any of these explicitly and
used the default values, which was the Adam optimizer, MSE as the loss
and no callbacks. We will now show how to configure these three aspects
of a learner through the <code><a href="../reference/TorchOptimizer.html">mlr3torch::TorchOptimizer</a></code>,
<code><a href="../reference/TorchLoss.html">mlr3torch::TorchLoss</a></code>, and
<code><a href="../reference/TorchCallback.html">mlr3torch::TorchCallback</a></code> classes.</p>
<div class="section level3">
<h3 id="loss">Loss<a class="anchor" aria-label="anchor" href="#loss"></a>
</h3>
<p>The loss function, also known as the objective function or cost
function, measures the discrepancy between the predicted output and the
true output. It quantifies how well the model is performing during
training. The R package <code>torch</code>, which underpins the
<code>mlr3torch</code> framework, already provides a number of
predefined loss functions such as the Mean Squared Error
(<code>nn_mse_loss</code>), the Mean Absolute Error
(<code>nn_l1_loss</code>), or the cross entropy loss
(<code>nn_cross_entropy_loss</code>). In <code>mlr3torch</code>, we
represent loss functions using the <code><a href="../reference/TorchLoss.html">mlr3torch::TorchLoss</a></code>
class. It provides a thin wrapper around the torch loss functions and
annotates them with meta information, most importantly a
<code><a href="https://paradox.mlr-org.com/reference/ParamSet.html" class="external-link">paradox::ParamSet</a></code>. Such an object can be constructed using
<code><a href="../reference/t_loss.html">t_loss()</a></code>. Below, we construct the L1 loss function, which
is also known as Mean Absolute Error (MAE). The printed output below
informs us about the wrapped loss function <code>(nn_l1_loss</code>),
the set parameters, the pacakges it depends on and for which task types
it can be used.</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">l1</span> <span class="op">=</span> <span class="fu"><a href="../reference/t_loss.html">t_loss</a></span><span class="op">(</span><span class="st">"l1"</span><span class="op">)</span></span>
<span><span class="va">l1</span></span>
<span><span class="co">#&gt; &lt;TorchLoss:l1&gt; Absolute Error</span></span>
<span><span class="co">#&gt; * Generator: nn_l1_loss</span></span>
<span><span class="co">#&gt; * Parameters: list()</span></span>
<span><span class="co">#&gt; * Packages: torch</span></span>
<span><span class="co">#&gt; * Task Types: regr</span></span></code></pre></div>
<p>Its <code>ParamSet</code> contains only one parameter, namely
<code>reduction</code>, which specifies how the loss is reduced over the
batch.</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># the paradox::ParamSet of the loss</span></span>
<span><span class="va">l1</span><span class="op">$</span><span class="va">param_set</span></span>
<span><span class="co">#&gt; &lt;ParamSet&gt;</span></span>
<span><span class="co">#&gt;           id    class lower upper nlevels default value</span></span>
<span><span class="co">#&gt; 1: reduction ParamFct    NA    NA       2    mean</span></span></code></pre></div>
<p>The wrapped torch loss is accessible through the slot
<code>$generator</code>.</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">l1</span><span class="op">$</span><span class="va">generator</span></span>
<span><span class="co">#&gt; &lt;nn_l1_loss&gt; object generator</span></span>
<span><span class="co">#&gt;   Inherits from: &lt;inherit&gt;</span></span>
<span><span class="co">#&gt;   Public:</span></span>
<span><span class="co">#&gt;     .classes: nn_l1_loss nn_loss nn_module</span></span>
<span><span class="co">#&gt;     initialize: function (reduction = "mean") </span></span>
<span><span class="co">#&gt;     forward: function (input, target) </span></span>
<span><span class="co">#&gt;     clone: function (deep = FALSE) </span></span>
<span><span class="co">#&gt;   Parent env: &lt;environment: 0x55c0e4104d10&gt;</span></span>
<span><span class="co">#&gt;   Locked objects: FALSE</span></span>
<span><span class="co">#&gt;   Locked class: FALSE</span></span>
<span><span class="co">#&gt;   Portable: TRUE</span></span></code></pre></div>
<p>We can pass the <code>TorchLoss</code> as the argument
<code>loss</code> during initialization of the learner. The parameters
of the loss are added to the learner’s <code>ParamSet</code>, prefixed
with <code>"loss."</code>. When added to the learner’s parameter set,
the loss’s parameters are prefixed with <code>"loss."</code>.</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">mlp_l1</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html" class="external-link">lrn</a></span><span class="op">(</span><span class="st">"regr.mlp"</span>, loss <span class="op">=</span> <span class="va">l1</span><span class="op">)</span></span>
<span><span class="va">mlp_l1</span><span class="op">$</span><span class="va">param_set</span><span class="op">$</span><span class="va">params</span><span class="op">$</span><span class="va">loss.reduction</span></span>
<span><span class="co">#&gt;                id    class lower upper   levels default</span></span>
<span><span class="co">#&gt; 1: loss.reduction ParamFct    NA    NA mean,sum    mean</span></span></code></pre></div>
<p>All predefined loss functions are stored in the
<code>mlr3torch_losses</code> dictionary, from which they can be
retrieved using <code>t_loss(&lt;key&gt;)</code>.</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">mlr3torch_losses</span></span>
<span><span class="co">#&gt; &lt;DictionaryMlr3torchLosses&gt; with 3 stored values</span></span>
<span><span class="co">#&gt; Keys: cross_entropy, l1, mse</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="optimizer">Optimizer<a class="anchor" aria-label="anchor" href="#optimizer"></a>
</h3>
<p>The optimizer determines how the model’s weights are updated based on
the calculated loss. It adjusts the parameters of the model to minimize
the loss function, optimizing the model’s performance. Optimizers work
analogous to loss functions, i.e. <code>mlr3torch</code> provides a thin
wrapper – the <code>TorchOptimizer</code> class – around the optimizers
such as Adam (<code>optim_adam</code>) or SGD (<code>optim_sgd</code>).
<code>TorchLoss</code> objects can be constructed using
<code><a href="../reference/t_opt.html">t_opt()</a></code>. For optimizers, the associated
<code>ParamSet</code> is more interesting as seen below.</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">sgd</span> <span class="op">=</span> <span class="fu"><a href="../reference/t_opt.html">t_opt</a></span><span class="op">(</span><span class="st">"sgd"</span><span class="op">)</span></span>
<span><span class="va">sgd</span></span>
<span><span class="co">#&gt; &lt;TorchOptimizer:sgd&gt; Stochastic Gradient Descent</span></span>
<span><span class="co">#&gt; * Generator: optim_sgd</span></span>
<span><span class="co">#&gt; * Parameters: list()</span></span>
<span><span class="co">#&gt; * Packages: torch</span></span>
<span></span>
<span><span class="co"># The paradox::ParamSet of the optimizer</span></span>
<span><span class="va">sgd</span><span class="op">$</span><span class="va">param_set</span></span>
<span><span class="co">#&gt; &lt;ParamSet&gt;</span></span>
<span><span class="co">#&gt;              id    class lower upper nlevels        default value</span></span>
<span><span class="co">#&gt; 1:           lr ParamDbl     0   Inf     Inf &lt;NoDefault[3]&gt;      </span></span>
<span><span class="co">#&gt; 2:     momentum ParamDbl     0     1     Inf              0      </span></span>
<span><span class="co">#&gt; 3:    dampening ParamDbl     0     1     Inf              0      </span></span>
<span><span class="co">#&gt; 4: weight_decay ParamDbl     0     1     Inf              0      </span></span>
<span><span class="co">#&gt; 5:     nesterov ParamLgl    NA    NA       2          FALSE</span></span></code></pre></div>
<p>The wrapped torch optimizer can be accessed through the slot
<code>generator</code>.</p>
<p>Parameters of <code>TorchOptimizer</code> (but also
<code>TorchLoss</code> and <code>TorchCallback</code>) can be set in the
usual <code>mlr3</code> way, i.e. either during construction, or
afterwards using the <code>$set_values()</code> method of the parameter
set.</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">sgd</span><span class="op">$</span><span class="va">param_set</span><span class="op">$</span><span class="fu">set_values</span><span class="op">(</span></span>
<span>  lr <span class="op">=</span> <span class="fl">0.5</span>, <span class="co"># increase learning rate</span></span>
<span>  nesterov <span class="op">=</span> <span class="cn">FALSE</span> <span class="co"># no nesterov momentum</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>Below we see that the optimizer’s parameters are added to the
learner’s <code>ParamSet</code> (prefixed with <code>"opt."</code>) and
that the values are set to the values we specified.</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">mlp_sgd</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html" class="external-link">lrn</a></span><span class="op">(</span><span class="st">"regr.mlp"</span>, optimizer <span class="op">=</span> <span class="va">sgd</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://Rdatatable.gitlab.io/data.table/reference/as.data.table.html" class="external-link">as.data.table</a></span><span class="op">(</span><span class="va">mlp_sgd</span><span class="op">$</span><span class="va">param_set</span><span class="op">)</span><span class="op">[</span></span>
<span>  <span class="fu"><a href="https://rdrr.io/r/base/startsWith.html" class="external-link">startsWith</a></span><span class="op">(</span><span class="va">id</span>, <span class="st">"opt."</span><span class="op">)</span><span class="op">]</span><span class="op">[[</span><span class="fl">1L</span><span class="op">]</span><span class="op">]</span></span>
<span><span class="co">#&gt; [1] "opt.lr"           "opt.momentum"     "opt.dampening"    "opt.weight_decay"</span></span>
<span><span class="co">#&gt; [5] "opt.nesterov"</span></span>
<span><span class="va">mlp_sgd</span><span class="op">$</span><span class="va">param_set</span><span class="op">$</span><span class="va">values</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"opt.lr"</span>, <span class="st">"opt.nesterov"</span><span class="op">)</span><span class="op">]</span></span>
<span><span class="co">#&gt; $opt.lr</span></span>
<span><span class="co">#&gt; [1] 0.5</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $opt.nesterov</span></span>
<span><span class="co">#&gt; [1] FALSE</span></span></code></pre></div>
<p>By exposing the optimizer’s parameters, they can be conveniently
tuned using <a href="https://github.com/mlr-org/mlr3tuning" class="external-link"><code>mlr3tuning</code></a>.</p>
<p>All predefined optimizers are stored in the
<code>mlr3torch_optimizers</code> dictionary.</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">mlr3torch_optimizers</span></span>
<span><span class="co">#&gt; &lt;DictionaryMlr3torchOptimizers&gt; with 7 stored values</span></span>
<span><span class="co">#&gt; Keys: adadelta, adagrad, adam, asgd, rmsprop, rprop, sgd</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="callbacks">Callbacks<a class="anchor" aria-label="anchor" href="#callbacks"></a>
</h3>
<p>The third major configuration option are callbacks, which are objects
in <code>mlr3torch</code> that allow you to customize the behavior of
the training process at various stages. They are called at specific
points during training, such as the beginning or end of an epoch.
Callbacks enable you to perform additional actions, such as saving model
checkpoints, logging metrics, or implementing custom functionality for
specific training scenarios.</p>
<p>Callbacks are different from the optimizer and loss as the callbacks
themselves (not only their wrappers) are defined in
<code>mlr3torch</code>. For this reason, there is a more in-depth
coverage of the callback mechanism in a separate vignette (TODO:). For
the purpose of this “get started” guide we will only show how to use
predefined callbacks. The wrapper class is <code>TorchCallback</code>,
while callbacks themselves have class <code>CallbackSet</code>. Below,
we retrieve the predefined <code>"history"</code> callback using
<code><a href="../reference/t_clbk.html">t_clbk()</a></code>, which has no parameters and merely saves the
training history in the learner.</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">history</span> <span class="op">=</span> <span class="fu"><a href="../reference/t_clbk.html">t_clbk</a></span><span class="op">(</span><span class="st">"history"</span><span class="op">)</span></span>
<span><span class="va">history</span></span>
<span><span class="co">#&gt; &lt;TorchCallback:history&gt; History</span></span>
<span><span class="co">#&gt; * Generator: CallbackSetHistory</span></span>
<span><span class="co">#&gt; * Parameters: list()</span></span>
<span><span class="co">#&gt; * Packages: mlr3torch,torch</span></span></code></pre></div>
<p>It wraps the <code>CallbackSetHistory</code> class.</p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">history</span><span class="op">$</span><span class="va">generator</span></span>
<span><span class="co">#&gt; &lt;CallbackSetHistory&gt; object generator</span></span>
<span><span class="co">#&gt;   Inherits from: &lt;CallbackSet&gt;</span></span>
<span><span class="co">#&gt;   Public:</span></span>
<span><span class="co">#&gt;     on_begin: function () </span></span>
<span><span class="co">#&gt;     on_end: function () </span></span>
<span><span class="co">#&gt;     on_before_valid: function () </span></span>
<span><span class="co">#&gt;     on_epoch_end: function () </span></span>
<span><span class="co">#&gt;     plot: function (measures, set = "valid", epochs = NULL, theme = ggplot2::theme_minimal(), </span></span>
<span><span class="co">#&gt;     clone: function (deep = FALSE) </span></span>
<span><span class="co">#&gt;   Private:</span></span>
<span><span class="co">#&gt;     deep_clone: function (name, value) </span></span>
<span><span class="co">#&gt;   Parent env: &lt;environment: namespace:mlr3torch&gt;</span></span>
<span><span class="co">#&gt;   Locked objects: FALSE</span></span>
<span><span class="co">#&gt;   Locked class: FALSE</span></span>
<span><span class="co">#&gt;   Portable: TRUE</span></span></code></pre></div>
<p>If we wanted to learn about what the callback does, we can access the
help page of the wrapped object using the <code>$help()</code> method.
Note that this is also possible for the loss and optimizer.</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">history</span><span class="op">$</span><span class="fu">help</span><span class="op">(</span><span class="op">)</span></span></code></pre></div>
<p>All predefined callbacks are stored in the
<code>mlr3torch_callbacks</code> dictionary.</p>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">mlr3torch_callbacks</span></span>
<span><span class="co">#&gt; &lt;DictionaryMlr3torchCallbacks&gt; with 3 stored values</span></span>
<span><span class="co">#&gt; Keys: checkpoint, history, progress</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="putting-it-together">Putting it Together<a class="anchor" aria-label="anchor" href="#putting-it-together"></a>
</h3>
<p>We can proceed by defining our customized MLP learner using the loss,
optimizer and callback we have just covered. To really make use of the
history callback, we have to specify which scores we want to keep track
of through the <code>measures_train</code> parameter. It takes in one or
more <code><a href="https://mlr3.mlr-org.com/reference/Measure.html" class="external-link">mlr3::Measure</a></code> objects. Here we decide to only evaluate
the Mean Absolute Error (MAE) and leave the other parameters as
before.</p>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">mlp_custom</span> <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html" class="external-link">lrn</a></span><span class="op">(</span><span class="st">"regr.mlp"</span>,</span>
<span>  <span class="co"># construction arguments</span></span>
<span>  optimizer <span class="op">=</span> <span class="va">sgd</span>, loss <span class="op">=</span> <span class="va">l1</span>, callbacks <span class="op">=</span> <span class="va">history</span>,</span>
<span>  <span class="co"># scores to keep track of</span></span>
<span>  measures_train <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html" class="external-link">msr</a></span><span class="op">(</span><span class="st">"regr.mae"</span><span class="op">)</span>,</span>
<span>  <span class="co"># other parameters are left as-is:</span></span>
<span>  <span class="co"># architecture</span></span>
<span>  d_hidden <span class="op">=</span> <span class="fl">50</span>, layers <span class="op">=</span> <span class="fl">2</span>,</span>
<span>  <span class="co"># training arguments</span></span>
<span>  batch_size <span class="op">=</span> <span class="fl">32</span>, epochs <span class="op">=</span> <span class="fl">30</span>, device <span class="op">=</span> <span class="st">"cpu"</span></span>
<span><span class="op">)</span></span></code></pre></div>
<p>The printed output below informs us that the</p>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">mlp_custom</span></span>
<span><span class="co">#&gt; &lt;LearnerTorchMLP[regr]:regr.mlp&gt;: My Little Powny</span></span>
<span><span class="co">#&gt; * Model: -</span></span>
<span><span class="co">#&gt; * Optimizer: sgd</span></span>
<span><span class="co">#&gt; * Loss: l1</span></span>
<span><span class="co">#&gt; * Callbacks: history</span></span>
<span><span class="co">#&gt; * Parameters: layers=2, d_hidden=50, p=0.5, activation=&lt;nn_relu&gt;,</span></span>
<span><span class="co">#&gt;   activation_args=&lt;list&gt;, batch_size=32, epochs=30, device=cpu,</span></span>
<span><span class="co">#&gt;   measures_train=&lt;MeasureRegrSimple&gt;, measures_valid=&lt;list&gt;,</span></span>
<span><span class="co">#&gt;   drop_last=FALSE, shuffle=TRUE, num_threads=1, seed=random,</span></span>
<span><span class="co">#&gt;   opt.lr=0.5, opt.nesterov=FALSE</span></span>
<span><span class="co">#&gt; * Packages: mlr3, mlr3torch, torch</span></span>
<span><span class="co">#&gt; * Predict Types:  [response]</span></span>
<span><span class="co">#&gt; * Feature Types: integer, numeric</span></span>
<span><span class="co">#&gt; * Properties: -</span></span></code></pre></div>
<p>We now train the learner on the “mtcars” task from the introductory
example using the same train-test split.</p>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">mlp_custom</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span><span class="va">task</span>, row_ids <span class="op">=</span> <span class="va">splits</span><span class="op">$</span><span class="va">train</span><span class="op">)</span></span>
<span><span class="va">prediction_custom</span> <span class="op">=</span> <span class="va">mlp_custom</span><span class="op">$</span><span class="fu">predict</span><span class="op">(</span><span class="va">task</span>, row_ids <span class="op">=</span> <span class="va">splits</span><span class="op">$</span><span class="va">test</span><span class="op">)</span></span></code></pre></div>
<p>Below we make predictions on the unseen test data and compare the
scores. Because we directly optimized the L1 (aka MAE) loss and tweaked
the learning rate, our configured <code>mlp_custom</code> learner has a
lower MAE than the default <code>mlp</code> learner.</p>
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">prediction_custom</span><span class="op">$</span><span class="fu">score</span><span class="op">(</span><span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html" class="external-link">msr</a></span><span class="op">(</span><span class="st">"regr.mae"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; regr.mae </span></span>
<span><span class="co">#&gt; 5.323898</span></span>
<span><span class="va">prediction</span><span class="op">$</span><span class="fu">score</span><span class="op">(</span><span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html" class="external-link">msr</a></span><span class="op">(</span><span class="st">"regr.mae"</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; regr.mae </span></span>
<span><span class="co">#&gt; 10.92429</span></span></code></pre></div>
<p>To understand the impact of the history callback, we will need to dig
into the state of a trained <code>LearnerTorch</code>, which we will do
in the next section.</p>
</div>
</div>
<div class="section level2">
<h2 id="learnertorchs-state">
<code>LearnerTorch</code>’s State<a class="anchor" aria-label="anchor" href="#learnertorchs-state"></a>
</h2>
<p>After training a <code>LearnerTorch</code> like above, the trained
model can be accessed through the <code>$model</code> slot, which is a
list, whose most important elements are:</p>
<ul>
<li>
<p><code>network</code> - The trained network, i.e. a
<code><a href="https://rdrr.io/pkg/torch/man/nn_module.html" class="external-link">torch::nn_module</a></code>.</p>
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">mlp_custom</span><span class="op">$</span><span class="va">model</span><span class="op">$</span><span class="va">network</span></span>
<span><span class="co">#&gt; An `nn_module` containing 3,151 parameters.</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; ── Modules ─────────────────────────────────────────────────────────────────────</span></span>
<span><span class="co">#&gt; • 0: &lt;nn_linear&gt; #550 parameters</span></span>
<span><span class="co">#&gt; • 1: &lt;nn_relu&gt; #0 parameters</span></span>
<span><span class="co">#&gt; • 2: &lt;nn_dropout&gt; #0 parameters</span></span>
<span><span class="co">#&gt; • 3: &lt;nn_linear&gt; #2,550 parameters</span></span>
<span><span class="co">#&gt; • 4: &lt;nn_relu&gt; #0 parameters</span></span>
<span><span class="co">#&gt; • 5: &lt;nn_dropout&gt; #0 parameters</span></span>
<span><span class="co">#&gt; • 6: &lt;nn_linear&gt; #51 parameters</span></span></code></pre></div>
</li>
<li>
<p><code>optimizer</code> - The torch optimizer used for training.
This is the actual optimizer – in this case a
<code><a href="https://rdrr.io/pkg/torch/man/optim_sgd.html" class="external-link">torch::optim_sgd</a></code> and not the <code>TorchOptimizer</code>
wrapper.</p>
<div class="sourceCode" id="cb22"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/class.html" class="external-link">class</a></span><span class="op">(</span><span class="va">mlp_custom</span><span class="op">$</span><span class="va">model</span><span class="op">$</span><span class="va">optimizer</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "optim_sgd"       "torch_optimizer" "R6"</span></span></code></pre></div>
</li>
<li>
<p><code>loss_fn</code> - The loss function – in this case the
<code>nn_l1_loss</code> – used during training.</p>
<div class="sourceCode" id="cb23"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">mlp_custom</span><span class="op">$</span><span class="va">model</span><span class="op">$</span><span class="va">loss_fn</span></span>
<span><span class="co">#&gt; An `nn_module` containing 0 parameters.</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/class.html" class="external-link">class</a></span><span class="op">(</span><span class="va">mlp_custom</span><span class="op">$</span><span class="va">model</span><span class="op">$</span><span class="va">loss_fn</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "nn_l1_loss" "nn_loss"    "nn_module"</span></span></code></pre></div>
</li>
<li>
<p><code>callbacks</code> - A list of callbacks used during
training.</p>
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">mlp_custom</span><span class="op">$</span><span class="va">model</span><span class="op">$</span><span class="va">callbacks</span></span>
<span><span class="co">#&gt; $history</span></span>
<span><span class="co">#&gt; &lt;CallbackSetHistory&gt;</span></span>
<span><span class="co">#&gt;   Inherits from: &lt;CallbackSet&gt;</span></span>
<span><span class="co">#&gt;   Public:</span></span>
<span><span class="co">#&gt;     clone: function (deep = FALSE) </span></span>
<span><span class="co">#&gt;     ctx: NULL</span></span>
<span><span class="co">#&gt;     on_before_valid: function () </span></span>
<span><span class="co">#&gt;     on_begin: function () </span></span>
<span><span class="co">#&gt;     on_end: function () </span></span>
<span><span class="co">#&gt;     on_epoch_end: function () </span></span>
<span><span class="co">#&gt;     plot: function (measures, set = "valid", epochs = NULL, theme = ggplot2::theme_minimal(), </span></span>
<span><span class="co">#&gt;     train: data.table, data.frame</span></span>
<span><span class="co">#&gt;     valid: data.table, data.frame</span></span>
<span><span class="co">#&gt;   Private:</span></span>
<span><span class="co">#&gt;     deep_clone: function (name, value)</span></span></code></pre></div>
</li>
<li><p><code>seed</code> - The seed that was used during training. By
default, a random seed is sampled at the beginning of the training loop
but provided in the model afterwards.</p></li>
</ul>
<p>To show the training loss over time, we can use the
<code>$plot()</code> method of the history callback. We only plot the
epochs 5 to 30 because the loss is very unstable before that.</p>
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://ggplot2.tidyverse.org" class="external-link">ggplot2</a></span><span class="op">)</span></span>
<span><span class="va">mlp_custom</span><span class="op">$</span><span class="va">model</span><span class="op">$</span><span class="va">callbacks</span><span class="op">$</span><span class="va">history</span><span class="op">$</span><span class="fu">plot</span><span class="op">(</span><span class="st">"regr.mae"</span>, set <span class="op">=</span> <span class="st">"train"</span>, epochs <span class="op">=</span> <span class="fl">5</span><span class="op">:</span><span class="fl">30</span><span class="op">)</span></span></code></pre></div>
<p><img src="get_started_files/figure-html/unnamed-chunk-26-1.png" width="50%" style="display: block; margin: auto;"></p>
<p>To not only track the training, but also the validation loss, we need
to be able to specify a validation set, which we will learn how to do in
the next section.</p>
</div>
<div class="section level2">
<h2 id="validation">Validation<a class="anchor" aria-label="anchor" href="#validation"></a>
</h2>
<p>While a decreasing training loss is necessary for a model to
generalize well, it is not sufficient. For this reason, it is common to
also track the loss of a neural network on a validation set. To learn
how to specify validation sets for <code>mlr3torch</code>, we need to
understand the different row roles that can be set in
<code><a href="https://mlr3.mlr-org.com/reference/Task.html" class="external-link">mlr3::Task</a></code>s. Each observation can be assigned to the
following roles:</p>
<ul>
<li>
<code>use</code> - These are the observations that are used for
training when calling <code>$train()</code>.</li>
<li>
<code>test</code> - When observations are set to this role, they are
available as an independent validation set during training.</li>
<li>
<code>holdout</code> - These observations are neither used for
training nor validation and we will not discuss them here further.</li>
</ul>
<p>For the mtcars task, all observations had the row roles
<code>"use"</code>.</p>
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">task</span><span class="op">$</span><span class="va">row_roles</span></span>
<span><span class="co">#&gt; $use</span></span>
<span><span class="co">#&gt;  [1]  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25</span></span>
<span><span class="co">#&gt; [26] 26 27 28 29 30 31 32</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $test</span></span>
<span><span class="co">#&gt; integer(0)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $holdout</span></span>
<span><span class="co">#&gt; integer(0)</span></span></code></pre></div>
<p>We use our train-test split from the beginning of this vignette and
set the test observations to the role <code>"test"</code>. By using
<code>$set_row_roles</code>, they are automatically removed from the
<code>"use"</code> role.</p>
<div class="sourceCode" id="cb27"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">task</span><span class="op">$</span><span class="fu">set_row_roles</span><span class="op">(</span><span class="va">splits</span><span class="op">$</span><span class="va">test</span>, <span class="st">"test"</span><span class="op">)</span></span>
<span><span class="va">task</span><span class="op">$</span><span class="va">row_roles</span></span>
<span><span class="co">#&gt; $use</span></span>
<span><span class="co">#&gt;  [1]  2  3  4  5  6  8  9 11 13 14 16 18 19 22 23 24 27 28 29 30 32</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $test</span></span>
<span><span class="co">#&gt;  [1]  1 10 21 25  7 12 15 17 31 20 26</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; $holdout</span></span>
<span><span class="co">#&gt; integer(0)</span></span></code></pre></div>
<p>We can now set some validation metrics that we want to evaluate on
the test set during training. Below, we set the validation measures to
also track the MAE during validation, re-train the learner and then plot
the results.</p>
<div class="sourceCode" id="cb28"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">mlp_custom</span><span class="op">$</span><span class="va">param_set</span><span class="op">$</span><span class="fu">set_values</span><span class="op">(</span></span>
<span>  measures_valid <span class="op">=</span> <span class="fu"><a href="https://mlr3.mlr-org.com/reference/mlr_sugar.html" class="external-link">msr</a></span><span class="op">(</span><span class="st">"regr.mae"</span><span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">mlp_custom</span><span class="op">$</span><span class="fu">train</span><span class="op">(</span><span class="va">task</span><span class="op">)</span></span>
<span><span class="va">mlp_custom</span><span class="op">$</span><span class="va">history</span><span class="op">$</span><span class="fu">plot</span><span class="op">(</span><span class="st">"regr.mae"</span>, set <span class="op">=</span> <span class="st">"valid"</span>, epochs <span class="op">=</span> <span class="fl">5</span><span class="op">:</span><span class="fl">30</span><span class="op">)</span></span></code></pre></div>
<p><img src="get_started_files/figure-html/unnamed-chunk-29-1.png" width="50%" style="display: block; margin: auto;"></p>
<p>We can confirm that not only the training loss, but also the
validation loss was decreasing over time, all is well!</p>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by Sebastian Fischer, Martin Binder.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
